{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "457a86fa",
   "metadata": {},
   "source": [
    "# Graded exercise 3: Sampling in high dimension"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "af18a9ab",
   "metadata": {},
   "source": [
    "The aim of the exercices is to test the following:\n",
    "- Python/Numpy/plotting skills\n",
    "- Using external sources to find the functions you need\n",
    "- Theoretical comprehension of the notion of:\n",
    "    1. sampling with Markov Chain Monte Carlo.\n",
    "    2. empirical estimators and their error.\n",
    "    3. high dimensional phenomena.\n",
    "\n",
    "Each question is given in a markdown cell and should be answered in the cell/cells below (you can add cells if needed). \n",
    "The figures must be generated and shown directly in this notebook. \n",
    "If a questions demands that you write an answer, use a 'markdown' cell, which can include latex between \\\\$ symbols. As an example,\n",
    "\\\\$\\vec{F}=m\\vec{a}\\\\$\n",
    "gives $\\vec{F}=m\\vec{a}$.\n",
    "\n",
    "Remember:\n",
    "- Add labels for both axes to each plot and a legend whenever you are plotting multiple curves on the same figure, even when not specified in the question.\n",
    "- Your code (including the plotting of the figures) should run  when restarting the kernel and executing all the cells in sequence from top to bottom. \n",
    "- Running all cells should take a reasonable time on noto (<2 min.), with the possible exception of Question 4.20 which should not take more than 5 min., see below.\n",
    "- All the sources you consult should be explicitly cited, except the numpy and matplotlib official documentation, the lecture notes and the previous exercises. \n",
    "    You are encouraged to use external sources, since every function needed in this exercise has not necessarily been seen in the previous exercises.\n",
    "    **You can cite the sources directly in the cells where you used them by adding either a comment in the Markdown text, or a Python comment in the code.**\n",
    "\n",
    "**Working together and citing each other does not mean copy/pasting each other code, text or equations. Please write your own responses to each of the questions, whatever sources you used.**\n",
    "\n",
    "Hint: Recall that using for-loops over elements of a large array is very inefficent. It should be avoided everywhere in the code, i.e. you should instead use numpy array operations over the data samples as much as possible."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "966ff548",
   "metadata": {},
   "source": [
    "### Introduction \n",
    "\n",
    "Before starting, let us remind some definitions.\n",
    "The $d$-dimensional unit cube $H_d$ is the set $H_d = [-1, 1]^d \\subset \\mathbb{R}^d$, i.e. the centered  cube with side of length 2. \n",
    "The $d$-dimensional disk $D_d$ is the set $D_d = \\{ x \\in \\mathbb{R}^d \\text{ such that } ||x|| \\leq 1\\} \\subset \\mathbb{R}^d$. \n",
    "The $d$-dimensional Euclidean norm is defined as \n",
    "$$\n",
    "    ||x||^2 = \\sum_{i=1}^d x_i^2 \\, .\n",
    "$$\n",
    "\n",
    "In this exercise, we will experiment how Markov Chain Monte Carlo (MCMC) methods can drastically improve sampling efficiency in high dimension.\n",
    "We will consider and compare 2 strategies for sampling i.i.d. points uniformly distributed in the disk:\n",
    "1) Direct sampling: we sample $n$ i.i.d. points in the cube, and reject all the points that are not in the disk. The remaining samples are i.i.d. in the disk.\n",
    "2) MCMC: we perform a carefully engineerd random walk for $T$ time steps in the disk, such that all samples are directly i.i.d. in the disk.\n",
    "\n",
    "We will use the i.i.d. sample that we obtain to compute the average length of a vector uniformly distributed on the disk, i.e. to compute\n",
    "$$\n",
    "    \\mathbb{E}_{x \\sim D_d} [||x||] = \\frac{\n",
    "        \\int_{||x|| \\leq 1} d^dx \\, ||x||\n",
    "    }{\n",
    "        \\int_{||x|| \\leq 1} d^dx \n",
    "    }\n",
    "$$\n",
    "where the notation $\\mathbb{E}_{x \\sim D_d} f(x)$ is a shortand for \"average of the function $f(x)$ for $x$ uniformly distributed over the set $D_d$\". \n",
    "The uniform distribution over $D_d$ is given by\n",
    "$$\n",
    "    \\rho_{D_d}(x) = \\begin{cases}\n",
    "        \\frac{\n",
    "        1\n",
    "        }{\n",
    "            \\int_{||x|| \\leq 1} d^dx \n",
    "        } & \\text{ if } ||x||\\leq 1 \\\\\n",
    "        0 & \\text{ otherwise}\n",
    "    \\end{cases}\n",
    "$$\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4aabfa37",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "e19fea5d",
   "metadata": {},
   "source": [
    "## Exercise 1: computing the theoretical expectation"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "6ec0213a",
   "metadata": {},
   "source": [
    "The average value we want to compute using direct sampling and MCMC is given by \n",
    "$$\n",
    "    \\mathbb{E}_{x \\sim D_d} [||x||] = \\frac{\n",
    "        \\int_{||x|| \\leq 1} d^dx \\, ||x||\n",
    "    }{\n",
    "        \\int_{||x|| \\leq 1} d^dx \n",
    "    }\n",
    "$$\n",
    "\n",
    "In this particular case, we can compute the integral by hand in order to obtain a true value against which we can compare the performance of our algorithms."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "5f29dbe3",
   "metadata": {},
   "source": [
    "**Ex 1.1 (1 pt)** Compute the expectation value defined above. Your computation should fit in one line. \n",
    "\n",
    "Hint: use hyperspherical coordinates, and realise that you do not really need to explicitate nor compute the angular part."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "9b90027e",
   "metadata": {},
   "source": [
    "**Answer**: complete"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "8126b84c",
   "metadata": {},
   "source": [
    "**Ex 1.2 (0.5 pt)** Write a function that computes the theoretical expectation defined above, given the dimension $d$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa5abf7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def theoretical_avg(d):\n",
    "    # complete"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "b2f8a5aa",
   "metadata": {},
   "source": [
    "## Exercise 2: direct sampling in high dimension"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "08289586",
   "metadata": {},
   "source": [
    "We start by implementing direct sampling. \n",
    "\n",
    "**2.1 (1 pt)**\n",
    "Write a function that:\n",
    "- Takes as input the dimension $d$ and the number of samples $n$\n",
    "- Generates $n$ independent points uniformly distributed in $H_d$\n",
    "- Discards all points that are not in $D_d$\n",
    "- Returns the non-discarded points as an array of d-dimensional arrays\n",
    "\n",
    "Before returning the result, your function should check that the returned array is not empty. If it is, print the string f\"No points sampled for d={d} and n={n}\" and return `np.array([[0,0]])`.\n",
    "\n",
    "Print the result of your function for $d=2$ and $n=10$, and for $d=15$ and $n=10$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0793ef9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def direct_sampling(d, n):\n",
    "    # complete\n",
    "\n",
    "# complete"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "21b1da36",
   "metadata": {},
   "source": [
    "We now have a function that generates some i.i.d. points in the disk. Notice that the way we wrote the function we set only the total number of trials, and in general the number of points generated will be smaller than $n$ due to rejections.\n",
    "\n",
    "**2.2 (2 pt)** \n",
    "Write a function that takes as input the output of the `direct_sampling` function, and computes\n",
    "- the empirical average of the observable $f(x) = ||x||$\n",
    "- the error of the empirical average\n",
    "\n",
    "Return the two quantities you computed as a list with 2 elements.\n",
    "Print the result of your function applied to a dataset of points generated with the `direct_sampling` function, setting $d=2$ and $n=10000$.\n",
    "Print also the theoretical value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "928c05aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def direct_sampling_empirical_estimator_norm(points):\n",
    "    # complete\n",
    "\n",
    "# complete"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "389d6287",
   "metadata": {},
   "source": [
    "**2.3 (1.5 pt)** \n",
    "Generate a dataset using `direct_sampling` with $d=2$ and $n=20000$.\n",
    "For `k in np.arange(start=10, stop=10000, step=100)`, compute the empirical estimator for the norm using the first `k` points of the dataset, and its error.\n",
    "Plot the value of the empirical estimator as a function of `k`.\n",
    "Add errorbars using the error of the empirical estimator. \n",
    "Add an horizontal line with the theoretical value.\n",
    "\n",
    "Hint: you can either use `plt.errorbar`, or `plt.fill_between` with `alpha = 0.5` for a better graphical result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c5cf5a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# complete"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "2a063e3f",
   "metadata": {},
   "source": [
    "**2.4 (0.5 pt)** \n",
    "Reproduce exactly the plot in 2.3 for `d = [3,5,8,20]`. Plot each value of d on a different plot.\n",
    "You should see something strange happening in the plots as the dimension grows. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "023b7f1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# complete"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "07dd50db",
   "metadata": {},
   "source": [
    "**2.5 (1.5 pt)** Answer the following questions, use max two lines per answer.\n",
    "1. What unexpected feature appears in the plots as the dimension grows?\n",
    "2. Why does it appear?\n",
    "3. What happens at $d=20$?"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "cc39d949",
   "metadata": {},
   "source": [
    "**Answer**: complete"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "352275be",
   "metadata": {},
   "source": [
    "**2.6 (1 pt)** \n",
    "Plot the fraction of rejected points in the `direct_sampling` algorithm as a function of `d`, using `d=2, ..., 20`. Fix `n=100000`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c786ee3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#  complete"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "ab636a62",
   "metadata": {},
   "source": [
    "**2.7 (1 pt)**\n",
    "What happens as $d$ increases? Is our estimation of the norm reliable in high dimension? Answer in max two lines."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "b800ed2c",
   "metadata": {},
   "source": [
    "**Answer**:  complete"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "4f10fe4d",
   "metadata": {},
   "source": [
    "## Exercise 3: sampling with MCMC in high dimension\n",
    "\n",
    "In the last exercise, we saw that in high dimensions the curse of dimensionality may hinder the effectiveness of sampling algorithms that work just fine in low dimension. We also learned that \"high dimension\" can mean as little as $d=15$!\n",
    "We need to come up with another (smarter) strategy to sample uniformly the disk $D_d$. In the specific case of the disk, one could come up with many smart solutions, but here we will focus on MCMC.\n",
    "\n",
    "In class, and in Exercise 12, we learned that it is possible to use Markov Chain Monte Carlo (MCMC) to sample from a generic distribution. \n",
    "MCMC is first of all a random walk, i.e. a sequence of points $x(t) \\in \\mathbb{R}^d$ that are generated one after the other with some kind of randomness. \n",
    "It is also a Markov Chain, which means that the evolution $x(t) \\to x(t+1)$ is random, and that $x(t+1)$ depends only on $x(t)$, and not on all previous positions of the chain.\n",
    "Finally, through the detailed balance condition, we can design the random evolution $x(t) \\to x(t+1)$ such that the stationary distribution of $x(t)$ is the distribution we want.\n",
    "The detailed balance condition reads\n",
    "$$\n",
    "    P(a \\to b) \\pi(a) = P(b \\to a) \\pi(b) \\quad \\forall a,b\n",
    "$$\n",
    "where $P(a \\to b)$ is the transition probability of going from $a \\to b$, $\\pi$ is the distribution we want to sample from (the stationary distribution of our MCMC), and $a,b$ are possible states of our random walker (for us, points in $\\mathbb{R}^d$).\n",
    "\n",
    "Let us design an MCMC algorithm to sample from $\\pi = $ uniform distribution on the disk, i.e.\n",
    "$$\n",
    "    \\pi(x) = \\begin{cases}\n",
    "        \\frac{1}{\\Omega_d} & \\text{if } x \\in D_d \\\\\n",
    "        0 & \\text{otherwise}\n",
    "    \\end{cases}\n",
    "$$\n",
    "where $\\Omega_d$ is the volume of the $d$-dimensional disk $D_d$ (and acts as a normalisation of our p.d.f. $\\pi$).\n",
    "We consider a simple random walker, defined by $x(t=0) = 0 \\in \\mathbb{R}^d$, and $x(t+1) = x(t) + \\Delta$.\n",
    "$\\Delta$ is our evolution step, and is a random variable, independent and identically distributed with p.d.f. $\\rho(\\Delta)$ at each different time step.\n",
    "The only freedom we have left in the design of the random walk is the distribution $\\rho(\\Delta)$, so we need to use the detailed balance condition to fix $\\rho$ in order to sample from $\\pi$."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "d9fd2ad9",
   "metadata": {},
   "source": [
    "**3.1 (1 pt)** Write the detailed balance condition for $a = x$, $b = x + \\Delta$, supposing that both $a, b \\in D_d$. In other words, we are considering the case in which we propose a step for our random walker, and the step keeps it in the disk. What does it imply for $P(a\\to b)$? Answer in 2 lines."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "01bde8a9",
   "metadata": {},
   "source": [
    "**Answer**: complete"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "b6d6c946",
   "metadata": {},
   "source": [
    "**3.2 (1pt)** Write the detailed balance condition for $a = x$, $b = x + \\Delta$, supposing that both $a \\in D_d$, and that $b \\notin D_d$. In other words, we are proposing a step for our random walker that would bring it out of the disk. What does it imply for the transition probability? Answer in two lines."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "de3b4678",
   "metadata": {},
   "source": [
    "**Answer**: complete"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "2eed1809",
   "metadata": {},
   "source": [
    "The two points above give us the following MCMC algorithm, which is guaranteed to sample the uniform distribution on the disk $D_d$ (after possibly some time steps of equilibration).\n",
    "- Initialize $x(t=0) = 0$\n",
    "- Generate randomly an increment $\\Delta \\in \\mathbb{R}^d$ using a symmetric p.d.f. $\\rho$ (due to point 3.1).\n",
    "- If $x(t) + \\Delta \\in D_d$, then set $x(t+1) = x(t)+\\Delta$\n",
    "- If $x(t) + \\Delta \\notin D_d$, then set $x(t+1) = x(t)$ (due to point 3.2)\n",
    "- Repeat up to a total time $T$.\n",
    "\n",
    "In what follows, we set $\\rho$ to be the uniform distribution in the interval $[-c, c]^d$.\n",
    "\n",
    "**3.3 (2 pt)** Write a function that implements the MCMC algorithm. It takes as input $d, T, c$ and returns an array of the positions $x(t)$ in the same format as the output of `direct_sampling`. Print the output of the function for $d=2$, $T=5$ and $c=1$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48f2973b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mcmc(d, T, c):\n",
    "    # complete\n",
    "\n",
    "# complete"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "a3729981",
   "metadata": {},
   "source": [
    "**3.4 (1 pt)** Write a function that computes the empirical average of the norm $||x||$ on the output of the function `mcmc`. Print its output on a dataset produced with `mcmc` and $d=2$, $T=5000$, $c=0.2$. Print also the corresponding theoretical value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd75daf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mcmc_empirical_avg(points):\n",
    "    # complete\n",
    "\n",
    "# complete"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "5aa6404b",
   "metadata": {},
   "source": [
    "**3.5 (1 pt)** \n",
    "Generate a dataset using `mcmc` with $d=2$, $T=10000$ and $c=0.5$.\n",
    "For `k in np.arange(start=10, stop=10000, step=100)`, compute the empirical estimator for the norm using the first `k` points of the dataset.\n",
    "Plot the value of the empirical estimator as a function of `k`.\n",
    "Add an horizontal line with the theoretical value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59ee2486",
   "metadata": {},
   "outputs": [],
   "source": [
    "# complete"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "1a486c07",
   "metadata": {},
   "source": [
    "**3.6 (1 pt)** Answer the following questions in one line per question.\n",
    "1. Compute the compute the expected square distance $||x(t+1)-x(t)||^2$, i.e. the square of how far the walker moves at each step.\n",
    "2. How should we set $c$ in order to have expected square distance $||x(t+1)-x(t)||^2 = 1/2$? "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "96bb9eb3",
   "metadata": {},
   "source": [
    "**Answer**: complete"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "27836b93",
   "metadata": {},
   "source": [
    "**Comment** We see that in order to have displacements that are of constant size, i.e. of length that is not growing with $d$, we need to rescale $c$ with a precise function of the dimension $d$ (the constant factor is unimportant, as it does not depend on the dimension). If we do not do that, we risk talking too large steps when the dimension increases. The concept of _scaling_ is ubiquitous when considering systems whose dimensionality can change."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "fd4e227f",
   "metadata": {},
   "source": [
    "**3.7 (0.5 pt)** \n",
    "Reproduce exactly the plot in 3.5 for `d = [3,5,10,15,20,100]`. Plot each value of d on a different plot. Use $T = 10000$ and $c = 0.5 / \\sqrt{d}$ (in order to have steps of length that is not scaling with $d$, see 3.6)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9652fb99",
   "metadata": {},
   "outputs": [],
   "source": [
    "# complete"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "06d0be62",
   "metadata": {},
   "source": [
    "**3.8 (2 pt)** Answer the following questions, using max two lines per answer.\n",
    "1. What is the main difference between the plots for MCMC and the corresponding plots for direct sampling (points 2.3 and 2.4)?\n",
    "2. Why is the convergence speed of the mcmc algorithm mostly independent on the dimension? Compare with direct sampling."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "ba1de2df",
   "metadata": {},
   "source": [
    "**Answer**: complete"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "770fe424",
   "metadata": {},
   "source": [
    "## Exercise 4: error on the mcmc empirical estimator, and the dependence of the average norm on the dimension"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "1f4b7826",
   "metadata": {},
   "source": [
    "**4.1 (2 pt)**\n",
    "Write a function that estimates the average of the norm and its error. \n",
    "It takes as input $d$, $T$, $c$ and another parameter that you may need, and returns a list with two elements, the empirical average and its error.\n",
    "Print your estimate for the average norm and its error using $d=20$, $T=5000$ and $c = 0.5 / \\sqrt{d}$.\n",
    "Print also the theoretical value.\n",
    "\n",
    "Hint: revise Exercise 12."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5bab756",
   "metadata": {},
   "outputs": [],
   "source": [
    "# complete"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "f8b6270c",
   "metadata": {},
   "source": [
    "**4.2 (2pt)** Plot for `d in np.arange(2,20,2)` \n",
    "$$\n",
    "    \\frac{1 - \\text{Estimate of average norm}(d)}{1 - \\text{theoretical value of average norm}(d)}\n",
    "$$\n",
    "using $T=5000$ and $c = 0.5 / \\sqrt{d}$. \n",
    "Add two kinds of error bars:\n",
    "- one with your estimate of the error\n",
    "- one with 2 times your estimate of the error. \n",
    "\n",
    "Plot an horizontal line at $y = 1$.\n",
    "\n",
    "_This cell may take a bit more time to run. We suggest that you start coding it with a smaller value of $T$, or a smaller vector of values of $d$, and once you believe that your code works correctly, use the given values._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70f079db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# complete\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "c81a4ee0",
   "metadata": {},
   "source": [
    "**Comment**\n",
    "We observe that as the dimension grows, the average norm $||x||$ of the points in the unit disk converges to 1. This means that in high dimension, the volume inside the disk is very much concentrated on a thin shell next to the boundary $||x||=1$, while the center of the disk occupies a vanishingly small fraction of the volume. Weird!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  },
  "unianalytics_cell_mapping": [
   [
    "457a86fa",
    "457a86fa"
   ],
   [
    "af18a9ab",
    "af18a9ab"
   ],
   [
    "966ff548",
    "966ff548"
   ],
   [
    "4aabfa37",
    "4aabfa37"
   ],
   [
    "e19fea5d",
    "e19fea5d"
   ],
   [
    "6ec0213a",
    "6ec0213a"
   ],
   [
    "5f29dbe3",
    "5f29dbe3"
   ],
   [
    "9b90027e",
    "9b90027e"
   ],
   [
    "8126b84c",
    "8126b84c"
   ],
   [
    "fa5abf7b",
    "fa5abf7b"
   ],
   [
    "b2f8a5aa",
    "b2f8a5aa"
   ],
   [
    "08289586",
    "08289586"
   ],
   [
    "c0793ef9",
    "c0793ef9"
   ],
   [
    "21b1da36",
    "21b1da36"
   ],
   [
    "928c05aa",
    "928c05aa"
   ],
   [
    "389d6287",
    "389d6287"
   ],
   [
    "9c5cf5a9",
    "9c5cf5a9"
   ],
   [
    "2a063e3f",
    "2a063e3f"
   ],
   [
    "023b7f1d",
    "023b7f1d"
   ],
   [
    "07dd50db",
    "07dd50db"
   ],
   [
    "cc39d949",
    "cc39d949"
   ],
   [
    "352275be",
    "352275be"
   ],
   [
    "1c786ee3",
    "1c786ee3"
   ],
   [
    "ab636a62",
    "ab636a62"
   ],
   [
    "b800ed2c",
    "b800ed2c"
   ],
   [
    "4f10fe4d",
    "4f10fe4d"
   ],
   [
    "d9fd2ad9",
    "d9fd2ad9"
   ],
   [
    "01bde8a9",
    "01bde8a9"
   ],
   [
    "b6d6c946",
    "b6d6c946"
   ],
   [
    "de3b4678",
    "de3b4678"
   ],
   [
    "2eed1809",
    "2eed1809"
   ],
   [
    "48f2973b",
    "48f2973b"
   ],
   [
    "a3729981",
    "a3729981"
   ],
   [
    "cd75daf8",
    "cd75daf8"
   ],
   [
    "5aa6404b",
    "5aa6404b"
   ],
   [
    "59ee2486",
    "59ee2486"
   ],
   [
    "1a486c07",
    "1a486c07"
   ],
   [
    "96bb9eb3",
    "96bb9eb3"
   ],
   [
    "27836b93",
    "27836b93"
   ],
   [
    "fd4e227f",
    "fd4e227f"
   ],
   [
    "9652fb99",
    "9652fb99"
   ],
   [
    "06d0be62",
    "06d0be62"
   ],
   [
    "ba1de2df",
    "ba1de2df"
   ],
   [
    "770fe424",
    "770fe424"
   ],
   [
    "1f4b7826",
    "1f4b7826"
   ],
   [
    "d5bab756",
    "d5bab756"
   ],
   [
    "f8b6270c",
    "f8b6270c"
   ],
   [
    "70f079db",
    "70f079db"
   ],
   [
    "c81a4ee0",
    "c81a4ee0"
   ]
  ],
  "unianalytics_notebook_id": "fa9dbbfd-43ff-4828-ba12-6ec05e4076ff"
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
