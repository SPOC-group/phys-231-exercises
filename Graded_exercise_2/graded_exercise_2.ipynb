{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Graded homework 2: Maximum likelihood estimation"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This graded homework has 2 exercises. The aim of the exercices is to test the following:\n",
    "- Python/Numpy/plotting skills\n",
    "- Using external sources to find the functions you need\n",
    "- Theoretical comprehension of the notion of:\n",
    "    1. probability density functions.\n",
    "    2. Law of large numbers.\n",
    "    3. Central limit theorem.\n",
    "    4. Properties of estimators.\n",
    "    5. Maximum likelihood estimators.\n",
    "\n",
    "Each question is given in a markdown cell and should be answered in the cell/cells below (you can add cells if needed). The figures must be generated and shown directly in this notebook. \n",
    "Remember to add labels for both axes to each plot, even when not explicitly required. \n",
    "If a questions demands that you write an answer, use a 'markdown' cell, which can include latex between \\\\$ symbols. As an example,\n",
    "\\\\$\\vec{F}=m\\vec{a}\\\\$\n",
    "gives $\\vec{F}=m\\vec{a}$.\n",
    "\n",
    "\n",
    "Your code (including the plotting of the figures) should run  when restarting the kernel and executing all the cells in sequence from top to bottom. \n",
    "Running all cells should take a reasonable time on noto (<2 min.).\n",
    "\n",
    "All the sources you consult should be explicitly cited, except the numpy and matplotlib official documentation, the lecture notes and the previous exercises. You are encouraged to use external sources, since every function needed in this exercise has not necessarily been seen in the previous exercises.\n",
    "You can cite the sources directly in the cells where you used them by adding either a comment in the Markdown text, or a Python comment in the code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ex 1: Particle decay"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this exercise, we will look numerically at Example 4 (see chapter 9 of the lecture notes).\n",
    "\n",
    "A radioactive source emits $n$ unstable particles. The particles are emitted at constant speed along the $x$ axis. For each particle, indexed by  $i\\in\\{1,\\dots,n\\}$, we mark the  position $x_i$ where it decays. Our detector however only detects decays between $x_{min}$ and $x_{max}$.\n",
    "\n",
    "The probability density of decaying at $x$ is \n",
    "$$\n",
    "\\rho(x|\\lambda_*)=\\frac{e^{-x/\\lambda_*} }{\\lambda_* \\left(e^{-x_{min}/\\lambda_*}-e^{-x_{max}/\\lambda_*}\\right)}\n",
    "$$\n",
    "for an unknown parameter $\\lambda_*$.\n",
    "\n",
    "Given the dataset of $n$ measurements $\\mathbf x=(x_1,\\dots,x_n)$ our objective is to estimate $\\lambda_*$.\n",
    "\n",
    "\n",
    "In our case we set $x_{min}=0.5,x_{max}=3,\\lambda_*=4$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lambda_star=4 #value of the parameter that we want to estimate. Also said \"ground truth\".\n",
    "xmin=0.5\n",
    "xmax=3"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ex 1.1: simulating the particle decay"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Your first task is to simulate this physical system, namely to generate the dataset $x_1, x_2, \\dots, x_n$ given the true value $\\lambda_*$.\n",
    "\n",
    "To generate the data $\\pmb x$, you will use the so called *rejection method*. \n",
    "The idea is that the p.d.f. from which we want to sample is just a truncated exponential, i.e. an exponential with scale parameter $\\lambda_*$ in which we discard events in which $x < x_{\\rm min}$ or $x > x_{\\rm max}$.\n",
    "Thus, we can just generate exponential random numbers using `np.random.exponential` one by one. \n",
    "For each sample, we check if $x_{\\rm min} < x < x_{\\rm max}$. If this is the case, we keep this sample, otherwise we discard it. \n",
    "We iterate the procedure until we have $n$ valid measurements.\n",
    "\n",
    "**1.1.1 (1pt)**\n",
    "\n",
    "Write a function that takes as input $n$, $\\lambda_*$, $x_{\\rm min}$ and $x_{\\rm max}$ and generates the dataset as a lenght $n$ vector. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here\n",
    "def generate_dataset_decay(# Complete here):\n",
    "    # Complete here"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1.1.2 (1pt)**\n",
    "\n",
    "Generate a dataset with $x_{min}=0.5,x_{max}=3,\\lambda_*=4$. and $n=2000$ and plot its histogram. \n",
    "Use ```density = True``` (to have the histrogram represent a p.d.f.) and the x axis limits to $(0, 5)$.\n",
    "\n",
    "You will observe the truncations at $x_{min}$ and $x_{max}$ and (albeit very noisily) the exponential decay of the distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ex 1.2: computing the maximum likelihood estimator"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Review Example 4 in chapter 9 of the lecture notes. There, we derived the maximum likelihood estimator for $\\lambda_*$ as \n",
    "$$\n",
    "\\hat{\\lambda} \n",
    "    = \\arg\\max_{\\lambda} \\frac{e^{-\\frac{1}{\\lambda n}\\sum_{i=1}^n x_i} }{\\lambda \\left( e^{-x_{\\rm min} / \\lambda} - e^{-x_{\\rm max} / \\lambda} \\right)}  \n",
    "    = \\arg\\max_{\\lambda} \\mathcal{L}(\\lambda)\n",
    "    \\, .\n",
    "$$\n",
    "It is easier to work with the negative normalized log-likelihood $- \\frac{1}{n} \\log \\mathcal{L}(\\lambda)$, which needs to be minimised (due to the negative sign), giving\n",
    "$$\n",
    "    \\hat \\lambda=\\arg\\min_\\lambda \\ell(\\lambda)\n",
    "$$\n",
    "where\n",
    "$$\n",
    "\\ell(\\lambda) =\n",
    "\\log\\lambda + \\log\\left(e^{-x_{min}/\\lambda}-e^{-x_{max}/\\lambda}\\right) + \\frac{1}{\\lambda} \\frac{1}{n}\\sum_{i=1}^n x_i \\, .\n",
    "$$\n",
    "\n",
    "**1.2.1 (1pt)**\n",
    "Complete the code below to implement the $\\ell(x)$ function. Notice that ```lambda``` is a reserved keyword in python, so we use ```l```."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here\n",
    "def ell(l, xmin, xmax, dataset):\n",
    "    # Complete here"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1.2.2 (1pt)** Use the function ```scipy.optimize.minimize``` to numerically minimize $\\ell(\\lambda)$ for the dataset generated in 1.1.2 and compute the MLE. Use ```0.1``` for the initial condition of the minimization. Print the MLE that you obtain.\n",
    "\n",
    "Hint: ```scipy.optimize.minimize``` expects a function of the form ```f(l)```, while our function is of the form ```f(l, params)```. \n",
    "This happens often.\n",
    "The way to deal with this is through lambda functions, also called anonymous functions:\n",
    "```\n",
    "    scipy.optimize.minimize(lambda l: f(l, params), ...)\n",
    "```\n",
    "The keyword ```lambda``` introduces the anonymous function, ```l``` is the variable of the function, and whatever comes after the semicolon `:` is the body of the function.\n",
    "You can think of the expression \n",
    "```\n",
    "    lambda l: f(l, params)\n",
    "```\n",
    "as a shorthand for\n",
    "```\n",
    "    def f2(l):\n",
    "        return f(l, params)\n",
    "```\n",
    "with the advantages of\n",
    "\n",
    "1. not giving a name (```f2``` in the example) to a function that you will use only once.\n",
    "2. Not defining a function in which we hardcode a global variable (```dataset``` in the example)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1.2.3 (1pt)** Plot the function $\\ell(\\lambda)$ as a function of $\\lambda$ using the dataset generated in 1.1.2. \n",
    "Additionaly, plot a vertical line at the MLE estimator $x = \\hat{\\lambda}$ that you computed in 1.2.2, and another vertical line at the value $x = \\lambda_*$.\n",
    "Set the x axis limits to $(2,10)$ and add a legend."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1.2.4 (1pt)** \n",
    "The ```scipy.optimize.minimize``` uses under the hood sophisticated minimization methods, but for the sake of the argument let us imagine that it is using gradient descent to minimize the function $\\ell$. Discuss the following points:\n",
    "\n",
    "- does gradient descent converge to the global optimum in general (i.e. not in this specific case), and what are the consequences for the MLE if it does not? \n",
    "- In this case, does gradient descent converge to the global optimum of the likelihood?\n",
    "\n",
    "Motivate your answers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Your answer here:**"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ex 2: The lighthouse"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A lighthouse is placed on a small island in front of the coastline. \n",
    "In a coordinate system in which the coast is the $x$ axis, and the $y$ axis points towards open sea, the island is at position $(\\alpha_*, 1)$.\n",
    "The lighthouse emits a series of short flashes at random intervals and hence in random directions $\\theta$ (uniformly at random over the full angle of $2\\pi$).\n",
    "These pulses are intercepted on the coast by photo-detectors that record only the fact that a flash has occurred, but not the angle from which it came.\n",
    "\n",
    "In this exercise, we will consider the estimation task of finding the true position $\\alpha_*$ of the lighthouse along the coastline, assuming that \n",
    "- the angle $\\theta$ at which each light pulse is emitted is a uniformly distributed random variable on the interval $(-\\pi/2, \\pi/2)$ (the angle is measured with respect to the negative-vertical axis, see figure)\n",
    "- a dataset of $n$ detected positions $x_1, x_2, \\dots, x_n$ is given (each detection is statistically independent from the others)."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](lighthouse.png)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ex 2.1: simulating the lighthouse, i.e. generating the dataset of measurements"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Your first task is to simulate this physical system, namely to generate the dataset $x_1, x_2, \\dots, x_n$ given the true value $\\alpha_*$.\n",
    "To do this, we are going to generate randomly the angle $\\theta_i$ at which a light pulse is emitted and we will compute the corresponding detection position $x_i$ as\n",
    "$$\n",
    "    x_i = \\tan(\\theta_i) + \\alpha_* \\, .\n",
    "$$"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2.1.1 (1pt)** Write a function that takes as input $n$ and $\\alpha_*$ and generates the dataset as a lenght $n$ vector. Recall that each angle is independent from the others, and uniformly distributed in the interval $(-\\pi/2, \\pi/2)$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here\n",
    "def generate_dataset(# Complete here):\n",
    "    # Complete here"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2.1.2 (1.5pt)** Generate a dataset with $\\alpha_* = 20$ and $n=10000$ and plot its histogram. \n",
    "Set manually the bins to ```range(-300,300)``` (this means that we use 600 bins with width 1 on the interval $(-300,300)$) and the y axis to log-scale.\n",
    "Add a vertical line at $x = \\alpha_*$ in red.\n",
    "Add a legend and labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2.1.3 (1pt)** Answer the following questions.\n",
    "\n",
    "1. Around which value of $x$ is the histogram peaked? Motivate your answer.\n",
    "2. Which values of the angle $\\theta$ correspond to the extreme values (the rare outliers)?"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Your answer here:**"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2.1.4 (2pt)** Prove that the probability density function of the random variable $x(\\theta)$ is given by\n",
    "$$\n",
    "    \\rho_{x}(x | \\alpha_*) = \\frac{1}{\\pi(1+(x - \\alpha_*)^2)} \\, .\n",
    "$$\n",
    "Type your derivation in a Markdown cell using LaTeX.\n",
    "\n",
    "Hints:\n",
    "1. Write down the p.d.f. of the random variable $\\theta$, recalling that it is uniformly distributed on the interval $(-\\pi/2, \\pi/2)$.\n",
    "2. Use the change of variable formula for probability density functions (see the Probability and Statistics course)."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Your answer here:**"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2.1.5 (1pt)** \n",
    "Reproduce the plot you did in 2.1.2. \n",
    "This time, use the option ```density = True``` in the histogram (to plot frequencies instead of counts on the y axis), and plot in black the p.d.f. $\\rho_x$ you derived at the previous point on top of it.\n",
    "Plot from $x=-100$ to $x=100$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ex 2.2: estimation of $\\alpha_*$ through maximum likelihood"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section, we implement the maximum likelihood estimator for $\\alpha$, and use it to estimate $\\alpha_*$ in the given dataset."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2.2.1 (0.5pt)** \n",
    "Use ```np.load``` to load the file \"data-lighthouse.npy\" into a variable called ```dataset```. \n",
    "This will be your set of observations for this part of the exercise, in which we will try to estimate the value $\\alpha_*$ used to generate the dataset. \n",
    "How many measurements does it contain?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Your answer here:**"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2.2.2 (2pt)** Prove that the maximum likelihood estimator (MLE) $\\hat{\\alpha}$ is given by\n",
    "$$\n",
    "\\hat{\\alpha} = \\arg\\min_{\\alpha} \\sum_{i=1}^n \\log (1+(x_i - \\alpha)^2) = \\arg\\min_{\\alpha} f(\\alpha, \\{x_i\\}_{i=1}^n)\n",
    "$$\n",
    "Type your derivation in a Markdown cell using LaTeX.\n",
    "\n",
    "Hints: \n",
    "- you can follow and adapt the many examples of MLE given in class to this specific problem. \n",
    "- It is useful to maximise the log-likelihood instead of the likelihood to convert products into sums (recall that logs do not alter the position of maxima). \n",
    "- $\\max[f(x)] = \\min[- f(x)]$."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Your answer here:**"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2.2.3 (1pt)** Write a function that takes as input $\\alpha$ and the dataset (a length $n$ vector) and computes $f(\\alpha, \\{x_i\\}_{i=1}^n)$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2.2.4 (1.5pt)** Plot the function $f(\\alpha, \\{x_i\\}_{i=1}^n)$ as a function of $\\alpha$ and the given dataset. Plot it for $-10 < \\alpha < 30$.\n",
    "What can you deduce about the value of $\\hat{\\alpha}$ from the plot?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Your answer here**"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2.2.5 (1pt)** Minimize numerically $f$ for your dataset and compute the MLE. Use ```x0=0``` for the initial condition of the minimization. Print the MLE that you obtain."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2.2.6 (1pt)** \n",
    "Reproduce the plot you did in 2.2.4.. \n",
    "Additionaly, plot a vertical line at  $x = \\hat{\\alpha}$ and add a legend."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ex 2.3: law of large numbers and central limit theorem for the MLE estimator"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recall that the MSE estimator, and more in general every estimator, is a function of the dataset, i.e.\n",
    "$$\n",
    "    \\hat{\\alpha} = \\hat{\\alpha}(x_1, \\dots, x_n) \\, .\n",
    "$$\n",
    "Thus, given that the dataset is a random event (different runs of the experiment will produce different datasets), the estimator itself is a random variable. \n",
    "We say that the estimator is **unbiased** if on average it retrieves the correct value of $\\alpha$\n",
    "$$\n",
    "    \\mathbb{E}(\\hat{\\alpha}) = \\alpha_* \\, .\n",
    "$$\n",
    "Moreover, it can happen that an estimator satisfies a law of large numbers, i.e. that as $n \\to \\infty$\n",
    "$$\n",
    "    \\hat{\\alpha} \\to \\mathbb{E}(\\hat{\\alpha}) \\, .\n",
    "$$\n",
    "This means that the more measurements we have, the less randomness the estimator $\\hat{\\alpha}$ will exhibit, to the point that for $n \\to \\infty$ it is not a random quantity anymore (it has no fluctuations), and retrieves exactly the value $\\mathbb{E}(\\hat{\\alpha})$, which is the correct value $\\alpha_*$ **only if** the estimator is unbiased.\n",
    "\n",
    "More in detail, for $n\\to\\infty$:\n",
    "1. the MLE estimator satisfies a law of large numbers (LLN) and is unbiased, i.e. \n",
    "$$\n",
    "    \\hat{\\alpha} \\to \\alpha_* \\, .\n",
    "$$\n",
    "2. The MLE estimator satisfies a central limit theorem (CLT), i.e. the fact that the fluctuations of the MLE around the true value are Gaussian with variance decreasing as $\\sim 1/\\sqrt{n}$\n",
    "$$\n",
    "    \\hat{\\alpha} - \\alpha_* \\sim N(0, \\text{const} / \\sqrt{n}) \\, .\n",
    "$$\n",
    "\n",
    "This can be equivalently phrased as the fact that the standardised MLE\n",
    "$$\n",
    "    \\hat{\\alpha}_{\\rm std} = \\frac{\\hat{\\alpha} - \\mathbb{E}(\\hat{\\alpha})}{\\sqrt{\\text{Var} (\\hat{\\alpha})}}\n",
    "$$\n",
    "is distributed as a standard Gaussian (mean = 0, variance = 1).\n",
    "\n",
    "In this section, we will explore these properties for the MLE. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2.3.1 (1pt)** Write a function that takes as input a dataset and returns the MLE by combining the code you already wrote in exercise 2.2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To study the randomness of the estimator $\\hat{\\alpha}$ as a function of $n$, we need to generate many experiments at a fixed size $n$, and study the mean and standard deviation of the corresponding MLEs.\n",
    "\n",
    "**2.3.2 (2pt)** For ```n in np.arange(10,10000,200)```:\n",
    "- generate $k=50$ datasets with $\\alpha_* = 20$ and $n$ measurements.\n",
    "- Compute the MLE for each of the $k$ datasets, and store the $k$ MLE obtained in a vector.\n",
    "- Compute the empirical mean and empirical standard deviation of the sample of the $k$ MLE obtained.\n",
    "\n",
    "Store all the empirical means and standard deviations in two respective vectors. \n",
    "The two vectors obtained should have the same length of ```ns```: check this by priting the lengths of the three vectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2.3.3 (1pt)** Plot the empirical means vs ns on one plot, and the standard deviations vs ns on **another** plot. \n",
    "For the plot of the mean, add a horizontal line at $y=\\alpha_*$.\n",
    "For the plot of the standard deviation, set the scale of the plot to log-log. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2.3.4 (3pt)** Discuss:\n",
    "- is the mean behaving according to the law of large numbers?\n",
    "- is the standard deviation behaving according to the central limit theorem? Describe qualitatively the meaning of the slope of the plot when log-log scale is used, and relate this to the rate of decay of the standard deviation predicted by the CLT."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Your answer here:**"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The CLT gives us more detailed informations about the distribution of the standardised MLE, saying that the distribution is a standard Gaussian. Let us verify that.\n",
    "\n",
    "**2.3.5 (2pt)** For ```n in ns=[15,30,50,100]```:\n",
    "- generate $k=1000$ datasets with $\\alpha_* = 20$ and $n$ points.\n",
    "- Compute the MLE for each of the $k$ datasets, and store the $k$ MLE obtained in a vector.\n",
    "- Compute the empirical mean and standard deviation of the $k$ MLE obtained.\n",
    "- Standardise the vector of the $k$ MLE, i.e. subtract the mean and divide by the standard deviation from each MLE.\n",
    "- Plot the histogram of the $k$ standardised MLEs. Use 50 bins, add a label = n, and use ```density = True``` (to plot frequencies) and ```alpha = 0.5``` to use a semi-transparent filling color.\n",
    "\n",
    "All histograms should appear on the same plot, and the plot should have a legend.\n",
    "On the same plot, draw the p.d.f. of a standard gaussian for $-5 < x < 5$ in black.\n",
    "\n",
    "Hint: modify your code from 2.3.2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2.3.6 (1pt)** Comment the previous plot. Is the central limit theorem verified? Motivate your answer. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Your answer here:**"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4 Comparison with other estimators"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section, we compare the MLE with two other estimators, the empirical mean and the empirical median of the measurements."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Empirical mean\n",
    "\n",
    "The simplest estimator we could have considered for this estimation task is simply the empirical average of the measurement\n",
    "$$\n",
    "    \\hat{x} = \\frac{1}{n} \\sum_{i=1}^n x_i \\, .\n",
    "$$"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2.4.1 (1pt)** Compute the empirical average of the dataset we used in exercise 2.2. Comment: Is it as good as the MLE? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Your answer here:**"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To study the randomness of the empirical mean estimator as a function of $n$, we need to generate many experiments at a fixed size $n$, and study the mean and standard deviation of the corresponding empirical mean estimators. We did this already in Exercise 2.3 for the MLE.\n",
    "\n",
    "**2.4.2 (1pt)** \n",
    "For ```np.arange(10,10000,200)```:\n",
    "- generate $k=50$ datasets with $\\alpha_* = 20$ and $n$ measurements.\n",
    "- Compute the **empirical mean estimator** for each of the $k$ datasets, and store the $k$ estimators obtained in a vector\n",
    "- Compute the empirical mean and standard deviation of the $k$ **empirical mean estimators** obtained\n",
    "\n",
    "Store all the means and standard deviations in two respective vectors. \n",
    "\n",
    "Then plot the means vs ns on one plot, and the standard deviations vs ns on **another** plot. \n",
    "For the mean plot, add a horizontal line at $y=\\alpha_*$.\n",
    "For the standard deviation plot,  set the scale of the plot to log-log.\n",
    "\n",
    "Hint: you can reuse code from exercise 2.3.2 and 2.3.3 with minimal modifications."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2.4.3 (2pt)** Discuss the previous plots.\n",
    "\n",
    "1. Does the standard deviation go to zero? \n",
    "2. Given your answer to the previous question, is a law of large numbers holding for the empirical mean estimator in this case? "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Your answer here:**"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**A comment:** in class we saw that the empirical mean estimator **does converge** to the true mean of the $\\rho_x(x)$ distribution, which seems incompatible with our observations in 2.4.2. and 2.4.3.. There must be some assumption that we did in class that does not hold here. \n",
    "Indeed, the mean of the p.d.f. $\\rho_x$ is given by the integral\n",
    "$$\n",
    "    \\mathbb{E}(x) \n",
    "    = \\int_{-\\infty}^{+\\infty} dx \\, \\frac{x}{\\pi(1+(x - \\alpha_*)^2)}\n",
    "    = \\alpha_* + \\int_{-\\infty}^{+\\infty} dy \\, \\frac{y}{\\pi(1+y^2)}\n",
    "$$\n",
    "where we used the change of variable $y = x - \\alpha_*$ and the fact that $\\rho_x$ is normalised.\n",
    "The last integral does not exists, as (recalling the definition of improper integral) the limit\n",
    "$$\n",
    "    \\lim_{R_1, R_2 \\to +\\infty} \\int_{-R_1}^{+R_2} dy \\, \\frac{y}{1+y^2}\n",
    "$$\n",
    "does not converge. \n",
    "You can see this by noticing that if one takes the limit $R_2 \\to +\\infty$ first, the integral diverges to $+\\infty$, while if one takes the limit $R_1 \\to +\\infty$ first, the integral diverges to $-\\infty$.\n",
    "\n",
    "Now, recall that the existence of the mean of a p.d.f. is a necessary condition for a LLN to hold. \n",
    "This explains why a LLN is not holding here."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Empirical median"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another simple estimator we could have looked at is the median of the measurements, i.e.\n",
    "$$\n",
    "    x_{\\rm median} = \\text{Median}(x_1, \\dots,x_n) \\, .\n",
    "$$\n",
    "Recall that the empirical median is the element $x_i$ such that half of the other measurements are smaller then $x_i$, and half of the other measurements are bigger than $x_i$.\n",
    "\n",
    "**2.4.4. (1pt)** Compute the empirical **median** of the dataset we used in exercise 2.2 (there is a numpy function to do that). Is it as good as the MLE? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Your answer here:**"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To study the randomness of the empirical median estimator as a function of $n$, we need to generate many experiments at a fixed size $n$, and study the mean and standard deviation of the corresponding empirical median estimators. We did this already in 2.3.2, 2.3.3 and 2.4.2 for the MLE and for the empirical mean estimator.\n",
    "\n",
    "**2.4.5. (1pt)** \n",
    "\n",
    "For ```n in ns=np.arange(10,10000,200)```:\n",
    "- generate $k=50$ datasets with $\\alpha_* = 20$ and $n$ measurements.\n",
    "- Compute the **empirical median** estimator for each of the $k$ datasets, and store the $k$ estimators obtained in a vector.\n",
    "- Compute the empirical mean and standard deviation of the $k$ **empirical median** estimators obtained.\n",
    "\n",
    "Store all the means and standard deviations in two respective vectors. \n",
    "\n",
    "Then plot the means vs ns on one plot, and the standard deviations vs ns on **another** plot. \n",
    "For the mean plot, add a horizontal line at $y=\\alpha_*$.\n",
    "For the standard deviation plot, set the the scale of the plot to log-log. \n",
    "\n",
    "Hint: you can reuse code from 2.3.2, 2.3.3 and 2.4.2 with minimal modifications."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2.4.6. (2pt)** Discuss the plots above:\n",
    "1. Is the empirical median estimator converging to something? If so, to which value?\n",
    "2. Is the standard deviation converging to zero? If so, can you give a qualitative estimate of the rate of decay?"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Your answer here:**"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2.4.7. (1pt)** Can you find the reason why the empirical median estimator seems to work? \n",
    "\n",
    "Hint: what is the median of $\\rho_x$?"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Your answer here:**"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**A comment:** seeing that the median works so well, you might wonder why don't we use that always instead of doing maximum likelihood estimation. The reason is that the median works here only because the median of the distribution of the measurements $\\rho_x$ is specially related to $\\alpha_*$, which is not at all always the case! So the median works nicely only in a limited amount of cases, while MLE will always work."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
