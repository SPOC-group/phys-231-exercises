{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "872deae2-9654-4e0f-9c81-d3471c316ae2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy import special\n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fb43d0e-97b6-4f78-acba-104b89518fbe",
   "metadata": {},
   "source": [
    "# A toy model for attention"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "692ea50f-3ac1-40da-8092-c6bf55358405",
   "metadata": {},
   "source": [
    "## General instructions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "712c6c1d-f49b-4d30-b583-c6a487d0301d",
   "metadata": {},
   "source": [
    "Each question is provided in a _Markdown_ cell and should be answered in the cell(s) below. You may add new cells if needed. All figures must be generated and shown directly in this notebook. If a question demands that you write an answer, use a _Markdown_ cell, which can include latex between \\\\$ symbols. As an example,\n",
    "\\\\$\\vec{F}=m\\vec{a}\\\\$\n",
    "gives $\\vec{F}=m\\vec{a}$.\n",
    "\n",
    "Your code should run properly if you do the following: 1) restart the kernel 2) execute all cells in order from top to bottom. Running all cells should take a reasonable time on a standard computer (<10 min.).\n",
    "\n",
    "Avoid using `for` loops whenever possible. Instead, use vectorized operations or numpy functions.\n",
    "\n",
    "All external sources you consult must be explicitly cited, except for the official NumPy, Scipy and Matplotlib documentation, the lecture notes, and previous exercises. You are encouraged to use external sources, since every function needed in this exercise has not necessarily been seen in the previous exercises. Please also cite every person you discussed this exercise with.\n",
    "\n",
    "Overly long or unnecessarily complicated answers will be penalized."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e41c5b24",
   "metadata": {},
   "source": [
    "## Setting"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54392a56-73da-4084-852f-2e8c8bd12605",
   "metadata": {},
   "source": [
    "In this exercise, we illustrate how an attention mechanism can focus on a few relevant tokens (*fragments*). We consider a simple model of attention trained with gradient descent on a small dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bf59845-01ac-443d-a2e4-c724de57eb77",
   "metadata": {},
   "source": [
    "We consider $N$ training samples. Each sample, indexed by $\\mu = 1, \\ldots, N$, is a sequence of $L$ tokens. Each token, indexed by $\\ell = 1, \\ldots, L$, is a $D$-dimensional vector in $\\mathbb{R}^D$. We also have $N'$ test samples, indexed by $\\mu = N+1, \\ldots, N+N'$. We pack all these samples into the tensors $X^\\mathrm{train}\\in\\mathbb{R}^{N\\times L\\times D}$, $X^\\mathrm{test}\\in\\mathbb{R}^{N'\\times L\\times D}$ and $X\\in\\mathbb{R}^{(N+N')\\times L\\times D}$. $X$ is a short notation for the concatenation of $X^\\mathrm{train}$ and $X^\\mathrm{test}$.\n",
    "\n",
    "For each sample $\\mu=1,\\ldots,N+N'$ there is a relevant token $\\epsilon_\\mu\\in\\{1,\\ldots,L\\}$ whose first component contains all the information we want to extract. More precisely, for each sample $\\mu$ its label $y_\\mu\\in\\mathbb R$ is\n",
    "$$\n",
    "y_\\mu=X_{\\mu,\\epsilon_\\mu,1}+\\Xi_{\\mu}\n",
    "$$\n",
    "where $\\Xi_{\\mu}$ is some small noise, independent across samples. We define the train and test labels as $y^\\mathrm{train}=(y_\\mu)_{\\mu=1,\\ldots,N}$ and $y^\\mathrm{test}=(y_\\mu)_{\\mu=N+1,\\ldots,N+N'}$. We want to predict the test labels given $X$ and the train labels. The main difficulty is that we do not know $\\epsilon_\\mu$.\n",
    "\n",
    "We consider an estimator that tends to focus on the most relevant tokens by estimating $\\epsilon_\\mu$. For this reason, we call it attention. It is defined by\n",
    "$$\n",
    "\\hat y_\\mu(k)=\\sum_{\\ell=1}^L\\sigma(\\chi_\\mu)_\\ell X_{\\mu,\\ell,1}\\ ,\\qquad \\mathrm{where}\\qquad \\chi_\\mu=\\frac{1}{\\sqrt D}X_{\\mu}k \\in\\mathbb R^L \\qquad \\mathrm{and}\\qquad k\\in\\mathbb R^D\\ .\n",
    "$$\n",
    "$\\sigma : \\mathbb{R}^L \\to \\mathbb{R}^L$ is a fixed function, which we initially take to be the identity. $k$ are the parameters of the attention that have to be trained. Intuitively, $\\chi_{\\mu,\\ell}$ indicates how likely it is that the token $\\ell$ is the relevant token. We transform this score via $\\sigma$ and then take a linear combination of the tokens, weighted by these scores. If the attention perfectly identifies the relevant token—i.e., $\\sigma(\\chi_\\mu)\\ell = 1$ when $\\ell = \\epsilon_\\mu$ and $0$ otherwise—then $\\hat y_\\mu$ will match $y_\\mu$, up to the noise term $\\Xi_\\mu$.\n",
    "\n",
    "We train this estimator with the following quadratic loss:\n",
    "$$\n",
    "\\mathcal L(k) = \\frac{1}{2}\\sum_{\\mu=1}^N(y_\\mu-\\hat y_\\mu(k))^2\n",
    "$$\n",
    "The parameter vector $k$ is optimized on the training data using gradient descent to minimize $\\mathcal{L}(k)$.\n",
    "Once the optimized parameter $\\hat k$ is found, we evaluate the performance on the test set using the mean squared test error $E$:\n",
    "$$\n",
    "E = \\frac{1}{N'}\\sum_{\\mu=N+1}^{N+N'}(y_\\mu-\\hat y_\\mu(\\hat k))^2\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f2732db",
   "metadata": {},
   "source": [
    "## 1. Preprocessing the data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbbead95-729c-4491-bbfd-d7a82e659df3",
   "metadata": {},
   "source": [
    "The following cell loads the training and test data: $X^\\mathrm{train}$, $X^\\mathrm{test}$, $y^\\mathrm{train}$, and $y^\\mathrm{test}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "367e82ba-55f3-4997-b368-baf774f0201f",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = np.load(\"dataX.npy\")\n",
    "X_test = np.load(\"dataX_test.npy\")\n",
    "y_train = np.load(\"dataY.npy\")\n",
    "y_test = np.load(\"dataY_test.npy\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e8b9afd-9356-4a74-a448-64a29c784092",
   "metadata": {},
   "source": [
    "**1.1** Instantiate the variables `N`, `L` and `D` with their values obtained from `X_train`. Print their values. Also print `N'`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eae33004-924e-42e6-9122-1f1e0b30af37",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a770f7b-9d19-4402-ac16-86705582f98f",
   "metadata": {},
   "source": [
    "## 2. Loss and gradient"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41c56ce8-2a08-4ea8-a791-501f901fe0b9",
   "metadata": {},
   "source": [
    "Unless stated otherwise, we take $\\sigma(\\chi) = \\chi$ (the identity function). We will consider other cases later."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e22d244",
   "metadata": {},
   "source": [
    "**2.1** Write a function `attentionLin` that takes $X$ and $k$ as input and outputs $\\hat y$. The function must work with an arbitrary number of samples. The output $\\hat y$ must be a one-dimensional numpy array whose length is the number of samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be328a3b-dc8f-48df-8d90-4e19deaa261f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f8fb36c-1587-488a-a06b-ee021cf381f1",
   "metadata": {},
   "source": [
    "**2.2** Write a function `loss` that computes $\\mathcal L$. It takes as input the NumPy arrays $y^\\mathrm{train}$ and $\\hat y^\\mathrm{train}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46dee3ae-8f65-439f-9528-ba166ba4dc78",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a021d9a-bf97-44ea-b600-02384744b25b",
   "metadata": {},
   "source": [
    "**2.3** We now compute the gradient of the loss with respect to $k$,\n",
    "$\\nabla_k \\mathcal{L}(k) \\in \\mathbb{R}^D$. We define the gradient with respect to $k$ of a function $f$ to be\n",
    "$$\n",
    "\\nabla_kf = \\left(\\frac{\\partial}{\\partial k_i}f\\right)_{i=1,\\ldots,D}\\ .\n",
    "$$\n",
    "We proceed step by step. First compute $\\nabla_k\\chi_\\mu\\in\\mathbb R^{L\\times D}$. Write your result in one line in the following cell."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "690559be-1974-4af0-84dd-2d5a9ac2e2f6",
   "metadata": {},
   "source": [
    "Your answer here :"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e07c6e5f-a722-46ac-a48d-f3fe721b7433",
   "metadata": {},
   "source": [
    "**2.4** We remind that $\\sigma$ is the identity, i.e. $\\hat y_\\mu=\\sum_{\\ell=1}^L\\chi_{\\mu,\\ell} X_{\\mu,\\ell,1}$. Compute $\\nabla_k\\hat y_\\mu\\in\\mathbb R^D$. Write your result in one or two lines in the following cell."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8904345-3a4d-45cd-9010-871bf59ecb94",
   "metadata": {},
   "source": [
    "Your answer here:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a64b14e0-b78e-42dd-a198-5967e91ee20a",
   "metadata": {},
   "source": [
    "**2.5** Compute $\\nabla_k\\mathcal L$ using the chain rule. Write your result in one or two lines in the following cell."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d06d9360-caeb-4d9d-95ad-f08ed96eded1",
   "metadata": {},
   "source": [
    "Your answer here:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f55121e7-4d11-4693-b3fe-e06f5793b867",
   "metadata": {},
   "source": [
    "**2.6** Write a function `gradLossLin` that computes $\\nabla_k\\mathcal L(k)$. It takes as input the NumPy arrays $X^\\mathrm{train}$, $y^\\mathrm{train}$ and $k$. The output must be a one-dimensional numpy array. \n",
    "\n",
    "_Hint_ : to perform a summation over various indices you can use `np.einsum`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f33eda16-e4a9-4167-91c7-3502c9def471",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65a96ba9-3c0d-40ad-ac0f-6be25680d04a",
   "metadata": {},
   "source": [
    "**2.7** Check that your gradient makes sense by comparing it to finite differences: for any $k$ and any small perturbation $\\delta\\in\\mathbb R^D$ you should have\n",
    "$$\n",
    "2\\nabla_k\\mathcal L(k)^\\top\\delta \\approx \\mathcal L(k+\\delta)-\\mathcal L(k-\\delta).\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a074206-b13f-4c27-bfaf-b63ac8248cb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "559d14a4-b845-48b6-93fa-d8a8e9f3af26",
   "metadata": {},
   "source": [
    "## 3. Gradient descent"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5082b39d-9bb2-4086-95d4-88c304fab241",
   "metadata": {},
   "source": [
    "We can start implementing gradient descent to estimate $\\hat k$ that minimizes $\\mathcal L$. We initialize the descent at a random $k^{(0)}$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5256e0b",
   "metadata": {},
   "source": [
    "**3.1** Introduce the learning rate and formally write, in one line, a single iteration of gradient descent updating $k$ from time step $t$ to time step $t+1$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d9c84ab-95ac-4cf2-8dd4-4eff1d989646",
   "metadata": {},
   "source": [
    "Your answer here:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d07e689-d4a5-4299-b8d5-c1ff16fd2199",
   "metadata": {},
   "source": [
    "**3.2** Implement the gradient descent. Initialize the descent at a random $k^{(0)}$ and perform enough iterations so the descent converges. We define convergence when $|\\mathcal L(k^{(t+1)})-\\mathcal L(k^{(t)})|<10^{-2}$ is reached. You may need to adjust the learning rate to ensure convergence. In any case you should not need to go beyond $5\\times 10^3$ time steps. You can use a `for` or `while` loop. At each iteration, compute and store both the loss $\\mathcal L$ and the error $E$ in two separate lists. To help with debugging, you can print the value of the loss at each iteration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bee06c11-8330-4af7-8a47-abfd2d73271d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec1b7dd3-d271-451c-9f9b-c2a06893c274",
   "metadata": {},
   "source": [
    "**3.3** Plot the evolution of $\\mathcal L$ across iterations. Label the axes, and use a logarithmic scale for the x-axis (number of iterations)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71a4ba5c-8cf4-49b9-98d1-0a8720d746d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b07b724-60ed-4fcb-beba-30c8f182f008",
   "metadata": {},
   "source": [
    "**3.4** Plot $E$ at each iteration. Add labels to the axes. For the x-axis (number of iterations), use a logarithmic scale. Should we do early stopping ? Explain briefly. Print the achieved error.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc10ffa5-07b7-4de5-b988-0cea7a2cee68",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18528b68",
   "metadata": {},
   "source": [
    "Your answer here:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a44e8f5c-ba1b-442f-ad3d-c5b4c9e7cb6c",
   "metadata": {},
   "source": [
    "## 4. Softmax attention"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22d1f994-e633-440e-ad20-75f76d0ecce2",
   "metadata": {},
   "source": [
    "In this part we consider another $\\sigma$ : the softmax attention. It is defined for $\\chi_\\mu\\in\\mathbb R^L$ by\n",
    "$$\n",
    "\\sigma(\\chi_\\mu)_\\ell=\\frac{e^{\\chi_{\\mu,\\ell}}}{\\sum_{k=1}^Le^{\\chi_{\\mu,k}}}\\ , \\qquad\\mathrm{for\\ }\\ell=1,\\ldots,L\\ .\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12fc9c30",
   "metadata": {},
   "source": [
    "**4.1** Implement the softmax attention $\\sigma$. It takes in input a two-dimensional numpy array $\\chi\\in\\mathbb R^{N\\times L}$ and outputs a numpy array of the same size. It performs the softmax operation independently for each $\\chi_\\mu\\in\\mathbb R^L$, for $\\mu=1,\\ldots,N$. Do not use a built-in function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0937892b-44be-454c-b345-cda1e0681905",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b812c580-79be-4183-a592-81406f618c9e",
   "metadata": {},
   "source": [
    "**4.2** Take $N=1, L=3$ and compute $\\sigma((1,1,1))$, $\\sigma((3,1,1))$ and $\\sigma((10,1,1))$. Why is this function called a softmax ? Explain briefly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd3d3940-f4c8-4015-b41c-35d812de29a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f405045e",
   "metadata": {},
   "source": [
    "Your answer here:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d4251d2-e53c-4bdb-b812-c8260fbed4fd",
   "metadata": {},
   "source": [
    "**4.3** Would it be possible to run gradient descent with the following $\\sigma$ ?\n",
    "$$\n",
    "\\sigma(\\chi_\\mu)_\\ell=\\left \\{\\begin{array}{cl}\n",
    "1 & \\mathrm{if\\ }\\chi_{\\mu,\\ell}=\\mathrm{max}(\\chi_\\mu) \\\\\n",
    "0 & \\mathrm{else}\n",
    "\\end{array} \\right .\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9502500-f775-4db3-9119-2d424a6010f0",
   "metadata": {},
   "source": [
    "Your answer here:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b44570b-8031-40fe-a5d5-9e609777422b",
   "metadata": {},
   "source": [
    "**4.4** Implement a function `attentionSoftmax` that computes $\\hat y$ given $X$ and $k$. The function must work with an arbitrary number of samples. The output $\\hat y$ must be a one-dimensional numpy array whose length is the number of samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95af9b4d-5933-45ec-9185-9beb1edca0f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e725725-ad21-42a1-b00b-a0bc510316c9",
   "metadata": {},
   "source": [
    "In the following cell we give a function that computes $\\nabla_k\\mathcal L$ for the softmax attention. Note that we use your implementation of `attentionSoftmax` that you defined above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "355d5f4e-44ed-4fd9-96d7-91e39d2840a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradLossSoftmax(X, y, k):\n",
    "    \"\"\"\n",
    "    X : (N, L, D) numpy array\n",
    "    y : (N,) numpy array\n",
    "    k : (D,) numpy array\n",
    "    \"\"\"\n",
    "    yC = attentionSoftmax(X, k) # here we use your implementation of the attention\n",
    "    chis = X@k/np.sqrt(D)\n",
    "    dsoftmax = special.softmax(chis, axis=-1)[:,np.newaxis,:]*(np.identity(L)[np.newaxis,:,:]-special.softmax(chis, axis=-1)[:,:,np.newaxis])\n",
    "    return np.einsum(\"n,nl,nlk,nki->i\", yC-y, X[:,:,0], dsoftmax, X)/np.sqrt(D)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "038dd777-4534-4c0e-8dd9-aa80b896ce98",
   "metadata": {},
   "source": [
    "**4.5** Implement gradient descent for the softmax attention in the same way as in the previous section."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adcbf0a9-edf9-4d40-98e3-6cbbdf12bb03",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb0e0086-e7fb-4471-8785-c609575b3410",
   "metadata": {},
   "source": [
    "**4.6** Plot $\\mathcal L$ as a function of the iteration number. Label both axes, and use a logarithmic scale for the x-axis (number of iterations)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d95a94e3-bb7a-4c9a-ab9d-592fe72e7974",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "662013c6-82d1-493d-a561-0bb94faa9d02",
   "metadata": {},
   "source": [
    "**4.7** Plot the evolution of $E$ over iterations. Add labels to the axes. For the x-axis (number of iterations), use a logarithmic scale. Should we do early stopping ? Explain briefly. Print the minimal achieved error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7c9ef5c-cee3-427e-8615-e58a5c8cb4be",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "477d5f62",
   "metadata": {},
   "source": [
    "Your answer here:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad690826-806e-42f0-8704-3efad6fde78c",
   "metadata": {},
   "source": [
    "**4.8** Does the softmax attention or linear attention have better performance when comparing $E$ ?  In a few lines give an intuition why."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2689f4b-fcca-4c3b-9d6e-146700cf48a3",
   "metadata": {},
   "source": [
    "Your answer here:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bad7d7de-0a0e-4af4-bde8-5fbcc4f8cbb4",
   "metadata": {},
   "source": [
    "## 5. Application : attention for sentiment analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a6099e7-5301-4c9a-aa5e-c67e65d4df1f",
   "metadata": {},
   "source": [
    "We are not going to build ChatGPT, but at least, we will see, what are some important principles behind it. One of the main components of a Transformer model, such as ChatGPT is the attention module. It compares the tokens in the sequence (understand as words in the sentence) between each other to only ``attend'' to the relevant tokens for each position in the sequence.\n",
    "\n",
    "Let's take a look at an example: an attention module that classifies positive vs negative tweets. This task is called sentiment analysis. In the following cell we load the tweets together with their labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "697c6fcd-feb3-4d5f-a598-bca55b777989",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataTxt = np.loadtxt('tweets_clean.csv', delimiter=',', skiprows=1, usecols=0, dtype=str)\n",
    "dataY = np.loadtxt('tweets_clean.csv', delimiter=',', skiprows=1, usecols=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a71bcea-e5a5-45bf-91e5-aa4b812a6d46",
   "metadata": {},
   "source": [
    "- Print the first ten samples and their labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a76035a-6603-493d-86aa-f7ea9b1e7513",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1203ec1d-bbcc-4686-82d7-8c8a3616b29d",
   "metadata": {},
   "source": [
    "### Embeddings\n",
    "\n",
    "Now, we need to tokenize (*fragmenter*) the text, that is, split sentences into tokens. In common LLMs one token is a piece of a word ; in our case we consider that one token is exactly one word, separated by white spaces.\n",
    "\n",
    "Then each token should be embedded (*plonger*) to a vector in $\\mathbb{R}^D$, so that we can train a model on this data. Each sample $\\mu$ will consist of a sequence of $L$ embeddings: $X^\\mu = (X^\\mu_1, \\ldots, X^\\mu_L)\\in\\mathbb{R}^{L\\times D}$.\n",
    "\n",
    "To embed the words we will use the GloVe model https://nlp.stanford.edu/projects/glove/. We provide a file ``embeddings.csv``, which contains only a part of the glove embeddings that is required to encode the given tweets, and we load them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "676accc4-42bd-4c0e-a0eb-01f82fb2b284",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_glove_embeddings():\n",
    "    embeddings_dict = {}\n",
    "    with open(\"embeddings.csv\", 'r', encoding=\"utf-8\") as f:\n",
    "        for line in f:\n",
    "            values = line.split(\",\")\n",
    "            word = values[0]\n",
    "            vector = np.asarray(values[1:], \"float32\")\n",
    "            embeddings_dict[word] = vector\n",
    "    \n",
    "    return embeddings_dict\n",
    "\n",
    "embeddings_dict = get_glove_embeddings()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97e60575-0e58-4969-9bad-8148491ee409",
   "metadata": {},
   "source": [
    "**5.1** Print the embedding of the token `apple`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc79efad-95eb-47fe-8bfd-73bba8887978",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8d32563-bc3d-4ea0-a0e9-61796c294ca2",
   "metadata": {},
   "source": [
    "The embeddings encode the words so that the semantic structure is preserved. We give a few examples."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a501f820",
   "metadata": {},
   "source": [
    "**5.2** Write a function `find_closest_embeddings` that, given an embedding $X_{\\mu,\\ell}\\in\\mathbb R^D$, sorts all the embeddings in the dictionary `embeddings_dict` by how close they are to $X_{\\mu,\\ell}$. To measure \"closeness\" use Euclidean norm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bebd154-f0dd-4b40-a976-3f362f4affb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1aab70bf-0366-4498-a94a-f8a92e85302a",
   "metadata": {},
   "source": [
    "Your code should work on the following examples of semantic structure :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a28788df-3d05-4b38-be54-c9d93af4bf33",
   "metadata": {},
   "outputs": [],
   "source": [
    "find_closest_embeddings(embeddings_dict[\"king\"])[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96541350-e0aa-4548-891e-8686f55dd0f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "find_closest_embeddings(embeddings_dict[\"smarter\"] - embeddings_dict[\"smart\"] + embeddings_dict[\"strong\"])[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e82c947a-5188-40a3-b4c2-d972562430c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "find_closest_embeddings(embeddings_dict[\"king\"] - embeddings_dict[\"queen\"] + embeddings_dict[\"woman\"])[:2]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bea7c5a-f074-44ed-8f41-5314db3fd399",
   "metadata": {},
   "source": [
    "In the following we process the tweets : we tokenize, embed and split them in train and test sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d44d86e0-4731-4d2d-8012-e479b7c868a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def embed_tweet(tweet):\n",
    "    embedding = [embeddings_dict[word] for word in tweet.split()]\n",
    "    while len(embedding) < 16:\n",
    "        embedding.append(np.zeros_like(embeddings_dict['0']))\n",
    "    return embedding[:16]\n",
    "\n",
    "X = np.stack([embed_tweet(tweet) for tweet in dataTxt])\n",
    "\n",
    "N, Np = int(N*2/3), int(N/3)\n",
    "X_train, X_test = X[:N,:,:], X[N:,:,:]\n",
    "y_train, y_test = dataY[:N], dataY[N:]\n",
    "\n",
    "N, L, D = X_train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4014c885-ccdc-4560-a7bc-d7fe5069752a",
   "metadata": {},
   "source": [
    "### Attention estimator, gradient descent"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0a4de96-106e-4e5a-b0c6-6d24c08b7080",
   "metadata": {},
   "source": [
    "We now consider the following estimator :\n",
    "$$\n",
    "\\hat{y}_\\mu(k, v) = \\frac{1}{\\sqrt D}v^T\\sum_{\\ell=1}^L\\sigma(\\chi_\\mu)_\\ell X_{\\mu, \\ell} ,\\qquad \\mathrm{where}\\qquad \\chi_\\mu=\\frac{1}{\\sqrt D}X_{\\mu}k \\in\\mathbb R^L \\qquad \\mathrm{and}\\qquad k, v\\in\\mathbb R^D.\n",
    "$$\n",
    "$\\sigma : \\mathbb{R}^L \\to \\mathbb{R}^L$. The vectors $k$ and $v$ are the parameters of the model that have to be trained. Compared to the previous sections we do not know how the embeddings and the labels are related. We assume that the sentiment of the token $\\ell$ can be expressed as $v^\\top X_{\\mu,\\ell}$ for a good $v$, that has to be learnt. The sentiments of all the tokens are then summed with a ponderation given by the attention. The previous sections correspond to the special case $v=(1,0,\\ldots,0)$.\n",
    "\n",
    "To keep things simpler we still train the estimator using the quadratic loss (mean square error):\n",
    "$$\n",
    "\\mathcal L(k,v) = \\frac{1}{2}\\sum_{\\mu=1}^N(y_\\mu-\\hat y_\\mu(k,v))^2\n",
    "$$\n",
    "We minimise $\\mathcal L$ over $k$ and $v$ using gradient descent. Once the optimized parameters $\\hat k, \\hat v$ are found, the predicted class of the tweet $\\mu$ is $\\mathrm{sign}(\\hat y_\\mu(\\hat k, \\hat v))$, and we evaluate the performance on the test set using the test accuracy $\\mathrm{Acc}$ :\n",
    "$$\n",
    "\\mathrm{Acc} = \\frac{1}{2}+\\frac{1}{2N'}\\sum_{\\mu=N+1}^{N+N'}y_\\mu\\mathrm{sign}(\\hat y_\\mu(\\hat k, \\hat v))\\ .\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f95d192-cc48-4962-b063-a0aca3c6b39a",
   "metadata": {},
   "source": [
    "**5.3** Introduce the learning rate and formally write, in two lines, a single iteration of gradient descent updating $k$ and $v$ from time step $t$ to time step $t+1$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d38aed5a-c0ab-4b17-ad56-a7c71e2f2443",
   "metadata": {},
   "source": [
    "Your answer here:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "782a6e84-cbf9-4474-b8e5-b60569ed0b6d",
   "metadata": {},
   "source": [
    "**5.4** In one line compute $\\nabla_v \\hat y_\\mu$. In one line compute $\\nabla_v \\mathcal L$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dead9404-b566-41db-af45-741e48847af8",
   "metadata": {},
   "source": [
    "Your answer here:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30280e2b-8f3e-447a-abd5-486681005191",
   "metadata": {},
   "source": [
    "**5.5** We assume that the attention is $\\sigma(\\chi)=\\chi$. Remark that instead of $X_{\\mu,\\ell,1}$ we have now $D^{-1/2}v^\\top X_{\\mu,\\ell}$. Using your previous results, in one line compute $\\nabla_k\\mathcal L$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94891b77-d370-48e5-8111-dff25dba613b",
   "metadata": {},
   "source": [
    "Your answer here:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f761028-bc9c-4193-9166-08ca0aef9f92",
   "metadata": {},
   "source": [
    "We give an implementation of the linear and softmax attentions and of their gradients."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0249b377-73b7-41ff-ab76-91a30216c24a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def attentionLin(X, k, v):\n",
    "    chi = (X * k).sum(2) / np.sqrt(D)\n",
    "    return (X * chi.reshape(-1, L, 1)).sum(1) @ v / np.sqrt(D)\n",
    "\n",
    "def attentionSoftmax(X, k, v):\n",
    "    chis = (X * k).sum(2) / np.sqrt(D)\n",
    "    XTX = (X * special.softmax(chis, axis=-1).reshape(-1, L, 1)).sum(1)\n",
    "    return XTX @ v / np.sqrt(D)\n",
    "\n",
    "def gradLossLin(X, y, k, v):\n",
    "    yC = attentionLin(X, k, v)\n",
    "    chi = (X * k).sum(2) / np.sqrt(D)\n",
    "    XTX = np.transpose(X,(0, 2, 1))@X / D\n",
    "    return  ((XTX @ v) * (yC-y).reshape(-1, 1)).sum(0), (((X * chi.reshape(-1, L, 1)).sum(1) / np.sqrt(D)) * (yC-y).reshape(-1, 1)).sum(0)\n",
    "\n",
    "def gradLossSoftmax(X, y, k, v):\n",
    "    yC = attentionSoftmax(X, k, v)\n",
    "    chis = (X * k).sum(2) / np.sqrt(D)\n",
    "    activation = special.softmax(chis, axis=-1).reshape(-1, L, 1)\n",
    "    XTX = (X * activation).sum(1) / np.sqrt(D)\n",
    "    Xv_dif = ((X@v/ np.sqrt(D)) - yC.reshape(-1, 1)).reshape(-1, L, 1)\n",
    "    grad_y_k = (X * (Xv_dif * activation)).sum(1)/np.sqrt(D)\n",
    "    return  (grad_y_k * (yC-y).reshape(-1, 1)).sum(0), (XTX * (yC-y).reshape(-1, 1)).sum(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52aaeff4-bb16-42b8-9788-4e97603433b2",
   "metadata": {},
   "source": [
    "**5.6** Perform the gradient descent for both the linear attention and the softmax attention. Plot the test accuracy $\\mathrm{Acc}$ versus the iterations of the gradient descent. For the x-axis (number of iterations), use a logarithmic scale. Print the best accuracy you obtain for both models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0fe48fa-bddb-4da8-98d6-c779d510acd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c63bfb4c-82e5-487f-a9be-09d74ac85b0c",
   "metadata": {},
   "source": [
    "### Interpreting the results\n",
    "\n",
    "Now, let's take a look at the produced attention scores for both linear and softmax attention."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2943754f",
   "metadata": {},
   "source": [
    "**5.7** write a function `getScores` that takes a tweet string, the trained vector $k$ and a boolean flag ``apply_softmax`` as inputs and returns the attention scores $\\chi$ of the tweet. It should apply softmax to the scores if the ``apply_softmax`` flag is true and it should return the absolute values of the linear attention scores otherwise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "891a4ee3-0820-482b-a8d1-169c3e0a9a30",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a54028f-7924-4ebd-93dd-2d6ced02434e",
   "metadata": {},
   "source": [
    "We provide a function to plot linear and softmax attention scores $\\chi$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e4b161e-0097-4418-91f6-e999a963e426",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plotAttention(tweet, linear_scores, softmax_scores, ax):\n",
    "    L = min(len(tweet.split()), 16)\n",
    "    words = tweet.split()[:L][::-1]\n",
    "    linear_scores = linear_scores[::-1]\n",
    "    softmax_scores = softmax_scores[::-1]\n",
    "    \n",
    "    bar_height = 0.4\n",
    "\n",
    "    ax.barh(np.arange(L)-bar_height/2, linear_scores, bar_height, color=\"lightblue\", label=\"Linear attention\")\n",
    "    ax.set_xlabel(\"Linear attention scores\")\n",
    "    ax.set_yticks(np.arange(L))\n",
    "    ax.set_yticklabels(words)\n",
    "    leg = ax.legend()\n",
    "\n",
    "    ax2 = ax.twiny()\n",
    "\n",
    "    ax2.set_xlabel(\"Softmax attention scores\")\n",
    "    ax2.barh(np.arange(L)+bar_height/2, softmax_scores, bar_height, color=\"salmon\", label=\"Softmax attention\")\n",
    "    ax2.set_xlabel(\"Softmax attention scores\")\n",
    "\n",
    "    leg2 = ax2.legend()\n",
    "    ax2.legend(leg.get_patches()+leg2.get_patches(),\n",
    "            [text.get_text() for text in leg.get_texts()+leg2.get_texts()])\n",
    "    leg.remove()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27aa96b9-bd26-47c2-856a-3e7b79ba71bc",
   "metadata": {},
   "source": [
    "Now we can plot the attention scores for positive and negative examples with given indices."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6058ee13",
   "metadata": {},
   "source": [
    "**5.8** Complete the code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eafe0cde-ac05-411b-bf33-4871d457365d",
   "metadata": {},
   "outputs": [],
   "source": [
    "negative_ids = [1954, 1521, 1263]\n",
    "positive_ids = [1744, 176, 924]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f741b02-05b5-41b8-a9bc-49591d938c67",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, 3, figsize=(15, 4))\n",
    "for i in range(3):\n",
    "    tweet = dataTxt[negative_ids[i]]\n",
    "    linear_scores = # to be filled\n",
    "    softmax_scores = # to be filled\n",
    "    plotAttention(tweet, linear_scores, softmax_scores, axes[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d0d93d2-908a-447d-b097-2946492819ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, 3, figsize=(15, 4))\n",
    "for i in range(3):\n",
    "    tweet = dataTxt[positive_ids[i]]\n",
    "    linear_scores = # to be filled\n",
    "    softmax_scores = # to be filled\n",
    "    plotAttention(tweet, linear_scores, softmax_scores, axes[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "082ff3d4-3e4f-4267-a2cf-ffd1d599e94d",
   "metadata": {},
   "source": [
    "**5.9** What do you observe ?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "605388db-6b3a-4674-9229-011bd61c1fed",
   "metadata": {},
   "source": [
    "Your answer here:"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cedric_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  },
  "unianalytics_cell_mapping": [
   [
    "872deae2-9654-4e0f-9c81-d3471c316ae2",
    "872deae2-9654-4e0f-9c81-d3471c316ae2"
   ],
   [
    "0fb43d0e-97b6-4f78-acba-104b89518fbe",
    "0fb43d0e-97b6-4f78-acba-104b89518fbe"
   ],
   [
    "692ea50f-3ac1-40da-8092-c6bf55358405",
    "692ea50f-3ac1-40da-8092-c6bf55358405"
   ],
   [
    "712c6c1d-f49b-4d30-b583-c6a487d0301d",
    "712c6c1d-f49b-4d30-b583-c6a487d0301d"
   ],
   [
    "e41c5b24",
    "e41c5b24"
   ],
   [
    "54392a56-73da-4084-852f-2e8c8bd12605",
    "54392a56-73da-4084-852f-2e8c8bd12605"
   ],
   [
    "0bf59845-01ac-443d-a2e4-c724de57eb77",
    "0bf59845-01ac-443d-a2e4-c724de57eb77"
   ],
   [
    "6f2732db",
    "6f2732db"
   ],
   [
    "cbbead95-729c-4491-bbfd-d7a82e659df3",
    "cbbead95-729c-4491-bbfd-d7a82e659df3"
   ],
   [
    "367e82ba-55f3-4997-b368-baf774f0201f",
    "367e82ba-55f3-4997-b368-baf774f0201f"
   ],
   [
    "8e8b9afd-9356-4a74-a448-64a29c784092",
    "8e8b9afd-9356-4a74-a448-64a29c784092"
   ],
   [
    "eae33004-924e-42e6-9122-1f1e0b30af37",
    "eae33004-924e-42e6-9122-1f1e0b30af37"
   ],
   [
    "8a770f7b-9d19-4402-ac16-86705582f98f",
    "8a770f7b-9d19-4402-ac16-86705582f98f"
   ],
   [
    "41c56ce8-2a08-4ea8-a791-501f901fe0b9",
    "41c56ce8-2a08-4ea8-a791-501f901fe0b9"
   ],
   [
    "9e22d244",
    "9e22d244"
   ],
   [
    "be328a3b-dc8f-48df-8d90-4e19deaa261f",
    "be328a3b-dc8f-48df-8d90-4e19deaa261f"
   ],
   [
    "1f8fb36c-1587-488a-a06b-ee021cf381f1",
    "1f8fb36c-1587-488a-a06b-ee021cf381f1"
   ],
   [
    "46dee3ae-8f65-439f-9528-ba166ba4dc78",
    "46dee3ae-8f65-439f-9528-ba166ba4dc78"
   ],
   [
    "3a021d9a-bf97-44ea-b600-02384744b25b",
    "3a021d9a-bf97-44ea-b600-02384744b25b"
   ],
   [
    "690559be-1974-4af0-84dd-2d5a9ac2e2f6",
    "690559be-1974-4af0-84dd-2d5a9ac2e2f6"
   ],
   [
    "e07c6e5f-a722-46ac-a48d-f3fe721b7433",
    "e07c6e5f-a722-46ac-a48d-f3fe721b7433"
   ],
   [
    "d8904345-3a4d-45cd-9010-871bf59ecb94",
    "d8904345-3a4d-45cd-9010-871bf59ecb94"
   ],
   [
    "a64b14e0-b78e-42dd-a198-5967e91ee20a",
    "a64b14e0-b78e-42dd-a198-5967e91ee20a"
   ],
   [
    "d06d9360-caeb-4d9d-95ad-f08ed96eded1",
    "d06d9360-caeb-4d9d-95ad-f08ed96eded1"
   ],
   [
    "f55121e7-4d11-4693-b3fe-e06f5793b867",
    "f55121e7-4d11-4693-b3fe-e06f5793b867"
   ],
   [
    "f33eda16-e4a9-4167-91c7-3502c9def471",
    "f33eda16-e4a9-4167-91c7-3502c9def471"
   ],
   [
    "65a96ba9-3c0d-40ad-ac0f-6be25680d04a",
    "65a96ba9-3c0d-40ad-ac0f-6be25680d04a"
   ],
   [
    "1a074206-b13f-4c27-bfaf-b63ac8248cb8",
    "1a074206-b13f-4c27-bfaf-b63ac8248cb8"
   ],
   [
    "559d14a4-b845-48b6-93fa-d8a8e9f3af26",
    "559d14a4-b845-48b6-93fa-d8a8e9f3af26"
   ],
   [
    "5082b39d-9bb2-4086-95d4-88c304fab241",
    "5082b39d-9bb2-4086-95d4-88c304fab241"
   ],
   [
    "f5256e0b",
    "f5256e0b"
   ],
   [
    "0d9c84ab-95ac-4cf2-8dd4-4eff1d989646",
    "0d9c84ab-95ac-4cf2-8dd4-4eff1d989646"
   ],
   [
    "2d07e689-d4a5-4299-b8d5-c1ff16fd2199",
    "2d07e689-d4a5-4299-b8d5-c1ff16fd2199"
   ],
   [
    "bee06c11-8330-4af7-8a47-abfd2d73271d",
    "bee06c11-8330-4af7-8a47-abfd2d73271d"
   ],
   [
    "ec1b7dd3-d271-451c-9f9b-c2a06893c274",
    "ec1b7dd3-d271-451c-9f9b-c2a06893c274"
   ],
   [
    "71a4ba5c-8cf4-49b9-98d1-0a8720d746d0",
    "71a4ba5c-8cf4-49b9-98d1-0a8720d746d0"
   ],
   [
    "2b07b724-60ed-4fcb-beba-30c8f182f008",
    "2b07b724-60ed-4fcb-beba-30c8f182f008"
   ],
   [
    "bc10ffa5-07b7-4de5-b988-0cea7a2cee68",
    "bc10ffa5-07b7-4de5-b988-0cea7a2cee68"
   ],
   [
    "18528b68",
    "18528b68"
   ],
   [
    "a44e8f5c-ba1b-442f-ad3d-c5b4c9e7cb6c",
    "a44e8f5c-ba1b-442f-ad3d-c5b4c9e7cb6c"
   ],
   [
    "22d1f994-e633-440e-ad20-75f76d0ecce2",
    "22d1f994-e633-440e-ad20-75f76d0ecce2"
   ],
   [
    "12fc9c30",
    "12fc9c30"
   ],
   [
    "0937892b-44be-454c-b345-cda1e0681905",
    "0937892b-44be-454c-b345-cda1e0681905"
   ],
   [
    "b812c580-79be-4183-a592-81406f618c9e",
    "b812c580-79be-4183-a592-81406f618c9e"
   ],
   [
    "cd3d3940-f4c8-4015-b41c-35d812de29a5",
    "cd3d3940-f4c8-4015-b41c-35d812de29a5"
   ],
   [
    "f405045e",
    "f405045e"
   ],
   [
    "7d4251d2-e53c-4bdb-b812-c8260fbed4fd",
    "7d4251d2-e53c-4bdb-b812-c8260fbed4fd"
   ],
   [
    "a9502500-f775-4db3-9119-2d424a6010f0",
    "a9502500-f775-4db3-9119-2d424a6010f0"
   ],
   [
    "4b44570b-8031-40fe-a5d5-9e609777422b",
    "4b44570b-8031-40fe-a5d5-9e609777422b"
   ],
   [
    "95af9b4d-5933-45ec-9185-9beb1edca0f6",
    "95af9b4d-5933-45ec-9185-9beb1edca0f6"
   ],
   [
    "7e725725-ad21-42a1-b00b-a0bc510316c9",
    "7e725725-ad21-42a1-b00b-a0bc510316c9"
   ],
   [
    "355d5f4e-44ed-4fd9-96d7-91e39d2840a6",
    "355d5f4e-44ed-4fd9-96d7-91e39d2840a6"
   ],
   [
    "038dd777-4534-4c0e-8dd9-aa80b896ce98",
    "038dd777-4534-4c0e-8dd9-aa80b896ce98"
   ],
   [
    "adcbf0a9-edf9-4d40-98e3-6cbbdf12bb03",
    "adcbf0a9-edf9-4d40-98e3-6cbbdf12bb03"
   ],
   [
    "bb0e0086-e7fb-4471-8785-c609575b3410",
    "bb0e0086-e7fb-4471-8785-c609575b3410"
   ],
   [
    "d95a94e3-bb7a-4c9a-ab9d-592fe72e7974",
    "d95a94e3-bb7a-4c9a-ab9d-592fe72e7974"
   ],
   [
    "662013c6-82d1-493d-a561-0bb94faa9d02",
    "662013c6-82d1-493d-a561-0bb94faa9d02"
   ],
   [
    "b7c9ef5c-cee3-427e-8615-e58a5c8cb4be",
    "b7c9ef5c-cee3-427e-8615-e58a5c8cb4be"
   ],
   [
    "477d5f62",
    "477d5f62"
   ],
   [
    "ad690826-806e-42f0-8704-3efad6fde78c",
    "ad690826-806e-42f0-8704-3efad6fde78c"
   ],
   [
    "f2689f4b-fcca-4c3b-9d6e-146700cf48a3",
    "f2689f4b-fcca-4c3b-9d6e-146700cf48a3"
   ],
   [
    "bad7d7de-0a0e-4af4-bde8-5fbcc4f8cbb4",
    "bad7d7de-0a0e-4af4-bde8-5fbcc4f8cbb4"
   ],
   [
    "6a6099e7-5301-4c9a-aa5e-c67e65d4df1f",
    "6a6099e7-5301-4c9a-aa5e-c67e65d4df1f"
   ],
   [
    "697c6fcd-feb3-4d5f-a598-bca55b777989",
    "697c6fcd-feb3-4d5f-a598-bca55b777989"
   ],
   [
    "3a71bcea-e5a5-45bf-91e5-aa4b812a6d46",
    "3a71bcea-e5a5-45bf-91e5-aa4b812a6d46"
   ],
   [
    "3a76035a-6603-493d-86aa-f7ea9b1e7513",
    "3a76035a-6603-493d-86aa-f7ea9b1e7513"
   ],
   [
    "1203ec1d-bbcc-4686-82d7-8c8a3616b29d",
    "1203ec1d-bbcc-4686-82d7-8c8a3616b29d"
   ],
   [
    "676accc4-42bd-4c0e-a0eb-01f82fb2b284",
    "676accc4-42bd-4c0e-a0eb-01f82fb2b284"
   ],
   [
    "97e60575-0e58-4969-9bad-8148491ee409",
    "97e60575-0e58-4969-9bad-8148491ee409"
   ],
   [
    "bc79efad-95eb-47fe-8bfd-73bba8887978",
    "bc79efad-95eb-47fe-8bfd-73bba8887978"
   ],
   [
    "c8d32563-bc3d-4ea0-a0e9-61796c294ca2",
    "c8d32563-bc3d-4ea0-a0e9-61796c294ca2"
   ],
   [
    "a501f820",
    "a501f820"
   ],
   [
    "4bebd154-f0dd-4b40-a976-3f362f4affb0",
    "4bebd154-f0dd-4b40-a976-3f362f4affb0"
   ],
   [
    "1aab70bf-0366-4498-a94a-f8a92e85302a",
    "1aab70bf-0366-4498-a94a-f8a92e85302a"
   ],
   [
    "a28788df-3d05-4b38-be54-c9d93af4bf33",
    "a28788df-3d05-4b38-be54-c9d93af4bf33"
   ],
   [
    "96541350-e0aa-4548-891e-8686f55dd0f6",
    "96541350-e0aa-4548-891e-8686f55dd0f6"
   ],
   [
    "e82c947a-5188-40a3-b4c2-d972562430c1",
    "e82c947a-5188-40a3-b4c2-d972562430c1"
   ],
   [
    "2bea7c5a-f074-44ed-8f41-5314db3fd399",
    "2bea7c5a-f074-44ed-8f41-5314db3fd399"
   ],
   [
    "d44d86e0-4731-4d2d-8012-e479b7c868a1",
    "d44d86e0-4731-4d2d-8012-e479b7c868a1"
   ],
   [
    "4014c885-ccdc-4560-a7bc-d7fe5069752a",
    "4014c885-ccdc-4560-a7bc-d7fe5069752a"
   ],
   [
    "c0a4de96-106e-4e5a-b0c6-6d24c08b7080",
    "c0a4de96-106e-4e5a-b0c6-6d24c08b7080"
   ],
   [
    "6f95d192-cc48-4962-b063-a0aca3c6b39a",
    "6f95d192-cc48-4962-b063-a0aca3c6b39a"
   ],
   [
    "d38aed5a-c0ab-4b17-ad56-a7c71e2f2443",
    "d38aed5a-c0ab-4b17-ad56-a7c71e2f2443"
   ],
   [
    "782a6e84-cbf9-4474-b8e5-b60569ed0b6d",
    "782a6e84-cbf9-4474-b8e5-b60569ed0b6d"
   ],
   [
    "dead9404-b566-41db-af45-741e48847af8",
    "dead9404-b566-41db-af45-741e48847af8"
   ],
   [
    "30280e2b-8f3e-447a-abd5-486681005191",
    "30280e2b-8f3e-447a-abd5-486681005191"
   ],
   [
    "94891b77-d370-48e5-8111-dff25dba613b",
    "94891b77-d370-48e5-8111-dff25dba613b"
   ],
   [
    "0f761028-bc9c-4193-9166-08ca0aef9f92",
    "0f761028-bc9c-4193-9166-08ca0aef9f92"
   ],
   [
    "0249b377-73b7-41ff-ab76-91a30216c24a",
    "0249b377-73b7-41ff-ab76-91a30216c24a"
   ],
   [
    "52aaeff4-bb16-42b8-9788-4e97603433b2",
    "52aaeff4-bb16-42b8-9788-4e97603433b2"
   ],
   [
    "c0fe48fa-bddb-4da8-98d6-c779d510acd5",
    "c0fe48fa-bddb-4da8-98d6-c779d510acd5"
   ],
   [
    "c63bfb4c-82e5-487f-a9be-09d74ac85b0c",
    "c63bfb4c-82e5-487f-a9be-09d74ac85b0c"
   ],
   [
    "2943754f",
    "2943754f"
   ],
   [
    "891a4ee3-0820-482b-a8d1-169c3e0a9a30",
    "891a4ee3-0820-482b-a8d1-169c3e0a9a30"
   ],
   [
    "7a54028f-7924-4ebd-93dd-2d6ced02434e",
    "7a54028f-7924-4ebd-93dd-2d6ced02434e"
   ],
   [
    "6e4b161e-0097-4418-91f6-e999a963e426",
    "6e4b161e-0097-4418-91f6-e999a963e426"
   ],
   [
    "27aa96b9-bd26-47c2-856a-3e7b79ba71bc",
    "27aa96b9-bd26-47c2-856a-3e7b79ba71bc"
   ],
   [
    "6058ee13",
    "6058ee13"
   ],
   [
    "eafe0cde-ac05-411b-bf33-4871d457365d",
    "eafe0cde-ac05-411b-bf33-4871d457365d"
   ],
   [
    "8f741b02-05b5-41b8-a9bc-49591d938c67",
    "8f741b02-05b5-41b8-a9bc-49591d938c67"
   ],
   [
    "1d0d93d2-908a-447d-b097-2946492819ed",
    "1d0d93d2-908a-447d-b097-2946492819ed"
   ],
   [
    "082ff3d4-3e4f-4267-a2cf-ffd1d599e94d",
    "082ff3d4-3e4f-4267-a2cf-ffd1d599e94d"
   ],
   [
    "605388db-6b3a-4674-9229-011bd61c1fed",
    "605388db-6b3a-4674-9229-011bd61c1fed"
   ]
  ],
  "unianalytics_notebook_id": "327195e2-c389-4a13-a30e-882aa6a98ae7"
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
