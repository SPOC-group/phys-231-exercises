{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "996d9f46",
   "metadata": {},
   "source": [
    "# Exercise 7 - Theory Recap \n",
    "\n",
    "We give you a few examples to train your theoretical understanding, covering some topics from the first part of the course."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d62a1bd",
   "metadata": {},
   "source": [
    "------\n",
    "### A) PageRank\n",
    "\n",
    "Recall the adjacency matrix we defined in the lecture for a directed graph with $n$ nodes:\n",
    "$$\n",
    " A_{ij} = \\begin{cases}\n",
    "        1 & \\text{if website $j$ links towards website $i$} \\\\\n",
    "        0 & \\text{otherwise}\n",
    "    \\end{cases} \\, .\n",
    "$$\n",
    "\n",
    "-  **Question 1**: How can you determine the in-degree and out-degree of a node $i$ from its corresponding row and column in the adjacency matrix $A$? \n",
    "\n",
    "- **Question 2**: In which cases is the use of a dense array suboptimal as a datastructure in your algorithm? \n",
    "\n",
    "- **Question 3**: How can you use the given adjacency matrix to find the number of directed triangles in a graph? The existence of a directed triangle on the internet means that starting on website $i$, a surfer can return to the starting website by clicking three links (e.g. `Physics->Universe->Time->Physics` on wikipedia).\n",
    "Sketch the idea of an algorithm that would count these triangles, and explain why it works. (If you feel like it, you can go back to the solution of exercise 1 and implement it.)\n",
    "*Hint: What conditions would the matrix $A$ fullfill, if there existed a directed triangle between the nodes $i,j$ and $k$?*\n",
    "\n",
    "Recall our definition of the matrix $S$ which we used for Page Rank in Lecture 1:\n",
    "$$\n",
    "    S_{ij} = \\begin{cases}\n",
    "        \\frac{A_{ij}}{d_j} & \\text{if $d_j \\geq 1$} \\\\\n",
    "        \\frac{1}{n} & \\text{if $d_j = 0$}\n",
    "    \\end{cases} \\,,\n",
    "$$\n",
    "with \n",
    "$$\n",
    "d_j = \\sum_{i = 1}^n A_{ij} \\, .\n",
    "$$\n",
    "\n",
    "- **Question 4**: Prove that $S$ is *column-wise stochastic*, i.e. $$\\sum_{i=1}^n S_{ij} = 1 \\quad \\text{for} \\quad i = 1, \\dots, n \\,.$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d429d8bd",
   "metadata": {},
   "source": [
    "-------\n",
    "### B) Principle Component Analysis and Singular Value Decomposition\n",
    "\n",
    "Recall the singular value decomposition we discussed in Lecture 2. Assume that we are given a data matrix $A \\in \\mathbb{C}^{n\\times d}$.\n",
    "\n",
    "- **Question 1**: Define precisely what is meant by the SVD for the matrix $A$. \n",
    "\n",
    "- **Question 2**: What is a singular value of a matrix $A$? What are left and right singular eigenvectors in this context? How are they related to the SVD?\n",
    "\n",
    "- **Question 3**: You want to obtain a low rank approximation of rank $k$ of your original matrix $A$ using its SVD. The quality of the approximation is measured in terms of the Frobenius norm to the original matrix. How does one find such an approximation? Why?\n",
    "\n",
    "- **Question 4**: How can you calculate the Frobenius norm of matrix $A$ using the singular values obtained from SVD? Prove your suggestion."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8480185b",
   "metadata": {},
   "source": [
    "------\n",
    "### C) Linear Regression\n",
    "*Disclaimer: This exercise is probably a bit more difficult than what you would see in the exam.*\n",
    "\n",
    "Let $X\\in\\mathbb R^{n\\times d}$ be such that $X^TX$ is invertible and $\\vec y\\in\\mathbb R^n$. This is the case where we have enough datapoints, and we select exactly the solution which minimizes the least squares loss\n",
    "$$\n",
    "{\\arg \\min}_{\\vec \\alpha} \\| Y - \\hat Y( \\vec \\alpha) \\|^2\n",
    "$$\n",
    "where ${\\vec \\alpha} \\in \\mathbb{R}^d$ are the parameters we are fitting and $\\hat Y(\\vec \\alpha) = X \\vec \\alpha$.\n",
    "The goal of this exercise is to prove geometrically that under the given assumptions, the least square minimizer is such that $$\n",
    "\\hat {\\vec \\alpha} = (X^TX)^{-1}X^TY.\n",
    "$$\n",
    "- **Question 1**: What is the dimension of the vector $Y$? What is the basis $\\mathcal B=\\{\\vec b_i\\}_{i=1}^r$ of $\\mathbb R^n$ in which you can express $\\hat Y(\\vec \\alpha)$? I.e. what is the value of $r$, what are the $b_i$'s?\n",
    "\n",
    "- **Question 2**: Note that the least squares loss minimizes the Euclidian distance between the two points $Y$ and $\\hat Y$ in $\\mathbb R^n$. The degrees of freedom to achieve this minimization are $\\vec \\alpha \\in \\mathbb R^d$. Re-express the condition for a minimal loss in geometric terms.\n",
    "*Hint: Decompose $Y=\\hat Y + \\vec e$ with $\\vec e \\in \\mathbb R^n$. What is the norm of $\\vec e$ related to the least squares?  How can you interpret it geometrically? Use orthogonality to express the least squares condition.*\n",
    "\n",
    "- **Question 3**: Using this new formulation, deduce the above expression for $\\hat\\alpha$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d032b75",
   "metadata": {},
   "source": [
    "-------\n",
    "### D) Gradient Descent\n",
    "\n",
    "- **Question 1**: When and why do you need a validation and a test dataset? \n",
    "- **Question 2**: Gradient descent often requires selecting a learning rate. What is the learning rate, and what are the trade-offs in choosing a learning rate that is too small or too large?\n",
    "- **Question 3**:Why is generally not a good idea to use a the step function $$\\theta(x) = \\begin{cases}\n",
    "        0 & \\text{if $x > 0$} \\\\\n",
    "        1 & \\text{if $else$}\\\\\n",
    "    \\end{cases}$$ as a loss for gradient based optimization?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
