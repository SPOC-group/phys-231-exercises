{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "a79a6730",
   "metadata": {},
   "source": [
    "# Exercise 10: Maximum likelihood estimation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b62efe87",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "03abfefd",
   "metadata": {},
   "source": [
    "## Ex 1: Particle decay"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "11b2b005",
   "metadata": {},
   "source": [
    "In this exercise, we will numerically investigate Example 4 (see chapter 9 of the lecture notes).\n",
    "\n",
    "A radioactive source emits $n$ unstable particles. The particles are emitted at a constant speed along the $x$ axis. For each particle, indexed by  $i\\in\\{1,\\dots,n\\}$, we mark the  position $x_i$ where it decays. Our detector however only detects decays between $x_{min}$ and $x_{max}$.\n",
    "\n",
    "The probability density of decaying at $x$ is \n",
    "$$\n",
    "\\rho(x|\\lambda_*)=\\frac{e^{-x/\\lambda_*} }{\\lambda_* \\left(e^{-x_{min}/\\lambda_*}-e^{-x_{max}/\\lambda_*}\\right)}\n",
    "$$\n",
    "for an unknown parameter $\\lambda_*$. As we saw in Example 3 of the lecture notes, $\\lambda_*$ denotes the true mean of the decay distance. We expect the mean of the observations to be, however, in general different from $\\lambda_*$ due to the restriction of the observations to the range $[x_{min},x_{max}]$.\n",
    "\n",
    "Given the dataset of $n$ measurements $\\mathbf x=(x_1,\\dots,x_n)$ our objective is to estimate $\\lambda_*$.\n",
    "\n",
    "Suppose that $x_{min}=0.5,x_{max}=3$ are fixed. We consider two cases:\n",
    "1. $\\lambda_* = 2$\n",
    "2. $\\lambda_* = 4$\n",
    "\n",
    "Note that in case $1$, $\\lambda_*$ lies within the range $[x_{min},x_{max}]$ while in case $2$, $\\lambda_*$ is outside the range of measurements."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d5ffd4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "xmin=0.5\n",
    "xmax=3\n",
    "lambda_stars=[2,4] #value of the parameter that we want to estimate. Also said \"ground truth\"."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "959da11f",
   "metadata": {},
   "source": [
    "### Ex 1.1: simulating the particle decay"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "90975a3b",
   "metadata": {},
   "source": [
    "Your first task is to simulate this physical system, namely to generate the dataset $x_1, x_2, \\dots, x_n$ given the true value $\\lambda_*$.\n",
    "\n",
    "To generate the data $\\pmb x$, you will use the so called *rejection method*. \n",
    "The idea is that the p.d.f. from which we want to sample is just a truncated exponential, i.e. an exponential with scale parameter $\\lambda_*$ in which we discard events in which $x < x_{\\rm min}$ or $x > x_{\\rm max}$.\n",
    "Thus, we can just generate exponential random numbers using `np.random.exponential` one by one. \n",
    "For each sample, we check if $x_{\\rm min} < x < x_{\\rm max}$. If this is the case, we keep this sample, otherwise we discard it. \n",
    "We iterate the procedure until we have $n$ valid measurements.\n",
    "\n",
    "**1.1.1**\n",
    "\n",
    "Write a function that takes as input $n$, $\\lambda_*$, $x_{\\rm min}$ and $x_{\\rm max}$ and generates the dataset as a lenght $n$ vector. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "078c2451",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here\n",
    "def generate_dataset_decay(# Complete here):\n",
    "    # Complete here"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "6ddbb882",
   "metadata": {},
   "source": [
    "**1.1.2**\n",
    "\n",
    "Generate datasets with $n=2000, x_{min}=0.5,x_{max}=3$ and $\\lambda_*=2,4$ for the two cases. Plot the histograms in both cases. \n",
    "Use ```density = True``` (to have the histogram represent a p.d.f.) and the x axis limits to $(0, 5)$.\n",
    "\n",
    "You will observe the truncations at $x_{min}$ and $x_{max}$ and (albeit very noisily) the exponential decay of the distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b89ddce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "54fe87b1",
   "metadata": {},
   "source": [
    "### Ex 1.2: computing the maximum likelihood estimator"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "e47e3976",
   "metadata": {},
   "source": [
    "Review Example 4 in chapter 9 of the lecture notes. There, we derived the maximum likelihood estimator for $\\lambda_*$ as \n",
    "$$\n",
    "\\hat{\\lambda} \n",
    "    = \\arg\\max_{\\lambda} \\frac{e^{-\\frac{1}{\\lambda n}\\sum_{i=1}^n x_i} }{\\lambda \\left( e^{-x_{\\rm min} / \\lambda} - e^{-x_{\\rm max} / \\lambda} \\right)}  \n",
    "    = \\arg\\max_{\\lambda} \\mathcal{L}(\\lambda)\n",
    "    \\, .\n",
    "$$\n",
    "It is easier to work with the negative normalized log-likelihood $- \\frac{1}{n} \\log \\mathcal{L}(\\lambda)$, which needs to be minimised (due to the negative sign), giving\n",
    "$$\n",
    "    \\hat \\lambda=\\arg\\min_\\lambda \\ell(\\lambda)\n",
    "$$\n",
    "where\n",
    "$$\n",
    "\\ell(\\lambda) =\n",
    "\\log\\lambda + \\log\\left(e^{-x_{min}/\\lambda}-e^{-x_{max}/\\lambda}\\right) + \\frac{1}{\\lambda} \\frac{1}{n}\\sum_{i=1}^n x_i \\, .\n",
    "$$\n",
    "\n",
    "**1.2.1**\n",
    "Complete the code below to implement the $\\ell(x)$ function. Notice that ```lambda``` is a reserved keyword in python, so we use ```l```."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3955ea6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here\n",
    "def ell(l, xmin, xmax, dataset):\n",
    "    # Complete here"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "f5d4eab0",
   "metadata": {},
   "source": [
    "**1.2.2** For both cases $\\lambda^\\star=2$ and $\\lambda^\\star=4$, do the following:\n",
    "\n",
    "Use the function ```scipy.optimize.minimize``` to numerically minimize $\\ell(\\lambda)$ for the datasets generated in 1.1.2 and compute the MLE.\n",
    "\n",
    "Use ```0.1``` for the initial condition of the minimization. Print the MLE that you obtain.\n",
    "\n",
    "Hint: ```scipy.optimize.minimize``` expects a function of the form ```f(l)```, while our function is of the form ```f(l, params)```. \n",
    "This happens often.\n",
    "The way to deal with this is through lambda functions, also called anonymous functions:\n",
    "```\n",
    "    scipy.optimize.minimize(lambda l: f(l, params), ...)\n",
    "```\n",
    "The keyword ```lambda``` introduces the anonymous function, ```l``` is the variable of the function, and whatever comes after the semicolon `:` is the body of the function.\n",
    "You can think of the expression \n",
    "```\n",
    "    lambda l: f(l, params)\n",
    "```\n",
    "as a shorthand for\n",
    "```\n",
    "    def f2(l):\n",
    "        return f(l, params)\n",
    "```\n",
    "with the advantages of\n",
    "\n",
    "1. not giving a name (```f2``` in the example) to a function that you will use only once.\n",
    "2. Not defining a function in which we hardcode a global variable (```dataset``` in the example)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7f8e129",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "c2b8f353",
   "metadata": {},
   "source": [
    "**1.2.3** For both cases $\\lambda^\\star=2$ and $\\lambda^\\star=4$, plot the function $\\ell(\\lambda)$ as a function of $\\lambda$ using the dataset generated in 1.1.2. \n",
    "Additionaly, plot a vertical line at the MLE estimator $x = \\hat{\\lambda}$ that you computed in 1.2.2, and another vertical line at the value $x = \\lambda_*$.\n",
    "Set the x axis limits to $(2,10)$ and add a legend."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac70f268",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0954533f",
   "metadata": {},
   "source": [
    "**1.2.4** For both cases $\\lambda^\\star=2$ and $\\lambda^\\star=4$, perform the following for $10$ independent trials:\n",
    "\n",
    "Repeat 1.2.2 (obtaining the MLE) for a $n$ spaced between $10,1000$ on a logarithmic scale. Generate a plot containing, for each of the $10$ trials (in the same plot), the obtained MLE $\\hat{\\lambda}$ on the $y$-axis as a function of $n$ on the $x$-axis. Use logarithmic scale for both the axes. Add a horizontal line at $y=\\lambda_*$ for comparison. \n",
    "- What do you observe as $n$ increases?\n",
    "- How does the behavior differ between the two cases $\\lambda^\\star=2$ $\\lambda^\\star=4$?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff98a49f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa2554d5",
   "metadata": {},
   "source": [
    "**1.2.5** We will now estimate the standard deviation of $\\hat{\\lambda}$ and compare it with the prediction using Fisher information.\n",
    "\n",
    "Fix $n=1000$. For both cases $\\lambda^\\star=2$ and $\\lambda^\\star=4$, generate $k=1000$ independent datasets each of size $n$. Let $\\hat{\\lambda}_i$ denote the Maximum Likelihood estimate for the $i_{th}$ dataset, $i \\in [k]$. Estimate the standard deviation of $\\hat{\\lambda}$ empirically as follows:\n",
    "\n",
    "$$\\hat{\\sigma}_n \\coloneqq \\sqrt{\\frac{1}{k}\\sum_{i=1}^k[\\hat{\\lambda}_i-\\bar{\\lambda}]^2}.$$\n",
    "\n",
    "where $\\bar{\\lambda}=\\frac{1}{k}\\sum_{i=1}^k\\hat{\\lambda}_k$ is the mean of $\\hat{\\lambda}$ across the $k$ datasets.\n",
    "\n",
    "The above quantity asymptotically converges to the standard deviation of the MLE $\\hat{\\lambda}$ around the asymptotic mean $\\lambda_*$. Recall from Equation 9.36 in the notes the definition of the Fisher information:\n",
    "\n",
    "$$I(\\lambda^\\star)= \\mathbb{E}_{\\rho(x|\\lambda^\\star)}[(\\frac{\\partial \\log \\rho(x|\\lambda)}{\\partial \\lambda}\\vert_{\\lambda=\\lambda^\\star})^2].$$\n",
    "\n",
    "Analytically compute $\\frac{\\partial \\log \\rho(x|\\lambda)}{\\partial \\lambda}$ and estimate $I$ by taking the empirical mean of $(\\frac{\\partial \\log \\rho(x|\\lambda)}{\\partial \\lambda}\\vert_{\\lambda=\\hat{\\lambda}_i})^2$, for $i \\in [k]$ over the datasets generated above. Note that we set $\\lambda=\\hat{\\lambda}_i$ (the estimated mean for the $i_{th}$ dataset) instead of $\\lambda=\\lambda^\\star$ since $\\lambda^\\star$ is unknown. From Theorem 9 in the notes, we expect that as $n \\rightarrow \\infty$:\n",
    "\n",
    "$$\\hat{\\sigma}_n \\rightarrow 1/\\sqrt{nI(\\lambda^\\star)}.$$\n",
    "\n",
    "Compare the value $\\hat{\\sigma}_n$ against the estimate of $1/\\sqrt{nI(\\lambda^\\star)}$ (both obtained using the estimates $\\hat{\\lambda}_i$ for $i \\in [k]$).\n",
    "\n",
    "Compare the values for $\\lambda^\\star=2$ and $\\lambda^\\star=4$.\n",
    "\n",
    "Using the evaluated standard deviation, we can report our estimate for $\\lambda^\\star$ as  $\\lambda^\\star = \\bar{\\lambda} \\pm \\hat{\\sigma}_n$. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8a185ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "547fba9e",
   "metadata": {},
   "source": [
    "**1.2.6** \n",
    "The ```scipy.optimize.minimize``` uses under the hood sophisticated minimization methods, but for the sake of the argument let us imagine that it is using gradient descent to minimize the function $\\ell$. Discuss the following points:\n",
    "\n",
    "- does gradient descent converge to the global optimum in general (i.e. not in this specific case), and what are the consequences for the MLE if it does not? \n",
    "- In this case, does gradient descent converge to the global optimum of the likelihood?\n",
    "\n",
    "Motivate your answers."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "0c0a1d81",
   "metadata": {},
   "source": [
    "## Ex 2: The lighthouse"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "de37ea72",
   "metadata": {},
   "source": [
    "A lighthouse is placed on a small island in front of the coastline. \n",
    "In a coordinate system in which the coast is the $x$ axis, and the $y$ axis points towards open sea, the island is at position $(\\alpha_*, 1)$.\n",
    "The lighthouse emits a series of short flashes at random intervals and hence in random directions $\\theta$ (uniformly at random over the full angle of $2\\pi$).\n",
    "These pulses are intercepted on the coast by photo-detectors that record only the fact that a flash has occurred, but not the angle from which it came.\n",
    "\n",
    "In this exercise, we will consider the estimation task of finding the true position $\\alpha_*$ of the lighthouse along the coastline, assuming that \n",
    "- the angle $\\theta$ at which each light pulse is emitted is a uniformly distributed random variable on the interval $(-\\pi/2, \\pi/2)$ (the angle is measured with respect to the negative-vertical axis, see figure)\n",
    "- a dataset of $n$ detected positions $x_1, x_2, \\dots, x_n$ is given (each detection is statistically independent from the others)."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "b482031a",
   "metadata": {},
   "source": [
    "![](lighthouse.png)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "dd6b1f4b",
   "metadata": {},
   "source": [
    "### Ex 2.1: simulating the lighthouse, i.e. generating the dataset of measurements"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "9b59da2b",
   "metadata": {},
   "source": [
    "Your first task is to simulate this physical system, namely to generate the dataset $x_1, x_2, \\dots, x_n$ given the true value $\\alpha_*$.\n",
    "To do this, we are going to generate randomly the angle $\\theta_i$ at which a light pulse is emitted and we will compute the corresponding detection position $x_i$ as\n",
    "$$\n",
    "    x_i = \\tan(\\theta_i) + \\alpha_* \\, .\n",
    "$$"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "912f7b6f",
   "metadata": {},
   "source": [
    "**2.1.1** Write a function that takes as input $n$ and $\\alpha_*$ and generates the dataset as a length $n$ vector. Recall that each angle is independent from the others, and uniformly distributed in the interval $(-\\pi/2, \\pi/2)$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d722a62",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here\n",
    "def generate_dataset(# Complete here):\n",
    "    # Complete here"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "54038558",
   "metadata": {},
   "source": [
    "**2.1.2** Using the change of variables formula, we can show that the probability density function of the random variable $x(\\theta)$ is given by\n",
    "$$\n",
    "    \\rho_{x}(x | \\alpha_*) = \\frac{1}{\\pi(1+(x - \\alpha_*)^2)} \\, .\n",
    "$$\n",
    "\n",
    "We wish to compare the above density with the frequencies of $x$ in a dataset of samples.\n",
    "\n",
    "Generate a dataset with $\\alpha_* = 20$ and $n=10000$ and plot its histogram. \n",
    "Set manually the bins to ```range(-300,300)``` (this means that we use 600 bins with width 1 on the interval $(-300,300)$) and use the option ```density = True``` in the histogram (to plot frequencies instead of counts on the y axis), and plot in black the p.d.f. $\\rho_x$ given above on top of it.\n",
    "Plot from $x=-100$ to $x=100$.\n",
    "Add a vertical line at $x = \\alpha_*$ in red.\n",
    "Add a legend and labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "270d49a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "f3fe57dc",
   "metadata": {},
   "source": [
    "**2.1.3** Answer the following questions.\n",
    "\n",
    "1. Around which value of $x$ is the histogram peaked? Motivate your answer.\n",
    "2. Which values of the angle $\\theta$ correspond to the extreme values (the rare outliers)?"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "911a6952",
   "metadata": {},
   "source": [
    "### Ex 2.2: estimation of $\\alpha_*$ through maximum likelihood"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "759d4ce1",
   "metadata": {},
   "source": [
    "In this section, we implement the maximum likelihood estimator for $\\alpha$, and use it to estimate $\\alpha_*$ in the given dataset."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "88c1b9ee",
   "metadata": {},
   "source": [
    "**2.2.1** \n",
    "Use ```np.load``` to load the file \"data-lighthouse.npy\" into a variable called ```dataset```. \n",
    "This will be your set of observations for this part of the exercise, in which we will try to estimate the value $\\alpha_*$ used to generate the dataset. \n",
    "How many measurements does it contain?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8dee76c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "9a5505fd",
   "metadata": {},
   "source": [
    "**Your answer here:**"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "1b3e59c5",
   "metadata": {},
   "source": [
    "**2.2.2** Prove that the maximum likelihood estimator (MLE) $\\hat{\\alpha}$ is given by\n",
    "$$\n",
    "\\hat{\\alpha} = \\arg\\min_{\\alpha} \\sum_{i=1}^n \\log (1+(x_i - \\alpha)^2) = \\arg\\min_{\\alpha} f(\\alpha, \\{x_i\\}_{i=1}^n)\n",
    "$$\n",
    "Type your derivation in a Markdown cell using LaTeX.\n",
    "\n",
    "Hints: \n",
    "- you can follow and adapt the many examples of MLE given in class to this specific problem. \n",
    "- It is useful to maximise the log-likelihood instead of the likelihood to convert products into sums (recall that logs do not alter the position of maxima). \n",
    "- $\\max[f(x)] = \\min[- f(x)]$."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "cf217b5c",
   "metadata": {},
   "source": [
    "**Your answer here:**"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "cc1210b1",
   "metadata": {},
   "source": [
    "**2.2.3** Write a function that takes as input $\\alpha$ and the dataset (a length $n$ vector) and computes $f(\\alpha, \\{x_i\\}_{i=1}^n)$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f87ea78",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "9df4cca9",
   "metadata": {},
   "source": [
    "**2.2.4** Plot the function $f(\\alpha, \\{x_i\\}_{i=1}^n)$ as a function of $\\alpha$ and the given dataset. Plot it for $-10 < \\alpha < 30$.\n",
    "What can you deduce about the value of $\\hat{\\alpha}$ from the plot?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e3dad8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "ae21e392",
   "metadata": {},
   "source": [
    "**2.2.5** Minimize numerically $f$ for your dataset and compute the MLE. Use ```x0=0``` for the initial condition of the minimization. Print the MLE that you obtain."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67a271cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "2fbb11ee",
   "metadata": {},
   "source": [
    "**2.2.6** \n",
    "Reproduce the plot you did in 2.2.4.. \n",
    "Additionaly, plot a vertical line at  $x = \\hat{\\alpha}$ and add a legend."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88fe9653",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  },
  "unianalytics_cell_mapping": [
   [
    "a79a6730",
    "a79a6730"
   ],
   [
    "b62efe87",
    "b62efe87"
   ],
   [
    "03abfefd",
    "03abfefd"
   ],
   [
    "11b2b005",
    "11b2b005"
   ],
   [
    "5d5ffd4b",
    "5d5ffd4b"
   ],
   [
    "959da11f",
    "959da11f"
   ],
   [
    "90975a3b",
    "90975a3b"
   ],
   [
    "078c2451",
    "078c2451"
   ],
   [
    "6ddbb882",
    "6ddbb882"
   ],
   [
    "9b89ddce",
    "9b89ddce"
   ],
   [
    "54fe87b1",
    "54fe87b1"
   ],
   [
    "e47e3976",
    "e47e3976"
   ],
   [
    "3955ea6f",
    "3955ea6f"
   ],
   [
    "f5d4eab0",
    "f5d4eab0"
   ],
   [
    "e7f8e129",
    "e7f8e129"
   ],
   [
    "c2b8f353",
    "c2b8f353"
   ],
   [
    "ac70f268",
    "ac70f268"
   ],
   [
    "0954533f",
    "0954533f"
   ],
   [
    "ff98a49f",
    "ff98a49f"
   ],
   [
    "aa2554d5",
    "aa2554d5"
   ],
   [
    "d8a185ad",
    "d8a185ad"
   ],
   [
    "547fba9e",
    "547fba9e"
   ],
   [
    "0c0a1d81",
    "0c0a1d81"
   ],
   [
    "de37ea72",
    "de37ea72"
   ],
   [
    "b482031a",
    "b482031a"
   ],
   [
    "dd6b1f4b",
    "dd6b1f4b"
   ],
   [
    "9b59da2b",
    "9b59da2b"
   ],
   [
    "912f7b6f",
    "912f7b6f"
   ],
   [
    "3d722a62",
    "3d722a62"
   ],
   [
    "54038558",
    "54038558"
   ],
   [
    "270d49a6",
    "270d49a6"
   ],
   [
    "f3fe57dc",
    "f3fe57dc"
   ],
   [
    "911a6952",
    "911a6952"
   ],
   [
    "759d4ce1",
    "759d4ce1"
   ],
   [
    "88c1b9ee",
    "88c1b9ee"
   ],
   [
    "8dee76c2",
    "8dee76c2"
   ],
   [
    "9a5505fd",
    "9a5505fd"
   ],
   [
    "1b3e59c5",
    "1b3e59c5"
   ],
   [
    "cf217b5c",
    "cf217b5c"
   ],
   [
    "cc1210b1",
    "cc1210b1"
   ],
   [
    "4f87ea78",
    "4f87ea78"
   ],
   [
    "9df4cca9",
    "9df4cca9"
   ],
   [
    "0e3dad8a",
    "0e3dad8a"
   ],
   [
    "ae21e392",
    "ae21e392"
   ],
   [
    "67a271cc",
    "67a271cc"
   ],
   [
    "2fbb11ee",
    "2fbb11ee"
   ],
   [
    "88fe9653",
    "88fe9653"
   ]
  ],
  "unianalytics_notebook_id": "d155046b-6e48-46a8-8af9-e2e74708711a"
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
