{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "abdb31d8",
   "metadata": {},
   "source": [
    "# Graded Exercise 2 - Sampling with Boltzmann Machines and MCMC"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3815ca65",
   "metadata": {},
   "source": [
    "## General instruction\n",
    "Each question is provided in a _Markdown_ cell and should be answered in the cell(s) below. You may add new cells if needed. All figures must be generated and shown directly in this notebook. If a question demands that you write an answer, use a _Markdown_ cell, which can include latex between \\\\$ symbols. As an example,\n",
    "\\\\$\\vec{F}=m\\vec{a}\\\\$\n",
    "gives $\\vec{F}=m\\vec{a}$.\n",
    "\n",
    "Your code should run properly if you do the following: 1) restart the kernel 2) execute all cells in order from top to bottom. Running all cells should take a reasonable time on a standard computer or on Noto (<30 min.). Note that this exercise will require longer execution time than the first graded homework.\n",
    "\n",
    "Avoid using `for` loops whenever possible. Instead, use vectorized operations or numpy functions.\n",
    "\n",
    "All external sources you consult must be explicitly cited, except for the official NumPy, Scipy and Matplotlib documentation, the lecture notes, and previous exercises. You are encouraged to use external sources, since every function needed in this exercise has not necessarily been seen in the previous exercises. Please also cite every person you discussed this exercise with.\n",
    "\n",
    "Overly long or unnecessarily complicated answers will be penalized."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fada1979",
   "metadata": {},
   "source": [
    "## Context"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e65ff13b",
   "metadata": {},
   "source": [
    "In 2024, Geoffrey Hinton, one of the pioneers of deep learning, was awarded the Nobel Prize in Physics for his foundational contributions to energy-based models. His work demonstrated that ideas from physics, such as energy landscapes, entropy, and thermal fluctuations, can be used to build computational models capable of learning complex patterns from data. Central to this achievement is the Boltzmann machine, a model defined primarily by an energy function over binary variables, an idea directly inspired by the Ising model in physics.\n",
    "\n",
    "In this exercise, you will train a Boltzmann machine or variants thereof to approximately sample from a high-dimensional distribution."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff13ecf9",
   "metadata": {},
   "source": [
    "In part 1-4, we investigate the performance of various energy-based models on a synthetic dataset. In part 5, we study the performance of a specific energy-based model on a real dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32f9620b",
   "metadata": {},
   "source": [
    "# 1. The bars and stripes dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f5b0451",
   "metadata": {},
   "source": [
    "__1.1__ Load the dataset *bars_and_stripes.npy* with the numpy `load` function. Print its shape. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cf65bab9",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-10T13:16:32.281697Z",
     "iopub.status.busy": "2025-12-10T13:16:32.281470Z",
     "iopub.status.idle": "2025-12-10T13:16:32.285421Z",
     "shell.execute_reply": "2025-12-10T13:16:32.284506Z",
     "shell.execute_reply.started": "2025-12-10T13:16:32.281673Z"
    }
   },
   "outputs": [],
   "source": [
    "# Your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9873c93b",
   "metadata": {},
   "source": [
    "__1.2__ The *bars_and_stripes.npy* dataset consists of black and white images. Using `imshow` from the matplotlib library, draw the first 10 images of the dataset. Comment briefly on the content of this dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "15e6b7f8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-10T13:16:32.398885Z",
     "iopub.status.busy": "2025-12-10T13:16:32.398609Z",
     "iopub.status.idle": "2025-12-10T13:16:32.403810Z",
     "shell.execute_reply": "2025-12-10T13:16:32.402809Z",
     "shell.execute_reply.started": "2025-12-10T13:16:32.398864Z"
    }
   },
   "outputs": [],
   "source": [
    "# Your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "015a94f1",
   "metadata": {},
   "source": [
    "Your answer here:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93d4a86b",
   "metadata": {},
   "source": [
    "From now on, we treat each image as a flattened vector \n",
    "$$\\mathbf s_\\mu=(s_1^\\mu, s_2^\\mu, ..., s_{d}^\\mu),\\quad s_i\\in\\{0, 1\\}.$$\n",
    "$\\mu=1, ..., n$ is the sample index, and denote the whole dataset as $S$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac8afc23",
   "metadata": {},
   "source": [
    "**1.3** Reshape the dataset to a size $n\\times d$, where $n$ indicates the number of samples and $d$ the number of pixels in each image. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5f1c071c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-10T13:16:33.281079Z",
     "iopub.status.busy": "2025-12-10T13:16:33.280768Z",
     "iopub.status.idle": "2025-12-10T13:16:33.283904Z",
     "shell.execute_reply": "2025-12-10T13:16:33.283176Z",
     "shell.execute_reply.started": "2025-12-10T13:16:33.281056Z"
    }
   },
   "outputs": [],
   "source": [
    "# Your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9c3b4a7",
   "metadata": {},
   "source": [
    "We will now study various parameterized probability distribution of the form\n",
    "$$\n",
    "P_\\theta(\\mathbf s)=\\frac{e^{-E_\\theta(\\mathbf s)}}{Z_\\theta}.\n",
    "$$\n",
    "$\\theta$ are the parameters of the distribution, that we will find to match the given data. $E_\\theta$ is a function that depends on the parameters $\\theta$ and $\\mathbf s$. $Z_\\theta$ is the normalization such that $P_\\theta$ is a proper probability distribution. Distributions of this form are called *Boltzmann* or *Gibbs* distribution. Note that to model physical systems, the $E_\\theta$ is the energy and is usually multiplied by a term proportional to the inverse temperature."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79126c45",
   "metadata": {},
   "source": [
    "# 2. Independent field model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07b8c270",
   "metadata": {},
   "source": [
    "We first define the energy of the independent field model as\n",
    "$$\n",
    "E_{\\mathbf h}(\\mathbf s)=\\sum_{i=1}^d h_i s_i, \\quad \\mathbf h\\in\\mathbb{R}^d\n",
    "$$\n",
    "where $h_i$ are scalar parameter associated to pixel $i$. We consider the Boltzmann distribution associated to this energy\n",
    "$$\n",
    "P_{\\mathbf{h}} (\\mathbf s)=\\frac{1}{Z_{\\mathbf h}}e^{- E_{\\mathbf h}(\\mathbf s)}.\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f1d2d7f",
   "metadata": {},
   "source": [
    "**2.1** Show that in this model each pixel is statistically independent of the others.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f492af0d",
   "metadata": {},
   "source": [
    "Your answer here:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae758925",
   "metadata": {},
   "source": [
    "**2.2** The empirical and model averages are\n",
    "$$\n",
    "<s_i>_{emp}=\\frac{1}{n}\\sum_{\\mu=1}^n s_i^\\mu,\n",
    "$$\n",
    "$$\n",
    "<s_i>_{model}=\\sum_{\\mathbf s\\in\\{0,1\\}^d} s_i P_{\\mathbf h}(\\mathbf s).\n",
    "$$\n",
    "Note here that $s_i^\\mu$ is component $i$ of sample $\\mu$, while $s_i$ is a dummy variable which we sum over.\n",
    "We study this problem with maximum likelihood estimation.\n",
    "We want to find $\\hat{\\mathbf h}$ that minimizes the negative log-likelihood $\\mathcal{L}(\\mathbf h)=-\\sum_{\\mu=1}^n \\log P_{\\mathbf h}(\\mathbf s_\\mu)$. Show that this entails finding $\\hat{\\mathbf h}$ such that\n",
    "$$\n",
    "<s_i>_{emp}=<s_i>_{model}.\n",
    "$$\n",
    "\n",
    "Note: you can reuse the expression derived in the previous question. You are not required to check that $\\hat{\\mathbf h}$ is a minimum, it is sufficient to impose that it is a stationary/extremal point."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b11745e5",
   "metadata": {},
   "source": [
    "Your answer here:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48dfa032",
   "metadata": {},
   "source": [
    "**2.3** Deduce that\n",
    "$$\n",
    "\\hat h_i = \\log\\left(\\frac{1-<s_i>_{emp}}{<s_i>_{emp}}\\right).\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67c62407",
   "metadata": {},
   "source": [
    "Your answer here:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16c57bab",
   "metadata": {},
   "source": [
    "**2.4** Show that \n",
    "$$\n",
    "P_{\\hat h_i}(s_i)=\\begin{cases}\n",
    "  <s_i>_{emp} & \\text{if } s_i=1, \\\\\n",
    "  1-<s_i>_{emp}  & \\text{if } s_i=0\n",
    "\\end{cases}\n",
    "$$\n",
    "where $P_{\\hat h_i}(s_i)$ is the marginal probability that pixel $i$ has color $s_i$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5284c05b",
   "metadata": {},
   "source": [
    "Your answer here:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b4cc4c2",
   "metadata": {},
   "source": [
    "**2.5** Write a function `sample_independent` that takes as parameter the empirical mean $<s_i>_{emp}$ and *num_samples* which samples *num_samples* images $\\mathbf s$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f9dd726a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-10T13:16:33.306844Z",
     "iopub.status.busy": "2025-12-10T13:16:33.306636Z",
     "iopub.status.idle": "2025-12-10T13:16:33.325702Z",
     "shell.execute_reply": "2025-12-10T13:16:33.324733Z",
     "shell.execute_reply.started": "2025-12-10T13:16:33.306826Z"
    }
   },
   "outputs": [],
   "source": [
    "# Your code here:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1178e9ea",
   "metadata": {},
   "source": [
    "**2.6** Draw 5 samples with $\\mathbf h=\\hat{\\mathbf h}$ (i.e. with the probability distribution of 2.4) and plot them as images. What do you see ? Is it expected ? Comment briefly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fbc7e030",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-10T13:16:33.353406Z",
     "iopub.status.busy": "2025-12-10T13:16:33.353020Z",
     "iopub.status.idle": "2025-12-10T13:16:33.367944Z",
     "shell.execute_reply": "2025-12-10T13:16:33.366989Z",
     "shell.execute_reply.started": "2025-12-10T13:16:33.353383Z"
    }
   },
   "outputs": [],
   "source": [
    "# Your code here:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b6e240b",
   "metadata": {},
   "source": [
    "Your answer here:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38a74869",
   "metadata": {},
   "source": [
    "**2.7** Print the empirical mean $<\\mathbf{s}>_{emp}$ that you computed. What do you observe ? What does this indicate about the dataset ? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9c704900",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-10T13:16:33.550233Z",
     "iopub.status.busy": "2025-12-10T13:16:33.550010Z",
     "iopub.status.idle": "2025-12-10T13:16:33.553381Z",
     "shell.execute_reply": "2025-12-10T13:16:33.552474Z",
     "shell.execute_reply.started": "2025-12-10T13:16:33.550213Z"
    }
   },
   "outputs": [],
   "source": [
    "# Your code here:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31bfa42e",
   "metadata": {},
   "source": [
    "Your solution here:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4dc8e14e",
   "metadata": {},
   "source": [
    "# 3. Boltzmann Machines"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "917a96ec",
   "metadata": {},
   "source": [
    "The Boltzmann machine assumes the following energy function:\n",
    "$$\n",
    "E_{\\mathbf h, J}(\\mathbf s)=-\\sum_i h_i s_i-\\sum_{i<j} J_{ij} s_i s_j.\n",
    "$$\n",
    "$J\\in \\mathbb{R}^{d\\times d}$ are symmetric couplings ($J_{ij}=J_{ji}$), and $\\mathbf h$ is as before. These are the parameters that one wants to learn. Contrary to the independent field model, there is no analytical expression to obtain $\\hat J$ and $\\hat{\\mathbf h}$, the parameters that maximize the log-likelihood of this problem. Thus, we will estimate them using gradient descent (see also lecture 12 for more details):\n",
    "$$\n",
    "h_i(t+1)=h_i(t)-\\eta \\left( <s_i>_{model} - <s_i>_{emp}\\right),\n",
    "$$\n",
    "$$\n",
    "J_{ij}(t+1)=J_{ij}(t)-\\eta \\left( <s_i s_j>_{model} - <s_i s_j>_{emp}\\right).\n",
    "$$\n",
    "To obtain the quantities $<\\cdot>_{model}$, we will use Monte Carlo Markov Chains (MCMC). However, running a whole MCMC for each iteration of the gradient descent is computationally demanding. Instead of initializing the chain at a random $\\mathbf s$, we start $n$ chains on each data point and run only a few MCMC steps. This is called *contrastive divergence*.\n",
    "\n",
    "We will update these short MCMC using __Gibbs__ sampling, where each spin is resampled from its conditional distribution given all the others. The transition from a state $a$ to $b$ is in this case given by\n",
    "$$\n",
    "P(b\\rightarrow a)=\\frac{e^{-E(a)}}{e^{-E(a)}+e^{-E(b)}},\n",
    "$$\n",
    "where $a$ and $b$ differ at most at one pixel $s_i$. In the other cases the transition probability is $0$.\n",
    "Note that other sampling schemes are also possible, such as the Metropolis algorithm."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "146335ed",
   "metadata": {},
   "source": [
    "**3.1** Show that the Gibbs sampling Markov chain satisfies detailed balance for the stationary distribution given by the Boltzmann measure with energy $E_{\\mathbf h, J}(\\mathbf s)$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fdd3e70",
   "metadata": {},
   "source": [
    "Your answer here:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdac5c97",
   "metadata": {},
   "source": [
    "**3.2** We define $\\Delta E_i$ as the difference in energy between pixel $i$ being $0$ and $1$:\n",
    "$$\n",
    "    \\Delta E_i=E_{\\mathbf{h}, J}(s_i=0)-E_{\\mathbf{h}, J}(s_i=1)=h_i+\\sum_{j\\neq i}J_{ij}s_j\n",
    "$$\n",
    "Using the definition of the Boltzmann measure, show that\n",
    "$$\n",
    "    P_{\\mathbf h, J}(s_i=1 | s_{-i})=\\frac{1}{1+e^{-\\Delta E_i}}\n",
    "$$\n",
    "where $s_{-i}$ indicates all $s_j$ except for $j=i$ (i.e. we fix every pixel except pixel $i$)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75f53c40",
   "metadata": {},
   "source": [
    "Your solution here:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fc2b440",
   "metadata": {},
   "source": [
    "**3.3** Implement a function computing the sigmoid\n",
    "$$\n",
    "    \\sigma(x)=\\frac{1}{1+e^{-x}}.\n",
    "$$\n",
    "Make sure that the function also works if $x$ are 1D arrays."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4ba4f015",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-10T13:16:33.576584Z",
     "iopub.status.busy": "2025-12-10T13:16:33.576357Z",
     "iopub.status.idle": "2025-12-10T13:16:33.596270Z",
     "shell.execute_reply": "2025-12-10T13:16:33.595310Z",
     "shell.execute_reply.started": "2025-12-10T13:16:33.576563Z"
    }
   },
   "outputs": [],
   "source": [
    "# Your code here:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25b5824e",
   "metadata": {},
   "source": [
    "**3.4** We now estimate $<\\cdot>_{model}$ using MCMC. Write a function that takes as input *h*, *J*, the dataset *S* and *num_steps*. The function should:\n",
    "1. Initialize $\\mathbf s^\\mu$ in a configuration $\\mu$ from the dataset. \n",
    "2. Choose a pixel $i$ uniformly at random.\n",
    "3. Compute $p=\\sigma(\\Delta E_i)$ (Recall that $J_{ii}$ is defined to be $0$).\n",
    "4. Set $s_i^\\mu=1$ with probability $p$, and else $s_i^\\mu=0$.\n",
    "5. Repeat from step 2 *num_steps* times.\n",
    "\n",
    "This should be done for every initial configuration $\\mu$ from the dataset in a vectorized manner. Thus, your code should only contain a loop over *num_steps*.\n",
    "\n",
    "The function returns the updated *S* that we call $S_{new}$, an array of size $n\\times d$, where we recall that $n$ is the number of datapoints and that we have one Monte-Carlo chain for each datapoint.\n",
    "\n",
    "Note: In practice, the pixels are often updated synchronously and not randomly as asked above. This speeds up the computation, but can induce unwanted behaviours such as oscillations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5538a835",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-10T13:16:33.617832Z",
     "iopub.status.busy": "2025-12-10T13:16:33.617586Z",
     "iopub.status.idle": "2025-12-10T13:16:33.638051Z",
     "shell.execute_reply": "2025-12-10T13:16:33.637021Z",
     "shell.execute_reply.started": "2025-12-10T13:16:33.617812Z"
    }
   },
   "outputs": [],
   "source": [
    "# Your code here:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "234deceb",
   "metadata": {},
   "source": [
    "**3.5** Write a function that estimates $<s_i>_{model}$ and $<s_i s_j>_{model}$ from a given $S_{new}$, i.e. estimate the first 2 moments. You will also use this function to compute the empirical first and second moments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "07ec6435",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-10T13:16:33.666764Z",
     "iopub.status.busy": "2025-12-10T13:16:33.666535Z",
     "iopub.status.idle": "2025-12-10T13:16:33.683493Z",
     "shell.execute_reply": "2025-12-10T13:16:33.682576Z",
     "shell.execute_reply.started": "2025-12-10T13:16:33.666744Z"
    }
   },
   "outputs": [],
   "source": [
    "# Your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e172e374",
   "metadata": {},
   "source": [
    "**3.6** Implement the loop implementing the gradient descent:\n",
    "$$\n",
    "h_i(t+1)=h_i(t)-\\eta \\left( <s_i>_{model} - <s_i>_{emp}\\right),\n",
    "$$\n",
    "$$\n",
    "J_{ij}(t+1)=J_{ij}(t)-\\eta \\left( <s_i s_j>_{model} - <s_i s_j>_{emp}\\right).\n",
    "$$\n",
    "Choose $\\eta=0.5$ and do $1000$ gradient descent steps. Estimate $<\\cdot>_{model}$ using your previous code, with $300$ steps. Initialize the arrays for $\\mathbf h(t=0)$ and $J(t=0)$. Each element of $\\mathbf h$ should be $0$, while $J$ should be symmetric matrix, with no self coupling ($J_{ii}=0$), and the rest of the elements draw independently from a gaussian with mean $0$ and standard deviation $0.01$.\n",
    "\n",
    "After having updated $J$, you must force it to have $0$ on the diagonal and be symmetric. Indeed, this is a constraint of the BM model, and the gradient step does not guarantee that these properties are preserved.\n",
    "\n",
    "To monitor the convergence, print the reconstruction error\n",
    "$$\n",
    "E_{recon}=\\|S - S_{new}(t)\\|^2\n",
    "$$\n",
    "every 50 gradient descent step. $S$ is the data, while $S_{new}(t)$ are the generated samples used to compute $<\\cdot>_{model}$ at iteration $t$ of the gradient descent. $\\| \\cdot \\|$ is the Frobenius norm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "ecd942bf",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-10T13:16:33.703260Z",
     "iopub.status.busy": "2025-12-10T13:16:33.703001Z",
     "iopub.status.idle": "2025-12-10T13:16:33.723478Z",
     "shell.execute_reply": "2025-12-10T13:16:33.722523Z",
     "shell.execute_reply.started": "2025-12-10T13:16:33.703239Z"
    }
   },
   "outputs": [],
   "source": [
    "# Your code here:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcc1db34",
   "metadata": {},
   "source": [
    "**3.7** Now that the model is trained, we want to see what is has learned. To this end, we must sample from the distribution $P_{\\hat{\\mathbf h}, \\hat J}$. We do this by again running a MCMC. Reuse your code from 3.6, but this time initialize it with 25 configurations $\\mathbf s$ initialized uniformly at random. Now we also want the chain to mix, so run it for $10^4$ steps.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "ef2f4d88",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-10T13:18:45.889606Z",
     "iopub.status.busy": "2025-12-10T13:18:45.889391Z",
     "iopub.status.idle": "2025-12-10T13:18:45.894788Z",
     "shell.execute_reply": "2025-12-10T13:18:45.893666Z",
     "shell.execute_reply.started": "2025-12-10T13:18:45.889585Z"
    }
   },
   "outputs": [],
   "source": [
    "# Your code here:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7cf5141",
   "metadata": {},
   "source": [
    "**3.8** Plot the obtained samples. What do you observe ? Do you find some images that contain both horizontal and vertical bars ? If yes, explain briefly why this could happen. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "6a6805e8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-10T13:18:46.595373Z",
     "iopub.status.busy": "2025-12-10T13:18:46.595147Z",
     "iopub.status.idle": "2025-12-10T13:18:46.599325Z",
     "shell.execute_reply": "2025-12-10T13:18:46.598477Z",
     "shell.execute_reply.started": "2025-12-10T13:18:46.595352Z"
    }
   },
   "outputs": [],
   "source": [
    "# Your code here:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a18eea35",
   "metadata": {},
   "source": [
    "Your answer here :"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5f0950a",
   "metadata": {},
   "source": [
    "**3.9** Plot the trajectory of a single chain. Initialize it randomly, and plot what it looks like after $1, 100, 500, 10^3, 10^4, 10^5$ steps. Does the pattern significantly change between step $10^4$ and step $10^5$ ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "56755584",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-10T13:18:47.044188Z",
     "iopub.status.busy": "2025-12-10T13:18:47.043974Z",
     "iopub.status.idle": "2025-12-10T13:18:47.047315Z",
     "shell.execute_reply": "2025-12-10T13:18:47.046416Z",
     "shell.execute_reply.started": "2025-12-10T13:18:47.044168Z"
    }
   },
   "outputs": [],
   "source": [
    "# Your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d30f7e1e",
   "metadata": {},
   "source": [
    "Your answer here:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cac18b7",
   "metadata": {},
   "source": [
    "# 4. Restricted Boltzmann machines"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc97aba3",
   "metadata": {},
   "source": [
    "The Boltzmann machine can struggle to capture some structure in the data. Moreover, it can be costly to train all the pairwise interaction. To mitigate these problems, the restricted Boltzmann machine (RBM) was introduced. An RBM adds a layer of hidden units $s_j^h, j=1, ..., d_H$, and only connects the hidden and visible units $s_i^V, i=1, ..., d$.\n",
    "\n",
    "The Restricted Boltzmann machine (RBM) assumes the following energy function:\n",
    "$$\n",
    "E_{\\mathbf h, \\tilde{\\mathbf{h}} , W}(\\mathbf s^v, \\mathbf s^h)=-\\sum_{i=1}^{d} h_i s^V_i-\\sum_{j=1}^{d_H}\\tilde h_j s_j^H-\\sum_{i=1}^d \\sum_{j=1}^{d_H} W_{ij} s_i^V s^H_j.\n",
    "$$\n",
    "$\\mathbf s^V \\in \\{0,1\\}^d$ are the visible units (the image), $\\mathbf s^H\\in\\{0,1\\}^{d_H}$ are the hidden units, $W\\in \\mathbb{R}^{d\\times d_H}$ is the weight matrix connecting visible and hidden units, and $\\mathbf h\\in \\mathbb{R}^{d}$ and $\\tilde{\\mathbf{h}}\\in \\mathbb{R}^{d_H}$ are the visible and hidden biases (or external fields) vectors.\n",
    "\n",
    "Because there are no intra-layer connections, the conditional probabilities factorize nicely. This allows to perform __Block Gibbs Sampling__, updating all hidden units at once, then all visible units at once:\n",
    "$$\n",
    "P_{\\mathbf h, \\tilde{\\mathbf{h}}, W}(s_j^H=1| \\mathbf s^V)=\\sigma\\left(\\tilde h_j+\\sum_{i=1}^{d}W_{ij}s_i^V\\right),\n",
    "$$\n",
    "$$\n",
    "P_{\\mathbf h, \\tilde{\\mathbf{h}}, W}(s_i^V=1| \\mathbf s^H)=\\sigma\\left(h_i+\\sum_{j=1}^{d_H}W_{ij}s_j^H\\right).\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4d6a2bb",
   "metadata": {},
   "source": [
    "**4.1** Implement a function performing one step of the Block Gibbs Sampling. It should take the current visible state $\\mathbf s^V$, and parameters $W$, $\\mathbf h$ and $\\tilde{\\mathbf{h}}$ as inputs. Then\n",
    "1. Compute the probabilities for hidden units given $\\mathbf s^V$.\n",
    "2. Sample the hidden state $\\mathbf s^H$ from these probabilities.\n",
    "3. Compute the probabilities for the visible units given this new $\\mathbf s^H$.\n",
    "4. Sample a new visible state $\\mathbf s^V$.\n",
    "The function should return both the new visible (sampled in 4) and hidden state (sampled in 2).\n",
    "\n",
    "Note: we could continue the chain by again sampling the hidden variables with the new visible variables, etc. Doing this $k$ times is refered to CD-k (contrastive divergence with k steps). For now we keep $k=1$, but larger $k$ will be used in the next exercise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "56ab393e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-10T13:18:52.126199Z",
     "iopub.status.busy": "2025-12-10T13:18:52.125868Z",
     "iopub.status.idle": "2025-12-10T13:18:52.130903Z",
     "shell.execute_reply": "2025-12-10T13:18:52.129936Z",
     "shell.execute_reply.started": "2025-12-10T13:18:52.126176Z"
    }
   },
   "outputs": [],
   "source": [
    "# Your code here:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eddcfb34",
   "metadata": {},
   "source": [
    "**4.2** The gradient descent to obtain $\\hat W, \\hat{\\mathbf{h}}, \\hat{\\tilde{\\mathbf{h}}}$ that maximize the likelihood in this setting is given by\n",
    "$$\n",
    "W_{ij}(t+1)=W_{ij}(t)-\\eta(<s_i^V s_j^H>_{model}-<s_i^V s_j^H>_{emp}),\n",
    "$$\n",
    "$$\n",
    "h_{i}(t+1)=h_{i}(t)-\\eta(<s_i^V>_{model}-<s_i^V>_{emp}),\n",
    "$$\n",
    "$$\n",
    "\\tilde h_{j}(t+1)=\\tilde h_{j}(t)-\\eta(<s_j^H>_{model}-<s_j^H>_{emp}).\n",
    "$$\n",
    "Initialize $W$ with small random noise, and set the biases $\\mathbf h$ and $\\tilde{\\mathbf{h}}$ to zeros. Set $d_H=100$. Then, run GD for $1000$ steps with $\\eta=0.5$:\n",
    "1. Set $\\mathbf s^V$ to be a sample of the dataset.\n",
    "2. Compute the probability $P_{\\mathbf h, \\tilde{\\mathbf{h}}, W}(\\mathbf s^H_j=1| \\mathbf s^V)$ for each hidden unit, and sample $\\mathbf s^H$ from these probabilities. $S^H$ designates the concatenation of $\\mathbf s^H$ for every sample of the dataset, and is an array of size $n\\times d_H$.\n",
    "3. Do this for every sample of the dataset in a vectorized form, and compute $<W_{ij}>_{emp}\\approx \\frac{1}{n}S^T S^H$ as well as $<s_i^V>_{emp}$ and $<s_j^H>_{emp}$.\n",
    "4. Compute the probability $P_{\\mathbf h, \\tilde{\\mathbf{h}}, W}(\\mathbf s^V_i=1| \\mathbf s^H)$ for each visible unit, where $\\mathbf s^H$ is computed in step 2. Sample $\\mathbf s^V_{new}$ from this probability.\n",
    "5. Compute the probability $P_{\\mathbf h, \\tilde{\\mathbf{h}}, W}(\\mathbf s^H_i=1| \\mathbf s^V_{new})$ for each hidden unit, where $\\mathbf s^V_{new}$ is computed in step 4. Sample $\\mathbf s^H_{new}$ from this probability.\n",
    "6. Do this for every sample of the dataset in a vectorized form, and estimate $<W_{ij}>_{model}\\approx \\frac{1}{n}S_{new}^T S^H_{new}$ as well as $<s_i^V>_{model}$ and $<s_j^H>_{model}$. $S_{new}$ designates the concatenation of $\\mathbf s^V_{new}$ for every sample of the dataset and is of size $n\\times d_H$, and similarly for $S^H_{new}$ which is of size $n\\times d$.\n",
    "7. Perform the gradient step.\n",
    "To monitor the convergence, again print the reconstruction error\n",
    "$$\n",
    "E_{recon}=\\|S - S_{new}(t)\\|^2\n",
    "$$\n",
    "every 50 gradient descent step. $S$ is the data, while $S_{new}(t)$ are the generated samples used to compute $<\\cdot>_{model}$ at iteration $t$ of the gradient descent. $\\| \\cdot \\|$ is the Frobenius norm. \n",
    "\n",
    "Note: We do only one step of block Gibbs sampling. This is mainly for computational reasons, and has been shown to already yield good results. However, to estimate the $<\\cdot>_{model}$ averages accurately, this chain should be run until equilibration.\n",
    "\n",
    "Hint: you can use your previous implementation of the block Gibbs sampler to sample $S^H$ and $S_{new}$.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "87b2a5cf",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-10T13:18:52.155755Z",
     "iopub.status.busy": "2025-12-10T13:18:52.155508Z",
     "iopub.status.idle": "2025-12-10T13:18:52.174326Z",
     "shell.execute_reply": "2025-12-10T13:18:52.173381Z",
     "shell.execute_reply.started": "2025-12-10T13:18:52.155734Z"
    }
   },
   "outputs": [],
   "source": [
    "# Your code here:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8de881f4",
   "metadata": {},
   "source": [
    "**4.3** Generate new samples by running the Block Gibbs sampler for $10^4$ steps using the learnt parameters. Generate $25$ new samples and plot them. What do you observe ? Do you generate images with both vertical and horizontal lines ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "a10eac73",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-10T13:20:16.173077Z",
     "iopub.status.busy": "2025-12-10T13:20:16.121332Z",
     "iopub.status.idle": "2025-12-10T13:20:16.176311Z",
     "shell.execute_reply": "2025-12-10T13:20:16.175429Z",
     "shell.execute_reply.started": "2025-12-10T13:20:16.173047Z"
    }
   },
   "outputs": [],
   "source": [
    "# Your code here:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2a69e5a",
   "metadata": {},
   "source": [
    "Your answer here:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2099fc50",
   "metadata": {},
   "source": [
    "**4.4** Reproduce point 4.2 and 4.3 with only 10 hidden units. What do you observe ? Give a brief possible explanation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56474b41",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "101eed90",
   "metadata": {},
   "source": [
    "Your answer here:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b7d201c",
   "metadata": {},
   "source": [
    "# 5. Restricted Boltzmann machines on real data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66be9b10",
   "metadata": {},
   "source": [
    "**5.1** Load the dataset *mnist.npy* using the numpy `load` function. It contains black and white $28\\times 28$ images of hand-drawn digits 6 and 9. These images are taken from the MNIST dataset. Load the labels *mnist_labels.npy*, containing the labels (6 or 9) of each sample. Plot the first 5 images of the dataset, with as title the label of each image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "1b0f7827",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-10T13:20:42.600119Z",
     "iopub.status.busy": "2025-12-10T13:20:42.599884Z",
     "iopub.status.idle": "2025-12-10T13:20:42.603233Z",
     "shell.execute_reply": "2025-12-10T13:20:42.602312Z",
     "shell.execute_reply.started": "2025-12-10T13:20:42.600097Z"
    }
   },
   "outputs": [],
   "source": [
    "# Your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6d81669",
   "metadata": {},
   "source": [
    "**5.2** Flatten the data. Using numpy's built-in functions, compute the first 2 principal components of the dataset (i.e. the two right singular vectors with largest singular values). Make sure they are normalized with respect to the euclidian norm.\n",
    "\n",
    "Note: for this exercise, do not normalize or center the data. You can do the full singular value decomposition (even though we only need the 2 leading singular vectors). You can use numpy functions to compute this decomposition."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "79b76500",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-10T13:20:42.826242Z",
     "iopub.status.busy": "2025-12-10T13:20:42.826020Z",
     "iopub.status.idle": "2025-12-10T13:20:42.829450Z",
     "shell.execute_reply": "2025-12-10T13:20:42.828446Z",
     "shell.execute_reply.started": "2025-12-10T13:20:42.826220Z"
    }
   },
   "outputs": [],
   "source": [
    "# Your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d0f5eaa",
   "metadata": {},
   "source": [
    "**5.3** Project each datapoint on the first 2 principal components (as an example, see figure 2 from lecture 2). Plot the resulting points, coloring them in different colors for each class, i.e. one color for the 6s and another color for the 9s. What do you notice ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "b9e2a6f7",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-10T13:20:45.055853Z",
     "iopub.status.busy": "2025-12-10T13:20:45.055626Z",
     "iopub.status.idle": "2025-12-10T13:20:45.058836Z",
     "shell.execute_reply": "2025-12-10T13:20:45.057957Z",
     "shell.execute_reply.started": "2025-12-10T13:20:45.055833Z"
    }
   },
   "outputs": [],
   "source": [
    "# Your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40a10df2",
   "metadata": {},
   "source": [
    "Your answer here:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7859f7d0",
   "metadata": {},
   "source": [
    "**5.4** In the previous exercise, we used contrastive divergence with one step (CD-1), i.e. 1 step of Gibbs sampling. Implement the training using k-CD. This means that instead of computing $S^V_{new}$ and $S^H_{new}$ in only one step, you should repeat this procedure $k$ times.\n",
    "\n",
    "Train the model on the MNIST data for $k=5$, $\\eta=0.5$ and $200$ gradient descent steps. Initialize the visible and hidden biases/fields at $0$, and the coupling between the visible and hidden units to a small noise. Use $200$ hidden units. You can reuse or copy-paste previous code if needed.\n",
    "\n",
    "Note: executing this part of the code can take ~15-20 minutes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "451c3d01",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0739d52",
   "metadata": {},
   "source": [
    "__5.5__ Write a function computing the energy $E$, given the values of visible and hidden units, and parameters $W$, $\\mathbf{h}$ and $\\tilde{\\mathbf{h}}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60c20d82",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6c5c3f0",
   "metadata": {},
   "source": [
    "__5.6__ We now generate a new sample. Initialize it uniformly at random with $0$ and $1$ with $50\\%$ probability, and plot how it looks like after $k=1, 10, 100, 500, 1000$ steps of CD. Additionally, compute the energy at each step and save it in a list or array."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c51f14e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82b35fa8",
   "metadata": {},
   "source": [
    "__5.7__ Plot the energy computed previously as a function of the number of steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1564d8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f5b4589",
   "metadata": {},
   "source": [
    "__5.8__ Generate new samples by running the Block Gibbs sampler for $1000$ steps using the learnt parameters. Generate $500$ new samples and plot them the first $25$ obtained samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0594b4f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a97e99c",
   "metadata": {},
   "source": [
    "__5.9__ Project the obtained samples on the 2 principal components obtained before and plot them. Also plot the projection of the trained samples as before. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21bb599f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  },
  "unianalytics_cell_mapping": [
   [
    "abdb31d8",
    "abdb31d8"
   ],
   [
    "3815ca65",
    "3815ca65"
   ],
   [
    "fada1979",
    "fada1979"
   ],
   [
    "e65ff13b",
    "e65ff13b"
   ],
   [
    "ff13ecf9",
    "ff13ecf9"
   ],
   [
    "32f9620b",
    "32f9620b"
   ],
   [
    "1f5b0451",
    "1f5b0451"
   ],
   [
    "cf65bab9",
    "cf65bab9"
   ],
   [
    "9873c93b",
    "9873c93b"
   ],
   [
    "15e6b7f8",
    "15e6b7f8"
   ],
   [
    "015a94f1",
    "015a94f1"
   ],
   [
    "93d4a86b",
    "93d4a86b"
   ],
   [
    "ac8afc23",
    "ac8afc23"
   ],
   [
    "5f1c071c",
    "5f1c071c"
   ],
   [
    "c9c3b4a7",
    "c9c3b4a7"
   ],
   [
    "79126c45",
    "79126c45"
   ],
   [
    "07b8c270",
    "07b8c270"
   ],
   [
    "6f1d2d7f",
    "6f1d2d7f"
   ],
   [
    "f492af0d",
    "f492af0d"
   ],
   [
    "ae758925",
    "ae758925"
   ],
   [
    "b11745e5",
    "b11745e5"
   ],
   [
    "48dfa032",
    "48dfa032"
   ],
   [
    "67c62407",
    "67c62407"
   ],
   [
    "16c57bab",
    "16c57bab"
   ],
   [
    "5284c05b",
    "5284c05b"
   ],
   [
    "2b4cc4c2",
    "2b4cc4c2"
   ],
   [
    "f9dd726a",
    "f9dd726a"
   ],
   [
    "1178e9ea",
    "1178e9ea"
   ],
   [
    "fbc7e030",
    "fbc7e030"
   ],
   [
    "9b6e240b",
    "9b6e240b"
   ],
   [
    "38a74869",
    "38a74869"
   ],
   [
    "9c704900",
    "9c704900"
   ],
   [
    "31bfa42e",
    "31bfa42e"
   ],
   [
    "4dc8e14e",
    "4dc8e14e"
   ],
   [
    "917a96ec",
    "917a96ec"
   ],
   [
    "146335ed",
    "146335ed"
   ],
   [
    "0fdd3e70",
    "0fdd3e70"
   ],
   [
    "cdac5c97",
    "cdac5c97"
   ],
   [
    "75f53c40",
    "75f53c40"
   ],
   [
    "1fc2b440",
    "1fc2b440"
   ],
   [
    "4ba4f015",
    "4ba4f015"
   ],
   [
    "25b5824e",
    "25b5824e"
   ],
   [
    "5538a835",
    "5538a835"
   ],
   [
    "234deceb",
    "234deceb"
   ],
   [
    "07ec6435",
    "07ec6435"
   ],
   [
    "e172e374",
    "e172e374"
   ],
   [
    "ecd942bf",
    "ecd942bf"
   ],
   [
    "dcc1db34",
    "dcc1db34"
   ],
   [
    "ef2f4d88",
    "ef2f4d88"
   ],
   [
    "e7cf5141",
    "e7cf5141"
   ],
   [
    "6a6805e8",
    "6a6805e8"
   ],
   [
    "a18eea35",
    "a18eea35"
   ],
   [
    "c5f0950a",
    "c5f0950a"
   ],
   [
    "56755584",
    "56755584"
   ],
   [
    "d30f7e1e",
    "d30f7e1e"
   ],
   [
    "8cac18b7",
    "8cac18b7"
   ],
   [
    "dc97aba3",
    "dc97aba3"
   ],
   [
    "a4d6a2bb",
    "a4d6a2bb"
   ],
   [
    "56ab393e",
    "56ab393e"
   ],
   [
    "eddcfb34",
    "eddcfb34"
   ],
   [
    "87b2a5cf",
    "87b2a5cf"
   ],
   [
    "8de881f4",
    "8de881f4"
   ],
   [
    "a10eac73",
    "a10eac73"
   ],
   [
    "b2a69e5a",
    "b2a69e5a"
   ],
   [
    "2099fc50",
    "2099fc50"
   ],
   [
    "56474b41",
    "56474b41"
   ],
   [
    "101eed90",
    "101eed90"
   ],
   [
    "5b7d201c",
    "5b7d201c"
   ],
   [
    "66be9b10",
    "66be9b10"
   ],
   [
    "1b0f7827",
    "1b0f7827"
   ],
   [
    "c6d81669",
    "c6d81669"
   ],
   [
    "79b76500",
    "79b76500"
   ],
   [
    "1d0f5eaa",
    "1d0f5eaa"
   ],
   [
    "b9e2a6f7",
    "b9e2a6f7"
   ],
   [
    "40a10df2",
    "40a10df2"
   ],
   [
    "7859f7d0",
    "7859f7d0"
   ],
   [
    "451c3d01",
    "451c3d01"
   ],
   [
    "a0739d52",
    "a0739d52"
   ],
   [
    "60c20d82",
    "60c20d82"
   ],
   [
    "a6c5c3f0",
    "a6c5c3f0"
   ],
   [
    "c51f14e6",
    "c51f14e6"
   ],
   [
    "82b35fa8",
    "82b35fa8"
   ],
   [
    "c1564d8a",
    "c1564d8a"
   ],
   [
    "3f5b4589",
    "3f5b4589"
   ],
   [
    "0594b4f3",
    "0594b4f3"
   ],
   [
    "9a97e99c",
    "9a97e99c"
   ],
   [
    "21bb599f",
    "21bb599f"
   ]
  ],
  "unianalytics_notebook_id": "7a11aaa6-a41f-4425-b71f-a3407a1d33e4"
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
