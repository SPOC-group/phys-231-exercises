{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "38d6d76e",
   "metadata": {},
   "source": [
    "## Linear regression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f25d5658",
   "metadata": {},
   "source": [
    "In this session you will :\n",
    "* practice more on numpy arrays ;\n",
    "* implement linear regression to fit experimental data that follows an affine law ;\n",
    "* implement linear regression to fit experimental data that follows a more complex law ;\n",
    "* do high-dimensional classification with regularized linear regression and cross-validate the regularization strength.\n",
    "\n",
    "We need the usual librairies to handle arrays and to plot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ebeaed8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1df12b6",
   "metadata": {},
   "source": [
    "### 1 – Warm-up : a few more computations with arrays"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "373b87e3",
   "metadata": {},
   "source": [
    "Compute $\\sum_{n=1}^N\\frac{(-1)^{n+1}}{n}$ for $N=1000$ (without a `for` loop)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28852e8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "### to be completed\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b0c5773",
   "metadata": {},
   "source": [
    "Plot the function $x\\rightarrow\\log(1+\\cos(\\pi x))$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3ca4bf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "### to be completed\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73cf30cd",
   "metadata": {},
   "source": [
    "### 2 – Simple regression to fit an affine law"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f9e0e74",
   "metadata": {},
   "source": [
    "We are given data from an experiment. We want to analyse it, i.e. to propose a simple relation bewteen the quantities we measure. We consider an experiment that was given here as a TP. It consists in studying the efficiency and the charasteristic curve of a fuel cell. A fuel cell generates electricity from a chemical reaction. It delivers a stationary current $I$ at voltage $U$. We focus on the law between $U$ and $I$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e068faf",
   "metadata": {},
   "source": [
    "Measurements were made by students, to whom we are grateful. They varied the current $I$ and measured the tension $U$. The values are stored in the file DataH9.csv. This is a common simple format for saving experimental results you will have to deal with. There is one line per sample ; the measured values of each sample are separated by comma. In the following cell we open the file and print its content. You can see how it is made."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4de4d58",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"DataH9.csv\", 'rt') as f:\n",
    "    data = f.read()\n",
    "    \n",
    "print(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fac7e51",
   "metadata": {},
   "source": [
    "Convert this text file DataH9.csv to a numpy array. Use the function `np.loadtxt` with the optional arguments `delimiter=\",\"` and `skiprows=1` (to skip the first line). Print its dimensions ; how many samples do we have ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcdfaf42",
   "metadata": {},
   "outputs": [],
   "source": [
    "### to be completed\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e144f1a1",
   "metadata": {},
   "source": [
    "From the numpy array extract the column corresponding to $U$ and the column corresponding to $I$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c04fb078",
   "metadata": {},
   "outputs": [],
   "source": [
    "### to be completed\n",
    "U =\n",
    "I ="
   ]
  },
  {
   "cell_type": "markdown",
   "id": "001b6af0",
   "metadata": {},
   "source": [
    "Before doing regression we check that an affine fit is relevant. Plot the characteristic curve $U$ vs $I$. Since we want separated dots to see the different measurements we use the function `plt.scatter`, and not `plt.plot`, that draws continuous lines. Add labels to the axes (these are the same functions for `scatter` and `plot`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe82a5bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "### to be completed\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb4f7301",
   "metadata": {},
   "source": [
    "We want to model the relationship between $U$ and $I$ by the equation $U=aI+b$, where $a$ and $b$ are real number to be determined. In the course you saw that the least square formula prescribes $a=\\frac{\\bar{UI}-\\bar{U}\\bar{I}}{\\bar{I^2}-\\bar{I}^2}$ and $b=\\bar{U}-a\\bar{I}$, where $\\bar{x}$ is the mean of the array $x$ across the samples.\n",
    "\n",
    "Compute the fit given by the least squares, i.e. compute a and b. You can use the function `np.mean`. Print the computed coefficients $a$ and $b$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07860508",
   "metadata": {},
   "outputs": [],
   "source": [
    "### to be completed\n",
    "a = \n",
    "b =\n",
    "print("
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd119b1e",
   "metadata": {},
   "source": [
    "We want to see how good the fit is. Plot the equation $U=aI+b$ above the experimental datapoints. You can create a new array of $I$ with the function `np.linspace` and compute the corresponding $U$. Use both functions `plt.plot` and `plt.scatter`, add labels to the axes and to the curves. To label the curves use the optional argument `label=` in the two functions and call the function `plt.legend` at the end to print the legend."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3664ffa6",
   "metadata": {},
   "outputs": [],
   "source": [
    "### to be completed\n",
    "plt.scatter(\n",
    "Inew =\n",
    "plt.plot("
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ec01871",
   "metadata": {},
   "source": [
    "If you want for future projects you can save your plot with `plt.savefig`.\n",
    "\n",
    "If you have time you can redo the regression excluding the datapoints that are too close from $I=0$ or $I=2.5$, where $U$ does not have a linear behaviour."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f501b1d",
   "metadata": {},
   "source": [
    "### 3 – Regression to fit a polynomial"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50ed3777",
   "metadata": {},
   "source": [
    "In the experiment of the fuel cell we are interested in the power delivered by the cell. Approximatively, the power is $P=U\\times I$. Since we have a linear fit between $U$ and $I$ we guess there is a quadratic fit between $P$ and $I$. Plot the experimental curve $P$ vs $I$ to check."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb2aad1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "### to be completed\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "753debf2",
   "metadata": {},
   "source": [
    "As seen in the course, although we are doing linear regression we can fit functions more complex than a linear function. In our case we want a quadratic fit $P=aI^2+bI+c$. We need to introduce some formalism.\n",
    "\n",
    "Considering a sample $\\mu$ among the $N$ ones, we want a fit $y_\\mu=X_\\mu^Tw$ where $y_\\mu$ is the predicted value, $X_\\mu=(1, x_\\mu, x_\\mu^2, \\ldots, x_\\mu^p)$ are all the measured quantities on which $y_\\mu$ may depend and $w=(w_1, \\ldots, w_p)$ are the weights to be computed. It is linear in the sense that $y_\\mu$ depends linearly on $w$. In our case $y_\\mu=P_\\mu$, $X_\\mu=(1, I_\\mu, I_\\mu^2)$ and $w=(c, b, a)$.\n",
    "\n",
    "We stack the samples $\\mu$ into a vector $Y=(y_1, \\ldots, y_N)^T$ and a matrix $X=(X_1,\\ldots,X_N)^T\\in\\mathbb R^{N\\times p}$. The fit is $Y=Xw$. In the course you saw that the least square formula gives $w=(X^TX)^{-1}X^Ty$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e67bf2d8",
   "metadata": {},
   "source": [
    "The first step is to build $y$ and $X$. You can use the function `np.stack` to stack vectors to form a matrix. Check the resulting matrix has the right dimensions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52dff209",
   "metadata": {},
   "outputs": [],
   "source": [
    "### to be completed\n",
    "y =\n",
    "X =\n",
    "print("
   ]
  },
  {
   "cell_type": "markdown",
   "id": "195c3979",
   "metadata": {},
   "source": [
    "Compute $w$ given by the formula above. To invert the matrix you can use a function from the module `np.linalg`. Print the computed coefficients $a$, $b$ and $c$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17790701",
   "metadata": {},
   "outputs": [],
   "source": [
    "### to be completed\n",
    "w =\n",
    "print("
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8683da28",
   "metadata": {},
   "source": [
    "We want to see how good the fit is. Plot the equation $P=aI^2+bI+c$ on top of the experimental datapoints. You can create an array of $I_n$ with the function `np.linspace` ; you need then to construct a new $X_n$ and compute the predicted $y_n=X_nw$.\n",
    "\n",
    "Use both functions `plt.plot` and `plt.scatter`, add labels to the axes and to the curves."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "986f55e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "### to be completed\n",
    "plt.scatter(\n",
    "\n",
    "Inew =\n",
    "Xnew =\n",
    "yNew =\n",
    "plt.plot(\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce7b023d",
   "metadata": {},
   "source": [
    "Again if you have time you can redo the regression excluding the datapoints that are too close from $I=0$ or $I=2.5$, where $U$ does not have a linear behaviour. Where does the fuel cell deliver its greatest power ?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ace8985",
   "metadata": {},
   "source": [
    "### 4 – High-dimensionnal regression, regularization and cross-validation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1038d069",
   "metadata": {},
   "source": [
    "In the previous parts we did regression to estimate the parameters $a$, $b$ and $c$ of the fuel cell. These have a physical interpretation and they are only a few parameters to be fitted. We were less interested in estimating $U$ for unseen $I$.\n",
    "\n",
    "In this part we consider a different point of view.\n",
    "* First we want build a classifier of images $w$ that given a new image $X_\\mu$ will predict its class $y_\\mu$. This is different from regression in the sense that the regressed value $y$ is discrete (here ±1) and that we are more interested in predicting new values than interpreting $w$. Since we want to measure the performance of the classifier on unseen images we need a test set.\n",
    "* Second we consider a high-dimensional case where there are many parameters $w$ ; we will see that they will have to be regularized.\n",
    "* Third we do cross-validation to find the optimal regularization. We use a validation set."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d37b67b",
   "metadata": {},
   "source": [
    "More precisely, we use linear regression to classify images of hand-written digits. We want to classify them in odd vs even numbers. Each image $\\mu=1, \\ldots, N$ contains $d$ pixels and is encoded as a vector $X_\\mu\\in\\mathbb R^d$ (we do as in the previous exercise saving the image in a vector and not a matrix thus discarding its geometry). The value we want to regress is $y_\\mu=+1$ if $X_\\mu$ represents an even digit and $y_\\mu=-1$ if $X_\\mu$ is odd. We want to compute $w\\in\\mathbb R^d$ such that $y_\\mu=X_\\mu^Tw$ for the $N$ different $\\mu$. $d$ is large."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95cdc660",
   "metadata": {},
   "source": [
    "The dataset is stored in the files mnist.npy and mnist_labels.npy in a particular compressed format. In the following cell we open and save them in two arrays."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "914b73d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "mnist = np.load(\"mnist.npy\")\n",
    "mnist_labels = np.load(\"mnist_labels.npy\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "954198bf",
   "metadata": {},
   "source": [
    "How many samples do we have ? what is the shape of an image ? what is $d$ ? Plot a few of them. Add colourbars with `plt.colorbar`. For each image print its label as a title with `plt.title`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5773c12f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "### to be completed\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de17435f",
   "metadata": {},
   "source": [
    "We pre-process the dataset. We reshape and normalize the images so the values of the pixels vary between 0 and 1. The labels vary from 0 to 9 ; we map them into ±1 odd vs even labels (`%` is the remainder of the Euclidian division)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f5d5587",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.reshape(mnist, (len(mnist), -1))\n",
    "X = (X-np.min(X))/(np.max(X)-np.min(X))\n",
    "y = (mnist_labels%2)*2.0-1\n",
    "print(np.shape(X))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4198e24f",
   "metadata": {},
   "source": [
    "We need to split the dataset in three parts : the train set, the validation set and the test set. We use the train set to compute the optimal weights $w$ ; the validation set for cross-validation and the test set to measure the performance of the classifier. The classifier must not depend on the test set.\n",
    "\n",
    "The split must be a partition over the samples : each sample $(X_\\mu, y_\\mu)$ is assigned to one of these three sets. It must be random uniform (we do not want to have all the 0s in the train set and all the 1s the test set) and each set must be large enough, so the results do not depend on the particular split, as we will be able to see in the following. \n",
    "\n",
    "Complete the code of the split. We want 60% of samples in the train set, 20% in the validation set and 20% and the test set. We use the function `np.random.permutation` to generate a shuffled array of indices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1117dc29",
   "metadata": {},
   "outputs": [],
   "source": [
    "### to be completed\n",
    "indices = np.random.permutation(len(y))\n",
    "iTr = 3*len(y)//5\n",
    "iTe = \n",
    "X_train = X[indices[:iTr]]\n",
    "y_train = y[indices[:iTr]]\n",
    "X_val =\n",
    "y_val =\n",
    "X_test =\n",
    "y_test ="
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1721e926",
   "metadata": {},
   "source": [
    "First we try to regress $y=Xw$ without regularization on $w$. Work with the train set. Apply the formula you coded in the previous part. What happens ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99690997",
   "metadata": {},
   "outputs": [],
   "source": [
    "### to be completed\n",
    "w ="
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3cfac7a",
   "metadata": {},
   "source": [
    "`Singular matrix` means the matrix is not invertible. This is a high-dimensionnal task in the sense that one has more parameters than enough to fit the data. You can check this by computing the rank of $X$ and comparing it to the number of parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5b65e36",
   "metadata": {},
   "outputs": [],
   "source": [
    "### to be completed\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbcc33b9",
   "metadata": {},
   "source": [
    "We need to constrain the problem and add some regularization to the weights. For a regularization strength $\\lambda$ the ridge regression formula gives $w=(X^TX+\\lambda Id)^{-1}X^Ty$. Choose a $\\lambda$ and check there is no longer an error. (Warning : in Python \"lambda\" is a reserved word you should not use for a variable.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6aa73f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "### to be completed\n",
    "\n",
    "w ="
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef3cf4d5",
   "metadata": {},
   "source": [
    "Complete the following cell to plot the train loss vs the regularization $\\lambda$. We recall that the train loss is the regularized square loss (or _ridge loss_) $\\frac{1}{N_\\mathbb{train}}\\sum_\\mu^{N_\\mathbb{train}}(y_\\mu-\\hat y_\\mu)^2+\\frac{\\lambda}{N_\\mathbb{train}}\\sum_i^dw_i^2$ where $\\hat y_\\mu=X_\\mu^Tw$ is the predicted value. To save time you can compute $X^TX$ only once ; and at the beginning take around ten $\\lambda$s. It is better to generate them on a logarithm scale, with `np.geomspace` ; then for plotting we choose the scale with `plt.xscale`.\n",
    "\n",
    "For classification there is another relevant quantity, the accuracy $\\frac{1}{N}\\sum_\\mu\\delta_{y_\\mu, \\hat y_\\mu}$, where $\\hat y_\\mu=\\mathrm{sign}(X_\\mu^Tw)$ is the predicted label and $\\delta_{y,\\hat y}$ is 1 if $y=\\hat y$ and 0 otherwise. This is the fraction of correctly classified points. We plot it vs $\\lambda$ in another figure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba81a9c4",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "### to be completed\n",
    "losses = []\n",
    "accs = []\n",
    "lambdas =\n",
    "\n",
    "for l in lambdas:\n",
    "    w = \n",
    "    y_predicted =\n",
    "    losses.append(np.mean((y_predicted-y_train)**2)+l*np.sum(w**2)/len(y_train))\n",
    "    accs.append(np.mean(np.sign(y_predicted)==y_train))\n",
    "    \n",
    "plt.plot(\n",
    "plt.title(\"Train loss.\")\n",
    "plt.xscale('log')\n",
    "plt.show()\n",
    "plt.plot(\n",
    "plt.title(\"Train accuracy.\")\n",
    "plt.xscale('log')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a65ff38",
   "metadata": {},
   "source": [
    "We observe that the more we regularize the less we can well fit the train data. However this does not mean the classifier is worst at higher $\\lambda$s, as we are going to see. One has to evaluate it on the test data.\n",
    "\n",
    "We want to find the $\\lambda$ that maximizes the performance. We do cross-validation : for many $\\lambda$s we compute the ridge classifier on the train set ; we compute the accuracy it achieves on the validation set ; we pick the best $\\lambda$ and compute the final accuracy on the test set. We use the test set only once ; the classifier must not depend on it.\n",
    "\n",
    "Complete the following cell to compute and plot the train and validation losses and accuracies for many $\\lambda$s (at the beginning around ten, to save time)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb3f02fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "### to be completed\n",
    "d = np.shape(X)[1]\n",
    "losses_train, losses_val = [], []\n",
    "accs_train, accs_val = [], []\n",
    "lambdas =\n",
    "\n",
    "for l in lambdas:\n",
    "    w =\n",
    "    \n",
    "    y_predicted =\n",
    "    losses_train.append(np.mean((y_predicted-y_train)**2)+l*np.sum(w**2)/len(y_train))\n",
    "    accs_train.append(np.mean(np.sign(y_predicted)==y_train))\n",
    "    \n",
    "    y_predicted =\n",
    "    losses_val.append(np.mean((y_predicted-y_val)**2)+l*np.sum(w**2)/len(y_val))\n",
    "    accs_val.append(np.mean(np.sign(y_predicted)==y_val))\n",
    "\n",
    "plt.plot( , label=\"train\")\n",
    "plt.plot( , label=\"validation\")\n",
    "plt.xscale('log')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "plt.plot( , label=\"train\")\n",
    "plt.plot( , label=\"validation\")\n",
    "plt.xscale('log')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d90b3d1",
   "metadata": {},
   "source": [
    "The training accuracy decreases as you increase lambda, due to not fitting the data perfectly, but the accuracy on the validation set is non-monotone, showing a tradeoff between fitting the training set and avoid fitting it too much. We expect the validation and test accuracies to be close because $w$ depends little on the validation set."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdca4950",
   "metadata": {},
   "source": [
    "Pick the best $\\lambda$ (possibly with `np.argmax`) and compute the test loss and accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "457b41c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "### to be completed\n",
    "lambdaBest = \n",
    "w = \n",
    "y_predicted = \n",
    "loss_test = \n",
    "acc_test = \n",
    "print(\"The test loss is {:.2} and the test accuracy is {:.1%}.\".format(loss_test, acc_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a45003f3",
   "metadata": {},
   "source": [
    "If you have time you can recompute these quantities for a different realization of the random split. These should not vary too much. $N$ is still a bit small though."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
