{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5a910df-6ef8-4c5d-a684-e9e919a26cf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19fc1e77-ec8f-4b85-94a1-48fa00e063f3",
   "metadata": {},
   "source": [
    "# Exercise 6"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd360c7c-6914-4213-b849-252b12cbcd5b",
   "metadata": {},
   "source": [
    "### The superconductivity dataset\n",
    "\n",
    "The superconductivity dataset (Hamidieh, Computational Materials Science, 2018) contains $d=81$ chemical and molecular features extracted from 21263 superconductors along with the critical temperature (the label) in the last column. The goal is to predict the critical temperature from the features. If you are interested in the physical meaning of those, you can check out the original paper at https://arxiv.org/pdf/1803.10260.pdf\n",
    "\n",
    "In the first part we will predict the critical temperature using least-square linear regression; in the second part we will train a neural network by gradient descent to predict the temperature."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9e5cb08-07cb-416f-898b-edbeb5cf7769",
   "metadata": {},
   "source": [
    "## 0 Loading and pre-processing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "055d87ca-eb5c-44c5-9d42-0849a1bca4d7",
   "metadata": {},
   "source": [
    "Download the dataset from moodle or from `https://archive.ics.uci.edu/dataset/464/superconductivty+data` (24MB). The dataset we use is `train.csv` ; move it to the folder containing the notebook. In the following cell we load it. `data` contains the numerical values while `fields` of size $d+1$ is the name of the features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d637450-db59-45a8-baa8-afd5131bd636",
   "metadata": {},
   "outputs": [],
   "source": [
    "fields = np.loadtxt(\"train.csv\", delimiter=\",\", max_rows=1, dtype=str)\n",
    "data = np.loadtxt(\"train.csv\", delimiter=\",\", skiprows=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f9578e2-01ba-4855-9421-7c8f7f81b538",
   "metadata": {},
   "source": [
    "For each sample (each compound) the following features are given. The critical temperature we want to regress is the last one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e970444-e844-4bb7-a346-8f3d062d4534",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(fields)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec276c75-e32c-4c42-af27-8da7b0a0cf38",
   "metadata": {},
   "source": [
    "**0.1** How many samples do we have ? What is the critical temperature of the 10th sample?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a46ec1d-1893-4725-b41f-d9e0edbf3457",
   "metadata": {},
   "outputs": [],
   "source": [
    "# your answer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bcc8940-aaca-4340-9b5d-635435fcc0d1",
   "metadata": {},
   "source": [
    "**0.2** `data` is of size $(N, d+1)$. The last column is the critical temperature. Extract it to form two numpy arrays : $y$ of size $(N,)$ containing the critical temperature and $X$ of size $(N,d)$ containing the remaining data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "facc18f8-0e22-49e8-b9ab-3b8d25267e7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# your answer\n",
    "y = \n",
    "X = "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3feea50f-2d0a-43cd-a6d0-d353a8b272f6",
   "metadata": {},
   "source": [
    "**0.3** Because the different features correspond to different physical quantities and have different units, thus different magnitudes of numerical values, we preprocess the arrays.\n",
    "\n",
    "Compute $m_y$ the mean of $y$ and $\\sigma_y$ its standard deviation. Then normalize and update $y$ accroding to $y\\to (y-m_y)/\\sigma_y$, where $y-m_y$ means that we substract the mean $m_y$ from each component of the vector $y$. Do the same for each column of $X$ i.e. for the mean of each feature across all datapoints. Use numpy functions and no `for` loop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f138f9ef-ce18-4522-8256-c4b1fda9768e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# your answer\n",
    "mY, stdY =\n",
    "y =\n",
    "X ="
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65f3ce08-0579-4d3b-9292-5ad55099c52a",
   "metadata": {},
   "source": [
    "**0.4** Write a function `split` that given $X$ and $y$ returns a shuffled train, validation and test set with a 70%â€‘15%-15% split."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3568c29-15a7-4774-b4f8-df1bf9fcde73",
   "metadata": {},
   "outputs": [],
   "source": [
    "# your answer\n",
    "def split(X, y):\n",
    "    \n",
    "    return X_train, y_train, X_val, y_val, X_test, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06925992-8afb-4b16-a18b-5ed54bd43ded",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, y_train, X_val, y_val, X_test, y_test = split(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "925dc293-f78d-4650-8d6f-b15e2c885efe",
   "metadata": {},
   "source": [
    "## 1 Least-square linear regression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c236ad7-e3a3-46a4-adbd-92cfbcc548e0",
   "metadata": {},
   "source": [
    "In this part we predict $y$ using least-square regression. More precisely we express our predictor $\\hat{y}$ as a linear combination of the features $\\hat{y}=Xw$ with $w\\in\\mathbb{R}^d$. We search for $w$ that minimizes the mean square error $L_\\mathrm{train}$ between the predictions and the true values.\n",
    "$$L_\\mathrm{train}=\\frac{1}{2}\\frac{1}{\\#\\mathrm{train}}\\sum_{n\\in\\mathrm{train}}(y_n-\\hat{y}_n)^2 \\qquad\\qquad L_\\mathrm{test}=\\frac{1}{2}\\frac{1}{\\#\\mathrm{test}}\\sum_{n\\in\\mathrm{test}}(y_n-\\hat{y}_n)^2$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c8cbdec-ae3b-46b2-9c52-b1b5f0056fcf",
   "metadata": {},
   "source": [
    "**1.1** Compute $w^*$ the minimizer of $L$ using the formula for the least-square estimator without regularization on the train set. Compute $\\hat y_\\mathrm{train}$ and $\\hat y_\\mathrm{test}$ the predicted values for the train and test sets. Compute and print the mean errors $L_\\mathrm{train}$ and $L_\\mathrm{test}$ you obtain with this estimator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e9549c8-df2c-42d0-81e6-ec0581785f8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# your answer\n",
    "w = \n",
    "y_train_predicted = \n",
    "y_test_predicted = "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f7a430d-baf6-4a3e-aede-820037d1ec2a",
   "metadata": {},
   "source": [
    "**1.2** Plot the predicted temperature vs the true one for the samples from the test set. Use a scatter plot with small dots and add labels. Do not forget to rescale $y$ with $m_y$ and $\\sigma_y$ to obtain physical temperatures. Plot the line $y=\\hat y$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35ba935c-d65f-4f64-8bc0-6a426b8303b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# your answer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d24b373c-424c-4b01-a4bd-03804fd38196",
   "metadata": {},
   "source": [
    "**1.3** In this case it is not required to add a regularization term on $w$ ; justify why."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2768673f-c3ef-4213-af65-4863c6ce90b0",
   "metadata": {},
   "source": [
    "*your answer*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fc72891-eae9-48ab-9d47-302192c80fac",
   "metadata": {},
   "source": [
    "## 2 Small neural network and gradient descent"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1439d0e5-98bc-4245-bca4-98c98773cc55",
   "metadata": {},
   "source": [
    "One can construct better estimators by combining the columns of $X$ to create new features. For instance you already saw bits of polynomial features where we extend $X=(X_{i\\mu})$ with powers of the data $X_{i\\mu}^k$.\n",
    "\n",
    "In this part we follow a different way. We let a neural network automatically constructs features from data, by combining it in a non-linear manner. We consider a simple neural network with two layers. Given data $X\\in\\mathbb{R}^{M\\times d}$ it returns the prediction $\\hat y\\in\\mathbb{R}^M$ defined by\n",
    "$$\n",
    "\\hat y=f(X;W_1,W_2)=\\sigma(XW_2+\\mathrm{1_M}B^T)W_1\n",
    "$$\n",
    "where we set\n",
    "* $W_1\\in\\mathbb{R}^p$ and $W_2\\in\\mathbb{R}^{d\\times p}$ the two layers of weights,\n",
    "* $B\\in\\mathbb{R}^p$ the biaises, $\\mathrm{1_M}\\in\\mathbb{R}^M$ the vector full of ones, and $\\mathrm{1_M}B^T$ is the matrix where each row is $B$,\n",
    "* $p$ the number of hidden units,\n",
    "* $\\sigma$ is the activation function that acts element-wise on the matrix $XW_2+\\mathrm{1_M}B^T$. We take $\\sigma(x)=max(0,x)$.\n",
    "\n",
    "We consider the same training loss as in the first part :\n",
    "$$L_\\mathrm{train}(W_1,W_2,B)=\\frac{1}{2N'}\\sum_{n=1}^{N'}(y_{\\mathrm{train},n}-\\hat y_{\\mathrm{train},n})^2 \\qquad\\qquad \\hat y_\\mathrm{train}=f(X_\\mathrm{train};W_1,W_2,B) \\qquad\\qquad N'=\\#\\mathrm{train}$$\n",
    "We seek to minimize $L_\\mathrm{train}$ with respect to $W_1$, $W_2$ and $B$. For this, we use gradient descent. The descent has to be done on the three sets of parameters $W_1$, $W_2$ and $B$ at the same time beause we want to minimize $L$ with respect to each of them."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a66a908a-983a-4c34-915f-c940e5d53244",
   "metadata": {},
   "source": [
    "**2.1** In the image classification tutorial you saw that one can use the logistic loss instead of the square loss, and that it is often more efficient. Why here do we prefer the quadratic loss ?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "565d73ee-e23a-4844-bcb6-4f3486760192",
   "metadata": {},
   "source": [
    "*your answer*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30cb292a-ece9-471f-92de-8ebb1f078dae",
   "metadata": {},
   "source": [
    "**2.2** We do gradient descent on the parameters $W_1$, $W_2$ and $B$, with a step size $\\gamma$. We write $\\nabla_{W_1}L\\in\\mathbb{R}^p$, $\\nabla_{W_2}L\\in\\mathbb{R}^{d\\times p}$ and $\\nabla_{B}L\\in\\mathbb{R}^p$ the gradients of $L_\\mathrm{train}$ with respect to them. We define the gradient of a function $g$ with respect to a vector $A$ or a matrix $C$ as the vector or the matrix of same dimensions with\n",
    "$$\n",
    "(\\nabla_A g)_i = \\frac{\\partial g}{\\partial A_i} \\qquad\\qquad (\\nabla_C g)_{ij} = \\frac{\\partial g}{\\partial C_{ij}}\\ .\n",
    "$$\n",
    "Given $W_1^{(t)}$, $W_2^{(t)}$ and $B^{(t)}$ the values of $W_1$, $W_2$ and $B$ at iteration $t$ of gradient descent, write down their values $W_1^{(t+1)}$, $W_2^{(t+1)}$ and $B^{(t+1)}$ at the next iteration (one line for each)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "053c6eb4-7bc2-4390-a037-11582442d19c",
   "metadata": {},
   "source": [
    "*your answer*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae8adfba-8538-4826-a9ca-edeecfd9eeb3",
   "metadata": {},
   "source": [
    "**2.3** We use the chain rule to compute $\\nabla_{W_1}L$, $\\nabla_{W_2}L$ and $\\nabla_BL$. It states that\n",
    "$$\n",
    "\\nabla_{W_1}L = (\\nabla_{\\hat y_\\mathrm{train}}L)^T\\nabla_{W_1}f \\qquad\\qquad \\nabla_{W_2}L = (\\nabla_{\\hat y_\\mathrm{train}}L)^T\\nabla_{W_2}f \\qquad\\qquad \\nabla_BL = (\\nabla_{\\hat y_\\mathrm{train}}L)^T\\nabla_Bf\n",
    "$$\n",
    "with $\\nabla_{W_1}f\\in\\mathbb{R}^{N'\\times p}$, $\\nabla_{W_2}f\\in\\mathbb{R}^{N'\\times d\\times p}$ and $\\nabla_Bf\\in\\mathbb{R}^{N'\\times p}$ the gradients of $f$ with respect to $W_1$, $W_2$ and $B$.\n",
    "\n",
    "$\\nabla_{\\hat y_\\mathrm{train}}L$ is the gradient of $L_\\mathrm{train}$ with respect to $\\hat y_\\mathrm{train}$. Show in two lines that $\\nabla_{\\hat y_\\mathrm{train}}L=(\\hat y_\\mathrm{train}-y_\\mathrm{train})/N'$. Compute $\\nabla_{W_1}f$ and then $\\nabla_{W_1}L$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4610feb-b930-4deb-a528-f4d80a95730a",
   "metadata": {},
   "source": [
    "*your answer*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c2c2c9c-c9e3-4deb-884e-a8c1be898f83",
   "metadata": {},
   "source": [
    "**2.4** We give an implementation of $\\nabla_{W_2}L$ and $\\nabla_BL$. Implement functions that compute $\\sigma$, $\\sigma'$, $f$, $L$ and $\\nabla_{W_1}L$. They should be vectorized to be fast enough."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3e9eb39-70fd-4202-921f-8e372f54ca56",
   "metadata": {},
   "outputs": [],
   "source": [
    "# your answer\n",
    "def sigma(x):\n",
    "    \"\"\"\n",
    "    activation function\n",
    "    \"\"\"\n",
    "    return \n",
    "\n",
    "def deriv_sigma(x):\n",
    "    \"\"\"\n",
    "    derivative of the activation function\n",
    "    \"\"\"\n",
    "    return \n",
    "\n",
    "def neural_net(X,W_1,W_2,B): \n",
    "    \"\"\"\n",
    "    function implementing the neural network f\n",
    "    W_1 is a (p,) array\n",
    "    W_2 is a (d, p) array\n",
    "    B is a (p,) array\n",
    "    X is a (n, d) array\n",
    "    \"\"\"\n",
    "    return \n",
    "\n",
    "def loss(y,z):\n",
    "    \"\"\"\n",
    "    quadratic loss\n",
    "    y and z are (n,) arrays\n",
    "    \"\"\"\n",
    "    return \n",
    "\n",
    "def grad_W_1(X,W_1,W_2,B,y):\n",
    "    \"\"\"\n",
    "    gradient of the loss with respect to W_1\n",
    "    \"\"\"\n",
    "    return\n",
    "    \n",
    "def grad_W_2(X,W_1,W_2,B,y):\n",
    "    \"\"\"\n",
    "    gradient of the loss with respect to W_2\n",
    "    \"\"\"\n",
    "    pre_activations=X@W_2+B\n",
    "    z=sigma(pre_activations)@W_1\n",
    "    return np.einsum('n,p,np,nd->dp', z-y, W_1, deriv_sigma(pre_activations), X)/len(y)\n",
    "\n",
    "def grad_B(X,W_1,W_2,B,y):\n",
    "    \"\"\"\n",
    "    gradient of the loss with respect to B\n",
    "    \"\"\"\n",
    "    pre_activations=X@W_2+B\n",
    "    z=sigma(pre_activations)@W_1\n",
    "    return (z-y)@(deriv_sigma(pre_activations)*W_1)/len(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60474e72-d717-4b27-93e1-4f207bf9c56d",
   "metadata": {},
   "source": [
    "**2.5** Implement the gradient descent. You shall :\n",
    "* compute and save the value of the train loss at each iteration ; same for the validation loss ;\n",
    "* stop the gradient descent after a fixed well-chosen number $t_\\mathrm{max}$ of iterations ;\n",
    "* plot the train and validation losses with labels and a legend, using a log-log scale ;\n",
    "* tune $\\gamma$ to obtain good results with reasonable computing time. Remember that large $\\gamma$ is unstable and small $\\gamma$ is slow.\n",
    "\n",
    "You should :\n",
    "* start from weights randomly initialised, each component of $W_1$ as a Gaussian with zero mean and variance 1 and each component of $W_2$ as a Gaussian with zero mean and variance $1/d$. Take $B$ all zeros ;\n",
    "* take $p=5$ ;\n",
    "* check that the train loss decreases ;\n",
    "* while debugging your code take $t_\\mathrm{max}$ not too large. Once it runs correctly take a larger $t_\\mathrm{max}$ to obtain a better test loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bde1fc7-6a04-42e0-b292-d84e5d3aa7c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# your answer\n",
    "tMax=\n",
    "p=5\n",
    "gamma=\n",
    "W_1=\n",
    "W_2=\n",
    "B=\n",
    "train_losses=[]\n",
    "val_losses=[]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f33d34a7-87cf-4fcc-b53b-08044e253c1c",
   "metadata": {},
   "source": [
    "**2.6** Once we have optimized the training compute the test loss of the trained model and print it. In this case does the model overfit ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b770540-1f96-4c10-86f0-922099124a3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# your answer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31ed425b-4829-4716-baf3-8effb482ef0e",
   "metadata": {},
   "source": [
    "**2.7** Using your trained neural network predict the temperature of the 10th sample of the test set and compare to the true value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df254099-fdd4-45b2-8c66-d0281a439091",
   "metadata": {},
   "outputs": [],
   "source": [
    "# your answer"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "unianalytics_cell_mapping": [
   [
    "e5a910df-6ef8-4c5d-a684-e9e919a26cf3",
    "e5a910df-6ef8-4c5d-a684-e9e919a26cf3"
   ],
   [
    "19fc1e77-ec8f-4b85-94a1-48fa00e063f3",
    "19fc1e77-ec8f-4b85-94a1-48fa00e063f3"
   ],
   [
    "b0123981-2070-4f12-9a2a-923f12673d5c",
    "b0123981-2070-4f12-9a2a-923f12673d5c"
   ],
   [
    "bd360c7c-6914-4213-b849-252b12cbcd5b",
    "bd360c7c-6914-4213-b849-252b12cbcd5b"
   ],
   [
    "b9e5cb08-07cb-416f-898b-edbeb5cf7769",
    "b9e5cb08-07cb-416f-898b-edbeb5cf7769"
   ],
   [
    "055d87ca-eb5c-44c5-9d42-0849a1bca4d7",
    "055d87ca-eb5c-44c5-9d42-0849a1bca4d7"
   ],
   [
    "5d637450-db59-45a8-baa8-afd5131bd636",
    "5d637450-db59-45a8-baa8-afd5131bd636"
   ],
   [
    "2f9578e2-01ba-4855-9421-7c8f7f81b538",
    "2f9578e2-01ba-4855-9421-7c8f7f81b538"
   ],
   [
    "1e970444-e844-4bb7-a346-8f3d062d4534",
    "1e970444-e844-4bb7-a346-8f3d062d4534"
   ],
   [
    "ec276c75-e32c-4c42-af27-8da7b0a0cf38",
    "ec276c75-e32c-4c42-af27-8da7b0a0cf38"
   ],
   [
    "3a46ec1d-1893-4725-b41f-d9e0edbf3457",
    "3a46ec1d-1893-4725-b41f-d9e0edbf3457"
   ],
   [
    "0bcc8940-aaca-4340-9b5d-635435fcc0d1",
    "0bcc8940-aaca-4340-9b5d-635435fcc0d1"
   ],
   [
    "facc18f8-0e22-49e8-b9ab-3b8d25267e7c",
    "facc18f8-0e22-49e8-b9ab-3b8d25267e7c"
   ],
   [
    "3feea50f-2d0a-43cd-a6d0-d353a8b272f6",
    "3feea50f-2d0a-43cd-a6d0-d353a8b272f6"
   ],
   [
    "f138f9ef-ce18-4522-8256-c4b1fda9768e",
    "f138f9ef-ce18-4522-8256-c4b1fda9768e"
   ],
   [
    "65f3ce08-0579-4d3b-9292-5ad55099c52a",
    "65f3ce08-0579-4d3b-9292-5ad55099c52a"
   ],
   [
    "f3568c29-15a7-4774-b4f8-df1bf9fcde73",
    "f3568c29-15a7-4774-b4f8-df1bf9fcde73"
   ],
   [
    "06925992-8afb-4b16-a18b-5ed54bd43ded",
    "06925992-8afb-4b16-a18b-5ed54bd43ded"
   ],
   [
    "925dc293-f78d-4650-8d6f-b15e2c885efe",
    "925dc293-f78d-4650-8d6f-b15e2c885efe"
   ],
   [
    "1c236ad7-e3a3-46a4-adbd-92cfbcc548e0",
    "1c236ad7-e3a3-46a4-adbd-92cfbcc548e0"
   ],
   [
    "0c8cbdec-ae3b-46b2-9c52-b1b5f0056fcf",
    "0c8cbdec-ae3b-46b2-9c52-b1b5f0056fcf"
   ],
   [
    "9e9549c8-df2c-42d0-81e6-ec0581785f8a",
    "9e9549c8-df2c-42d0-81e6-ec0581785f8a"
   ],
   [
    "4f7a430d-baf6-4a3e-aede-820037d1ec2a",
    "4f7a430d-baf6-4a3e-aede-820037d1ec2a"
   ],
   [
    "35ba935c-d65f-4f64-8bc0-6a426b8303b1",
    "35ba935c-d65f-4f64-8bc0-6a426b8303b1"
   ],
   [
    "d24b373c-424c-4b01-a4bd-03804fd38196",
    "d24b373c-424c-4b01-a4bd-03804fd38196"
   ],
   [
    "2768673f-c3ef-4213-af65-4863c6ce90b0",
    "2768673f-c3ef-4213-af65-4863c6ce90b0"
   ],
   [
    "3fc72891-eae9-48ab-9d47-302192c80fac",
    "3fc72891-eae9-48ab-9d47-302192c80fac"
   ],
   [
    "1439d0e5-98bc-4245-bca4-98c98773cc55",
    "1439d0e5-98bc-4245-bca4-98c98773cc55"
   ],
   [
    "a66a908a-983a-4c34-915f-c940e5d53244",
    "a66a908a-983a-4c34-915f-c940e5d53244"
   ],
   [
    "565d73ee-e23a-4844-bcb6-4f3486760192",
    "565d73ee-e23a-4844-bcb6-4f3486760192"
   ],
   [
    "30cb292a-ece9-471f-92de-8ebb1f078dae",
    "30cb292a-ece9-471f-92de-8ebb1f078dae"
   ],
   [
    "053c6eb4-7bc2-4390-a037-11582442d19c",
    "053c6eb4-7bc2-4390-a037-11582442d19c"
   ],
   [
    "ae8adfba-8538-4826-a9ca-edeecfd9eeb3",
    "ae8adfba-8538-4826-a9ca-edeecfd9eeb3"
   ],
   [
    "e4610feb-b930-4deb-a528-f4d80a95730a",
    "e4610feb-b930-4deb-a528-f4d80a95730a"
   ],
   [
    "6c2c2c9c-c9e3-4deb-884e-a8c1be898f83",
    "6c2c2c9c-c9e3-4deb-884e-a8c1be898f83"
   ],
   [
    "a3e9eb39-70fd-4202-921f-8e372f54ca56",
    "a3e9eb39-70fd-4202-921f-8e372f54ca56"
   ],
   [
    "60474e72-d717-4b27-93e1-4f207bf9c56d",
    "60474e72-d717-4b27-93e1-4f207bf9c56d"
   ],
   [
    "7bde1fc7-6a04-42e0-b292-d84e5d3aa7c7",
    "7bde1fc7-6a04-42e0-b292-d84e5d3aa7c7"
   ],
   [
    "f33d34a7-87cf-4fcc-b53b-08044e253c1c",
    "f33d34a7-87cf-4fcc-b53b-08044e253c1c"
   ],
   [
    "9b770540-1f96-4c10-86f0-922099124a3a",
    "9b770540-1f96-4c10-86f0-922099124a3a"
   ],
   [
    "31ed425b-4829-4716-baf3-8effb482ef0e",
    "31ed425b-4829-4716-baf3-8effb482ef0e"
   ],
   [
    "df254099-fdd4-45b2-8c66-d0281a439091",
    "df254099-fdd4-45b2-8c66-d0281a439091"
   ]
  ],
  "unianalytics_notebook_id": "f7600bcb-19c3-415c-b42c-052b903d6f8e"
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
