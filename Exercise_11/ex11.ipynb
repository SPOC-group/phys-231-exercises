{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "415b9df6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25adfac2",
   "metadata": {},
   "source": [
    "## Part 1 : errors in least-square regression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bf32d72",
   "metadata": {},
   "source": [
    "### Experiment\n",
    "\n",
    "We work on data obtained in the TP « Banc mécanique » that was gracefully shared with us. We consider a glider on a air-cushion bench. We measure its speed $v$ and we want to evaluate the laminar friction $f=-\\beta v$ it undergoes. We perform least square regression on $f$ and $v$ to compute $\\beta$.\n",
    "\n",
    "We set `l` the length of the glider ; `m` its mass ; `L` the distance between the two points of measurement.\n",
    "\n",
    "In the following cell the experimental measurements can be loaded. There are two setups ; you can choose one of the two files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9cf7604",
   "metadata": {},
   "outputs": [],
   "source": [
    "m = 0.142\n",
    "L = 0.79\n",
    "\n",
    "donnees = np.loadtxt(\"ex_11_données.csv\", delimiter=',')\n",
    "l = 0.091\n",
    "\n",
    "donnees = np.loadtxt(\"ex_11_donnéesBis.csv\", delimiter=',')\n",
    "l = 0.09"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcf98047",
   "metadata": {},
   "source": [
    "The experiment is repeated $n$ times ($n$ is about twenty) with various initial speeds. For the $\\mu$th repetition the measurements consist in four time intervals $t_{\\mu,1}$, $t_{\\mu,2}$, $t_{\\mu,3}$ and $t_{\\mu,4}$. They correspond to the times to go through the optical gates on the banch. In the following cell we unpack these. Print a few of them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30be7783",
   "metadata": {},
   "outputs": [],
   "source": [
    "t1, t2, t3, t4 = donnees[:,0], donnees[:,1], donnees[:,2], donnees[:,3]\n",
    "t1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffbb8150",
   "metadata": {},
   "source": [
    "For each $\\mu$ the speeds at these points are $v_{\\mu,a}=\\mathrm{l}/t_{\\mu,a}$ (the length of the glider divided by the duration). We define an average speed $\\bar v_\\mu=(v_{\\mu,1}+v_{\\mu,2}+v_{\\mu,3}+v_{\\mu,4})/4$. The average force is $\\bar f_\\mu=\\frac{\\mathrm{m}}{\\mathrm{L}}(v_{\\mu,1}^2-v_{\\mu,2}^2+v_{\\mu,3}^2-v_{\\mu,4}^2)/4$.\n",
    "\n",
    "Compute $\\bar v_\\mu$ and $\\bar f_\\mu$ ; plot the points $(\\bar v_\\mu, \\bar f_\\mu)$ as dots."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22ed2326",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d737f0ea",
   "metadata": {},
   "source": [
    "### Least squares"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d62b1edb",
   "metadata": {},
   "source": [
    "Perform the linear regression $\\bar f=\\beta\\bar v+c$ on the scalars $\\beta$ and $c$. Use least squares for arbitrary dimension : you should first create a matrix $X$ by stacking a column of 1s with the column of $\\bar v$ (check your resulting matrix is correct) ; the fit is then $\\bar f=Xw$ with\n",
    "$$X=\\pmatrix{1 & \\bar v_1 \\\\ \\vdots & \\vdots \\\\ 1 & \\bar v_n} \\quad\\mathrm{and}\\quad w=\\pmatrix{c \\\\ \\beta}\\ .$$\n",
    "We write $\\hat w=(\\hat c, \\hat\\beta)^T$ the computed estimator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "332586d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = ...\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77493bda",
   "metadata": {},
   "source": [
    "Plot the predicted $\\hat f=X\\hat w=\\hat\\beta\\bar v+\\hat c$ vs $\\bar v$ (as a continuous line) as well as the experimental datapoints (as dots)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9ca0a0d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "00e0d9ae",
   "metadata": {},
   "source": [
    "Compute the error on the estimated $\\hat\\beta$ with the formula of the course\n",
    "$$\\hat\\Delta^2=\\frac{1}{n}\\sum_\\mu^n(\\bar f_\\mu-\\hat f_\\mu)^2$$\n",
    "$$\\mathrm{Var}(\\hat w_i)=\\hat\\Delta^2[(X^TX)^{-1}]_{ii}$$\n",
    "Print the resulting estimation with a nice formating such that $\\beta=\\mathtt{x.xx±0.0y}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d05c0077",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a678c3c2",
   "metadata": {},
   "source": [
    "### Error bars and weigthed least squares"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb26f423",
   "metadata": {},
   "source": [
    "We have errors $\\Delta_\\mathtt{l}$, $\\Delta_\\mathtt{m}$, $\\Delta_\\mathtt{L}$ and $\\Delta_t$ on the measurements `l`, `m`, `L` and $t_a$. They have been estimated to the following values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b33a39a",
   "metadata": {},
   "outputs": [],
   "source": [
    "delta_l = 0.001\n",
    "delta_m = 0.001\n",
    "delta_L = 0.001\n",
    "delta_t = 0.001"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cf3a55c",
   "metadata": {},
   "source": [
    "We want an estimate of the error on each datapoint. Propagate the errors on $\\bar v$ and $\\bar f$ with the most generic formula to show that\n",
    "$$\\Delta_{\\bar f} = \\bar f\\left(\\frac{\\Delta_\\mathtt{m}}{\\mathtt{m}}+\\frac{\\Delta_\\mathtt{L}}{\\mathtt{L}}\\right)+2\\bar f\\left(\\Delta_t\\left(\\frac{1}{t_1}+\\frac{1}{t_2}+\\frac{1}{t_3}+\\frac{1}{t_4}\\right)+4\\frac{\\Delta_\\mathtt{l}}{\\mathtt{l}}\\right)\\ .$$\n",
    "Compute $\\Delta_{\\bar f}$ for each datapoint."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94ba3b47",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "32a93894",
   "metadata": {},
   "source": [
    "We do not consider the errors on $\\bar v$. Perform the linear regression $\\bar f=\\beta\\bar v+c$ with weigthed least squares as seen in the course (part 10.2 in the lecture notes). You can use `np.diag` to construct the diagonal matrix $\\Omega$ (eq. 10.26) from the different $\\Delta_{\\bar f}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7b45e5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = ...\n",
    "O = ...\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "150e73ec",
   "metadata": {},
   "source": [
    "Plot the regression. For the datapoints add errorbars on the dots with `plt.errorbar(..., yerr=...)`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e80ca75",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "954940ac",
   "metadata": {},
   "source": [
    "Compute the error on the estimated $\\hat\\beta$ with the formula of the course\n",
    "$$\\mathrm{Var}(\\hat w_i)=[(X^T\\Omega X)^{-1}]_{ii}\\ .$$\n",
    "Print the resulting estimation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0914e5fd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "6e1450f0",
   "metadata": {},
   "source": [
    "We can check that our result is consistent. Compute\n",
    "$$\\mathcal L(\\hat w)=\\frac{1}{n}\\sum_\\mu^n\\frac{1}{\\Delta_{\\bar f, \\mu}^2}(\\bar f_\\mu-\\hat f_\\mu)^2$$\n",
    "and check that $\\mathcal L(\\hat w)\\approx 1$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d8c33d1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "7088a849",
   "metadata": {},
   "source": [
    "Redo the previous step dividing or multiplying the estimated errors $\\Delta_\\mathtt{l}$, $\\Delta_\\mathtt{m}$, $\\Delta_\\mathtt{L}$ and $\\Delta_t$ by a factor five. What do you observe on $\\mathcal L(\\hat w)$ ?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a984aa0",
   "metadata": {},
   "source": [
    "## Part 2 : toward high dimension"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfb1ad66",
   "metadata": {},
   "source": [
    "We want to study how these estimators behave when the dimension i.e. the number of parameters is growing. In this part we use an artificial dataset to be able to compare our estimations to a ground truth while varying $D$ and $N$. We will see that, even if we know exactly how data was generated, when the dimension $D$ is larger the number $N$ of datapoints must be much larger for the estimators to be correct."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4084f9f6",
   "metadata": {},
   "source": [
    "### Setting\n",
    "Data : we have a train set $X\\in\\mathbb R^{N\\times D}$ and a test set $X'\\in\\mathbb R^{N'\\times D}$. $N$ is the size of the train set and $N'$ the size of the test set. We take the components $X_{\\mu i}, X_{\\mu i}'$ of the data drawn according to independent centered Gaussians.\n",
    "\n",
    "There is a teacher $w^*\\in\\mathbb R^D$ that outputs labels $y_\\mu=X_\\mu^Tw^*+\\epsilon_\\mu$ and $y_\\mu'=X_\\mu^{'T}w^*+\\epsilon'_\\mu$, where the $\\epsilon_\\mu, \\epsilon_\\mu' \\sim\\mathcal N(0,\\Delta^2)$ are independent and account for noise or experimental errors. We take the compoments of $w^*$ drawn according to independent standard Gaussians. $w^*$ is the ground truth ; it is not observed. We can compare how close to it is our empirical estimator.\n",
    "\n",
    "We perform linear regression : our estimator of the coefficients is $\\hat w=(X^TX)^{-1}X^Ty$. The predicted labels are then $\\hat y_\\mu=X_\\mu^T\\hat w$ and $\\hat y_\\mu'=X_\\mu^{'T}\\hat w$.\n",
    "\n",
    "We can study different errors : the train error $E_\\mathrm{train} = \\frac{1}{N}\\sum_\\mu^N(y_\\mu-\\hat y_\\mu)^2$ (how well data is adjusted), the test error $E_\\mathrm{test} = \\frac{1}{N'}\\sum_\\mu^{N'}(y_\\mu'-\\hat y_\\mu')^2$ (how well unseen data will be fitted), as well as the true error on the coefficients (the mean square error) $\\mathrm{MSE} = \\frac{1}{D}\\sum_i^D(w^*_i-\\hat w_i)^2$ (how well the hidden parameters are reconstructed).\n",
    "\n",
    "$E_\\mathrm{train}$ is an estimator of $E_\\mathrm{test}$, $\\hat\\Delta^2=\\frac{1}{N}\\sum_\\mu^N(y_\\mu-\\hat y_\\mu)^2=E_\\mathrm{train}$ an estimator of $\\Delta^2$ and $\\widehat{\\mathrm{MSE}}=\\hat\\Delta^2\\frac{1}{D}\\sum_i^D(X^TX)^{-1}_{ii}$ an estimator of $\\mathrm{MSE}$.\n",
    "We want to study how good they are, depending on the number of datapoints $N$ and the dimension $D$ of the problem.\n",
    "\n",
    "We will always take $N'$ fixed large so the empirical $E_\\mathrm{test}$ concentrates to the true test error.\n",
    "\n",
    "In part 1 we had $\\hat w=(c,\\beta)^T$, $D=2$ and $N\\approx 20$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4a16bec",
   "metadata": {},
   "outputs": [],
   "source": [
    "Ntest = int(10**4)\n",
    "Delta = 1.5"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa560980",
   "metadata": {},
   "source": [
    "### low-dimensional : D=5"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f75fb21",
   "metadata": {},
   "source": [
    "In the following cell, for $N$ going from 10 to $N_\\mathrm{max}=5000$ :\n",
    "- generate the data $X$ and the train labels $y_\\mu$ according to the teacher model ;\n",
    "- compute the least square estimator $\\hat w$ on $X$ and $y_\\mu$ and the predicted train and test labels $\\hat y_\\mu$ and $\\hat y_\\mu'$ ;\n",
    "- compute the train and test errors $E_\\mathrm{train}$ and $E_\\mathrm{test}$ as well as $\\widehat{\\mathrm{MSE}}$ and $\\mathrm{MSE}$. Store these values in lists.\n",
    "\n",
    "The $X_{\\mu,i}$ and the $X'_{\\mu,i}$ are drawn according to a centered normal of variance $1/D$. The $w_i^*$ are drawn according to a standard normal. $w^*$ and $X'$ are fixed at the beginning. $X$ should not vary in the sense that going from $N$ to $N+1$ only one datapoint $x\\in\\mathbb R^D$ is added to $X$. We study the behaviour of the estimators while increasing the number of datapoints we use to compute them.\n",
    "\n",
    "(hint : if you prefer, you can generate the $N_\\mathrm{max}$ datapoints `Xtot` first and, in the loop, select the right ones with `X = Xtot[:N,:]`. To generate the different $N$ you can use `np.geomspace(..., dtype=int)` ; take a hundred of $N$.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f5c9209",
   "metadata": {},
   "outputs": [],
   "source": [
    "D = 5\n",
    "Nmax = 5000\n",
    "\n",
    "Xtest = np.random.normal(...\n",
    "w = np.random.normal(..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26fecc4d",
   "metadata": {},
   "source": [
    "In a figure plot the curves $E_\\mathrm{train}$ and $E_\\mathrm{test}$ vs $N$. Add the line $E=\\Delta^2$.\n",
    "\n",
    "In another figure plot the curves $\\widehat{\\mathrm{MSE}}$ and $\\mathrm{MSE}$ vs $N$. Add the line $N\\to 1/N$.\n",
    "\n",
    "You should use a logarithm axis for $N$ and the MSEs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0544658",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "f8ec1cad",
   "metadata": {},
   "source": [
    "Comment on these graphs (you can re-run your code once or twice) :\n",
    "- at small $N$, does the train error over-estimates or under-estimates the actual error ?\n",
    "- is $\\hat\\Delta^2$ a consistent estimator ?\n",
    "- is $\\widehat{\\mathrm{MSE}}$ a consistent estimator ?\n",
    "- what is its convergence rate ?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4132f8f",
   "metadata": {},
   "source": [
    "### high-dimensional : D=200"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4ed5828",
   "metadata": {},
   "source": [
    "Redo the previous questions for $D=200$. Take $N$ going from 250 to 10000 (use only fifty values)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4728b523",
   "metadata": {},
   "outputs": [],
   "source": [
    "D = 200\n",
    "Nmax = 10000\n",
    "\n",
    "Xtest = np.random.normal(...\n",
    "w = np.random.normal(..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a879c9f",
   "metadata": {},
   "source": [
    "Comments :\n",
    "- observe the convergence of the estimators ;\n",
    "- is $N=1000$ still large enough ?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0166ccd8",
   "metadata": {},
   "source": [
    "### Scaling limit"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bfb249c",
   "metadata": {},
   "source": [
    "For $D$ larger we need $N$ larger for the estimators to be close to the quatities they estimate. We wonder how these two numbers are related, how the limit $N\\to\\infty$ and $D\\to\\infty$ should be taken, how large is $N$ with respect to $D$ for the estimators to converge. We suppose there is a scaling relation $N\\sim D^\\nu$ with $\\nu$ an exponent to be determined. This means that if one multiplies $D$ by $\\alpha$, one will have to take $N\\alpha^\\nu$ samples for the estimators to be as good.\n",
    "\n",
    "Draw the previous curves (you may focus on the MSEs) for two different $D$ (take $D=100$ and $D=\\alpha\\times100$ with $\\alpha=2$) and search how to scale the $N$ with respect to $\\alpha$ so the curves collapse (i.e. draw MSE vs $N/\\alpha^\\nu$). What is the scaling $\\nu$ between $N$ and $D$ ?\n",
    "\n",
    "This is the thermodynamic limit of this model !"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe5be13f",
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha = 2\n",
    "\n",
    "Nmax = 10000\n",
    "Ns = np.arange(250, Nmax, 50)\n",
    "\n",
    "...\n",
    "\n",
    "plt.plot(Ns, ...\n",
    "plt.plot(...Ns..., ..."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "unianalytics_cell_mapping": [
   [
    "415b9df6",
    "415b9df6"
   ],
   [
    "25adfac2",
    "25adfac2"
   ],
   [
    "7bf32d72",
    "7bf32d72"
   ],
   [
    "f9cf7604",
    "f9cf7604"
   ],
   [
    "dcf98047",
    "dcf98047"
   ],
   [
    "30be7783",
    "30be7783"
   ],
   [
    "ffbb8150",
    "ffbb8150"
   ],
   [
    "22ed2326",
    "22ed2326"
   ],
   [
    "d737f0ea",
    "d737f0ea"
   ],
   [
    "d62b1edb",
    "d62b1edb"
   ],
   [
    "332586d7",
    "332586d7"
   ],
   [
    "77493bda",
    "77493bda"
   ],
   [
    "d9ca0a0d",
    "d9ca0a0d"
   ],
   [
    "00e0d9ae",
    "00e0d9ae"
   ],
   [
    "d05c0077",
    "d05c0077"
   ],
   [
    "a678c3c2",
    "a678c3c2"
   ],
   [
    "eb26f423",
    "eb26f423"
   ],
   [
    "2b33a39a",
    "2b33a39a"
   ],
   [
    "4cf3a55c",
    "4cf3a55c"
   ],
   [
    "94ba3b47",
    "94ba3b47"
   ],
   [
    "32a93894",
    "32a93894"
   ],
   [
    "e7b45e5a",
    "e7b45e5a"
   ],
   [
    "150e73ec",
    "150e73ec"
   ],
   [
    "8e80ca75",
    "8e80ca75"
   ],
   [
    "954940ac",
    "954940ac"
   ],
   [
    "0914e5fd",
    "0914e5fd"
   ],
   [
    "6e1450f0",
    "6e1450f0"
   ],
   [
    "6d8c33d1",
    "6d8c33d1"
   ],
   [
    "7088a849",
    "7088a849"
   ],
   [
    "7a984aa0",
    "7a984aa0"
   ],
   [
    "dfb1ad66",
    "dfb1ad66"
   ],
   [
    "4084f9f6",
    "4084f9f6"
   ],
   [
    "d4a16bec",
    "d4a16bec"
   ],
   [
    "fa560980",
    "fa560980"
   ],
   [
    "1f75fb21",
    "1f75fb21"
   ],
   [
    "4f5c9209",
    "4f5c9209"
   ],
   [
    "26fecc4d",
    "26fecc4d"
   ],
   [
    "a0544658",
    "a0544658"
   ],
   [
    "f8ec1cad",
    "f8ec1cad"
   ],
   [
    "c4132f8f",
    "c4132f8f"
   ],
   [
    "b4ed5828",
    "b4ed5828"
   ],
   [
    "4728b523",
    "4728b523"
   ],
   [
    "3a879c9f",
    "3a879c9f"
   ],
   [
    "0166ccd8",
    "0166ccd8"
   ],
   [
    "4bfb249c",
    "4bfb249c"
   ],
   [
    "fe5be13f",
    "fe5be13f"
   ]
  ],
  "unianalytics_notebook_id": "ffc8fa23-8630-4a6e-a5fe-e6784c5aba86"
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
