{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "64cd7bfa-bbc7-4fd3-831d-5d4cc56f2599",
   "metadata": {},
   "source": [
    "# Exercise 3: working with image datasets, singular value decomposition and principal components"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "4377c25d-1703-4bea-85c5-070426f1d00d",
   "metadata": {},
   "source": [
    "In this exercise session we will:\n",
    "- learn how to store images in arrays, and review how to do the related manipulation (reshape from vector to matrix, from matrix to vector, etc)\n",
    "- learn how to perform the SVD of a matrix\n",
    "- play with a dataset of images called [Labeled Faces in the Wild](http://vis-www.cs.umass.edu/lfw/). In particular we will compute the principal components, and see what kind of information they can give to us when analysing data.\n",
    "\n",
    "As usual, we start importing python libraries to deal with numerical arrays (numpy) and to be able to plot (matplotlib)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b36e63ea-35f8-43db-8db6-ac818dfa9f4b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "759f3e68-8b88-48a7-b6a3-bea1ad012247",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Exercise 1: Images as vectors and matrices"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "30d73d01",
   "metadata": {},
   "source": [
    "Many datasets are dataset of images. \n",
    "How do we represent images in a computer? \n",
    "An image (say a black and white image) can be tought of as a matrix of pixels, the value of each pixel representing the light intensity at that pixel.\n",
    "Thus, an image can be tought of as an element of $\\mathbb{R}^{\\text{height} \\times \\text{width}}$, or, in python terms, as a 2-dimensional array of shape ```[height, width]```.\n",
    "\n",
    "\n",
    "In applications, it is often convenient to store images as vectors instead of matrices.\n",
    "The number of pixels does not change, but the array that we use to store the values of the pixels changes to a 1-dimensional array of shape ```[height * width]```.\n",
    "Of course, there are many ways to convert a matrix into a vector. Conventionally, for images, we build the vector by concatenating successively its rows.\n",
    "\n",
    "Why is it convenient to use 1d arrays? Because then we can store compactly our dataset (collection of images) as a 2d array, each row of the array collecting all the pixels of a different datapoint/image.\n",
    "For example, a dataset of ```n``` images with given ```height``` and ```width``` will be represented by a 2d array of pixels of shape ```[n, height * width]```.\n",
    "\n",
    "In this exercise, we warm up by reviewing how to convert from the 2d representation of images to a 1d representation, and viceversa."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "742aa51d-de3e-4745-9de3-8ca0888519ef",
   "metadata": {},
   "source": [
    "We start by initialising a numpy vector representing an image. For the moment, this is just a 1d array of pixel intensities that I am giving to you. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5f53cd2-986f-45bd-833d-1aa109b0f29d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "image = np.array([0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0.07,0.87,1.00,0.04,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0.55,0.99,0.89,0.03,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0.53,1.00,0.99,0.25,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0.02,0.54,0.99,1.00,0.99,0.17,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0.01,0.54,0.99,0.99,1.00,0.99,0.17,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0.12,0.56,0.99,0.99,0.99,1.00,0.90,0.11,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0.31,0.85,0.99,0.97,0.50,0.88,1.00,0.73,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0.40,0.86,0.99,0.97,0.15,0,0.87,1.00,0.66,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0.08,0.31,0.96,0.99,0.74,0.25,0,0.23,0.96,1.00,0.30,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0.06,0.77,0.99,0.99,0.62,0.08,0,0,0.50,0.99,1.00,0.30,0,0,0,0,0,0.27,0.44,0.06,0,0,0,0,0,0,0,0,0.76,1.00,1.00,0.96,0.28,0.13,0.13,0.05,0.74,1.00,0.89,0.13,0.13,0.13,0.23,0.56,0.77,1.00,0.96,0.30,0,0,0,0,0,0,0,0,0.42,0.80,0.99,0.99,0.99,0.99,0.99,0.81,0.92,0.99,1.00,0.99,0.99,0.99,0.99,0.99,0.99,0.84,0.32,0,0,0,0,0,0,0,0,0,0,0.05,0.34,0.60,0.86,0.99,0.99,0.99,0.99,0.99,1.00,0.99,0.99,0.96,0.60,0.60,0.21,0.07,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0.03,0.04,0.44,0.56,0.99,0.99,0.87,0.04,0.04,0.04,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0.06,0.83,0.99,0.87,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0.74,0.99,0.53,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0.09,0.86,0.99,0.23,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0.18,0.99,0.99,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0.18,0.99,0.99,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0.02,0.58,0.80,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "ea616771-324d-4b32-bc6e-5be6955f7a8e",
   "metadata": {},
   "source": [
    "This seems like a 1d array, but with all these numbers everything seems messy. Can you check (using ```numpy```'s utilities) that this is indeed a vector (and not a matrix or tensor), and find its dimension?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "930aff75-2d12-462b-9678-782f14d65ea2",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"The number of indices (or dimensions) of the array is { ... }.\")\n",
    "print(f\"The dimension is { ... }.\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "823fbfb4-0eeb-4abd-a76f-e039b41e864b",
   "metadata": {},
   "source": [
    "Now, let's reshape this 1d array into a 2d array (a matrix). To do that, I need to tell you the original \n",
    "shape of the image ```[height, width]```, as at the moment you only know the total amount of pixels, i.e. ```height * width```, but not how they are distributed.\n",
    "\n",
    "\n",
    "Knowing that the image is square (same width and height), compute its 2d shape (width and height), and reshape it (remember that shapes are integers, and to convert a floating point number to an integer you can use the python ```int()``` function)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69fd8e7e-67e4-4754-8cc4-375cefa23691",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "width = ...\n",
    "new_shape = ...\n",
    "image_2d = ...\n",
    "print(f\"The wanted shape is {new_shape}.\")\n",
    "print(f\"The new shape of our data is {np.shape(image_2d)}.\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "7ffed159-2e11-470b-8508-a43d14fb759b",
   "metadata": {},
   "source": [
    "To inspect matrices, it is useful to plot them. We learned how to use the [imshow](https://matplotlib.org/stable/api/_as_gen/matplotlib.pyplot.imshow.html) function (coming from the matplotlib library) last time. Try to visualise this image using it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "948b88a2-84cd-48bb-88f9-ffe9cea34071",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "..."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "43451166",
   "metadata": {},
   "source": [
    "This image represent an handwritten digit 4. \n",
    "It comes from a classic dataset, called MNIST, which is used to train neural networks to perform automatic digit recognition."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "124ff65c",
   "metadata": {},
   "source": [
    "Now, try to plot just the bottom-left quarter of the image by extracting the correct indices in the matrix (remember that ```//``` is the integer division)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65ae386b",
   "metadata": {},
   "outputs": [],
   "source": [
    "..."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "ea693d92",
   "metadata": {},
   "source": [
    "To get back the 1d array version of the image, we can reshape it. Try to do that using the ```flatten``` function, and check that the resulting dimension is what you expect."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02a35a39-30bb-4168-a439-44c0f1368a74",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_back_to_1d = ...\n",
    "# perform a check"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "bc33c953",
   "metadata": {},
   "source": [
    "In this warm-up exercise, we reviewed how to deal with vectors and matrices, and how to convert between the two representations.\n",
    "The take home messages are:\n",
    "- images can be represented both as vectors and matrices of pixels. The vector form is more compact, and nicer when dealing with an entire dataset of images. The matrix form is easier to visualise, and retains more information about which pixels are neighbouring. The two representation are equivalent.\n",
    "- use ```size, shape, flatten, reshape``` to inspect or alter the shape and size of a numpy vector. Notice in particular how often we used the ```shape``` function to check that the operations we did transformed the array the way we wanted/expected."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "0d55b4f3",
   "metadata": {},
   "source": [
    "## Exercise 2: SVD using numpy's built-in functionality"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "100dbda5",
   "metadata": {},
   "source": [
    "In this exercise, we learn how to compute the singular value decomposition (SVD) of a matrix.\n",
    "\n",
    "Consider a rectangular matrix $X \\in \\mathbb{R}^{n \\times d}$, where we think that each of the $n$ rows is a different datapoint of a dataset, and each datapoint lives in $d$ dimensions.\n",
    "Recall from lecture 2 that each matrix $X$ admits a SVD, i.e. it can be written as $X = U \\Sigma V^T$ where \n",
    "- $U \\in \\mathbb{R}^{n \\times n}$ is a unitary matrix (rows and columns are orthonormal systems). The columns are called _left singular vectors_.\n",
    "- $V \\in \\mathbb{R}^{d \\times d}$ is a unitary matrix (rows and columns are orthonormal systems). The columns are called _right singular vectors_.\n",
    "- $\\Sigma \\in \\mathbb{R}^{n \\times d}$ is a matrix of zeros, with only non-zero components on the diagonal. The diagonal entries are called _singular values_, and are all non-negative.\n",
    "\n",
    "Let's get comfortable with all this by considering a simple example matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "788b5813",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.array([[1, 4, 3, 2], [8, 7, 6, 5] ,[11, 12, 10, 9]])\n",
    "print(X)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "38132672",
   "metadata": {},
   "source": [
    "Find the dimensions of $X$, and call them ```n``` and ```d```."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98a9fb29",
   "metadata": {},
   "outputs": [],
   "source": [
    "n = ...\n",
    "d = ...\n",
    "print(f\"The dimensions of the matrix are (n,d) = ({n},{d})\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "91ae3edd",
   "metadata": {},
   "source": [
    "```numpy``` provides a [function](https://numpy.org/doc/stable/reference/generated/numpy.linalg.svd.html), ```np.linalg.svd```, to compute the SVD of a matrix.\n",
    "It returns a list of three objects: \n",
    "- the matrix $U \\in \\mathbb{R}^{n \\times n}$ of left singular vectors;\n",
    "- a vector of length $\\min(n, d)$ containing the singular values (the diagonal of the matrix $\\Sigma$);\n",
    "- the matrix $V^T \\in \\mathbb{R}^{d \\times d}$ of right singular vectors.\n",
    "\n",
    "Let's try it on $X$. Compute ```U, S, Vt``` and check what shape they have. Is it what you expected?\n",
    "\n",
    "Notice that if a function ```f()``` returns a list of values, we can either store its result in a list ```mylist = f()```, or we can _unpack_ directly the list into the appropriate number of variables ```a, b, c = f()``` (in case the return list has length three). You can discover how many results a function returns by inspecting the documentation. In this case, ```np.linalg.svd``` returns a list of three values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34ab631f",
   "metadata": {},
   "outputs": [],
   "source": [
    "U, S, Vt = ...\n",
    "print(f\"The shapes are (U) {...}, (S) {...} and (Vt) {...}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "207fc096",
   "metadata": {},
   "source": [
    "Verify that the singular values are decreasing (this is guaranteed when using ```np.linalg.svd``` according to the [documentation](https://numpy.org/doc/stable/reference/generated/numpy.linalg.svd.html), but we are checking it anyways). To check this, try to print and to plot the singular values. To plot them, use the ```plt.plot``` function, as we did in the last exercise session to plot the distances vs timesteps for the page-rank algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f032cd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "..."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "1b4e0737",
   "metadata": {},
   "source": [
    "One of the rules of the good physicist is to **always** label and title its plots. So let's do that. \n",
    "Use the functions ```plt.xlabel()```, ```plt.ylabel()``` and ```plt.title()``` to set the labels of each axis and the overall title of the plot.\n",
    "If you do not know how these work, Google is your friend (and then, only then, also TAs are your friends)!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66641103",
   "metadata": {},
   "outputs": [],
   "source": [
    "..."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "fbf8798d",
   "metadata": {},
   "source": [
    "It is useful to know how to convert the vector of singular values into a matrix, as often we want to multiply it with other matrices. \n",
    "We can just build a matrix of zeros (we saw how to do that in the first exercise session), and populate its diagonal with the entries of S using the ```np.fill_diagonal()``` function (look at the [documentation](https://numpy.org/doc/stable/reference/generated/numpy.fill_diagonal.html)!). Then, perform a sanity chack that the dimension are good."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dda6da58",
   "metadata": {},
   "outputs": [],
   "source": [
    "Sigma = ...\n",
    "...\n",
    "print(f\"The shapes of Sigma is {...}, and it should be ({...},{...})\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "f07a3973",
   "metadata": {},
   "source": [
    "Finally, we can check that the SVD reproduces the original matrix correctly, for example checking that their distance in norm is small."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8042d4fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "reconstructed_X = ...\n",
    "print(f\"||reconstructed_X - X|| = {...}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "6ea83e55",
   "metadata": {},
   "source": [
    "We can also check that the matrices $U$ and $V$ are unitary. To transpose a matrix, use ```np.transpose()```, and to create the $p \\times p$ identity matrix use ```np.identity(p)```."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffcf0e45",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"||U Ut - I_n|| = {...}\")\n",
    "print(f\"||Ut U - I_n|| = {...}\")\n",
    "print(f\"||V Vt - I_d|| = {...}\")\n",
    "print(f\"||Vt V - I_d|| = {...}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "4c78d742",
   "metadata": {},
   "source": [
    "In this exercise, we learned how to compute the SVD of a matrix, and how to check that what we obtained makes sense.\n",
    "The take home messages are:\n",
    "- the SVD can be used to diagonalise a rectangular matrix in the same way an eigendecomposition can be used to diagonalise a symmetric matrix.\n",
    "- ```numpy``` has _a lot of built-in functions_. If you need to perform any operation on numpy arrays that is trivial (e.g. filling a diagonal, transposing) or routine (e.g. computing the SVD), chance is that it is already implemented (in our case, through the functions ```np.fill_diagonal```, ```np.transpose``` and ```np.linalg.svd```). Do not be scared by this: google is your friend! Just search for the operation you need to do, append \"python numpy\" to the search, and you will discover ```numpy```'s functionality little by little. As an example, I learned about the ```np.fill_diagonal``` function right now, while writing this exercise, through a google search :)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "531ca1ba",
   "metadata": {},
   "source": [
    "## Exercise 3: \"Eigenfaces\", or the principal components of a dataset of images"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "0bd29426",
   "metadata": {},
   "source": [
    "Finally, we are equipped to see something cool! In this exercise, we look at the [Labeled Faces in the Wild](http://vis-www.cs.umass.edu/lfw/) dataset, which is a dataset of images of faces of famous people.\n",
    "We will download the dataset, and then compute its principal components, i.e. the right singular values. This will allow us to explore a bit this very high-dimensional dataset.\n",
    "\n",
    "We start by loading the dataset. We use a numpy function called ```np.load``` to load the file ```faces.npy```, where the dataset was pre-saved. This file was generated by saving a matrix with the function ```np.save```. This pair of functions is useful if you need to store and retrieve numpy data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "1f29332b",
   "metadata": {},
   "outputs": [],
   "source": [
    "faces = np.load(\"faces.npy\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "5d0f988d",
   "metadata": {},
   "source": [
    "Let's give a first look at the dataset. What shape is ```faces```? Knowing that the images are square (and thus some number must be a perfect square...), how many images are there?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "dfa35619",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The shape is Ellipsis.\n",
      "There are Ellipsis images, because ...\n"
     ]
    }
   ],
   "source": [
    "print(f\"The shape is {...}.\")\n",
    "print(f\"There are {...} images, because ...\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "aaf085c4",
   "metadata": {},
   "source": [
    "Define the following variables: ```n``` the number of images, ```d``` their dimension, and ```w``` the size of the side of the image in pixels (use int to convert them to integer, if they are not already)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "317e222d",
   "metadata": {},
   "outputs": [],
   "source": [
    "n = ...\n",
    "d = ...\n",
    "w = ..."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "c33763f5",
   "metadata": {},
   "source": [
    "Plot the first 10 images, after having reshaped them appropriately (see Exercise 1). Call ```plt.show()``` in between every ```imshow``` call. Otherwise you will only see the last image, as ```imshow``` overwrites its internal content (what he will plot) at each call."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2e38e38",
   "metadata": {},
   "outputs": [],
   "source": [
    "..."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "cdd7d6db",
   "metadata": {},
   "source": [
    "Before computing the principal components / SVD, we do some preprocessing on our data. We want to center our dataset, otherwise the first principal component will pick up the average of the data.\n",
    "Start by computing the average of all datapoints (use ```np.mean```) and plot it (do not forget reshaping!).\n",
    "Hint: look at the documentation, if you just call ```np.mean(faces)``` you will obtain the average of all pixels of all images!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f60abe1",
   "metadata": {},
   "outputs": [],
   "source": [
    "avg_face = np.mean(...)\n",
    "plt.imshow(...)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "740b7993",
   "metadata": {},
   "source": [
    "Already the average image tells us a lot! On average, the images have a face in the center, roughly half the size of the full image, and looking straight at us. \n",
    "Also, it seems that on average the people in the images like to wear white shirts!\n",
    "\n",
    "To center the dataset, we subtract the average face from all the datapoints. Try to do that,\n",
    "check that you succeded, and plot again the first 10 centered images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb1469dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "centered_faces = ...\n",
    "print(f\"Check that the new mean is 0 for every pixel. Mean = {...}\")\n",
    "\n",
    "# # plot\n",
    "..."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "b293ef98",
   "metadata": {},
   "source": [
    "Then, we normalise the images in order for the pixels to have the same standard deviation. \n",
    "You may have not heard of standard deviation before: it is a measure of how much some dataset varies around its mean. A high standard deviation means that the average is a poor estimate of the behaviour of the dataset, as there may be large deviations around it in each datapoint. A small standard deviation means that the average is a good estimate of the behaviour of every single datapoint, as each of them does not differ much from the average.\n",
    "\n",
    "The normalisation using the standard deviation avoids that some pixel that varies wildly dominates the SVD, while other pixel that vary less become negligible.\n",
    "Compute the standard deviation of the centered faces using ```np.std()``` (very similar to the ```np.mean``` function), and nomalise them by the standard deviation.\n",
    "Perform a check, and plot again the first 10 images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "373eba1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "std_face = ...\n",
    "normalised_faces = ...\n",
    "print(f\"Check that the new standard deviation is 1 for every pixel. Var = {...}\")\n",
    "\n",
    "# # plot\n",
    "..."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "b1714287",
   "metadata": {},
   "source": [
    "Now, with nicely normalised data, we can compute the SVD (see Exercise 2). It may take a couple of minutes as the dataset is \"big\" (it is very small compared to standard datasets that you will encounter in the future, but big with respect to our example in Exercise 2)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8397b534",
   "metadata": {},
   "outputs": [],
   "source": [
    "U, S, Vt = ..."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "429927b8",
   "metadata": {},
   "source": [
    "Plot the singular values (with labels and a title), to check that they are sorted in decreasing order."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9ec99e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "..."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "59336ae7",
   "metadata": {},
   "source": [
    "Many singular values are very close to zero. It is useful to plot them in y-log scale. \n",
    "To do so, use the command ```plt.yscale('log') ``` just after the plotting command."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e16a4cd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "..."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "33937c50",
   "metadata": {},
   "source": [
    "Thus, the first principal components (columns of V) will correspond to larges singular values, i.e. to more important features of our dataset, while the last principal components will correpsond to minor features of the dataset.\n",
    "\n",
    "Let's start by plotting the first 10 principal components (first ten columns of V, notice that Vt is the transpose of V!) as images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32611ebe",
   "metadata": {},
   "outputs": [],
   "source": [
    "..."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "f127f314",
   "metadata": {},
   "source": [
    "In these images, bright yellow regions are regions with positive entries, and blue regions have negative entries. Thus, for example, the first principal component tells us that there the most of the variation is in the region of the face (with respect to the average face), followed by variation in the shoulders region (second principal component). "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "c39eabe2",
   "metadata": {},
   "source": [
    "Now plot the the principal components corresponding to the 1000th, 1001st, 1002nd and 1003rd singular values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a369332",
   "metadata": {},
   "outputs": [],
   "source": [
    "..."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "165ea0f7",
   "metadata": {},
   "source": [
    "We see that the principal components corresponding to smaller singular values are very noisy, highlighting very high-frequency variation of the dataset. These principal components give us much less information about the overall variability of the dataset than the dominant principal components. \n",
    "This confirms our intuition that principal components with large singular values are generic features of the dataset, and principal components with small singualr values are very specific features, not highlighting any general feature of the dataset."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "936aa694",
   "metadata": {},
   "source": [
    "We can also project an original normalised sample along the first principal components. We can write a function to do that.\n",
    "Recall the the orthogonal projection of a vector $\\vec{a}$ over a set of orthonormal vectors $\\{\\vec{v}_m\\}_{m=1}^k$ (call the projection $\\vec{b}$) is given by \n",
    "$$\n",
    "    \\vec{b} = \\sum_{m = 1}^k (\\vec{v}_k \\cdot \\vec{a}) \\vec{v}_k\n",
    "$$\n",
    "where $\\cdot$ is the scalar product ```np.dot```. \n",
    "\n",
    "Write a function that takes as input a vector and a matrix whose columns are the orthonormal vectors we want to project our vector over, are returns the projected vector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa9e2488",
   "metadata": {},
   "outputs": [],
   "source": [
    "# image = 1d array of length d\n",
    "# pcs = 2d array of shape (d, k)\n",
    "def projection(image, pcs):\n",
    "    ..."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "3b5035bc",
   "metadata": {},
   "source": [
    "Now show the projection over the first ```[5,10,30,100,500,1000,1348]``` of the first image of the normalised dataset (recall that as the singular values are ordered decreasingly, the first principal components should be important features of the dataset, and the last principal components should be less important features)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22262868",
   "metadata": {},
   "outputs": [],
   "source": [
    "..."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "5eabc50d",
   "metadata": {},
   "source": [
    "We see that as the number of principal component increases, more and more details are recovered in the original image.\n",
    "In some sense, the first principal components (corresponding to larger singular values) pick up the general features of the dataset, while the last principal components (corresponding to smaller singular values) pick up image specific fine details.\n",
    "\n",
    "After this example, you may also be convinced that SVD can be also seen as a compression method. Instead of storing the entire dataset (```n * d``` floating point numbers), you can just store a limited number of principal components, say ```k```, and then for each image store the ```k``` projection coefficients along the selected principal components. This amounts to ```k * d + k * n``` floating point numbers, which may be much smaller than ```n * d``` if $n, d \\gg 1$ (large amount of high-dimensional data) and $k$ is not very large (first principal components allowing for faithful enough reconstruction of the dataset)."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "9b2ce7bf",
   "metadata": {},
   "source": [
    "This brings us to the conclusion of this exercise. We learned how to normalise a dataset, trained a bit of our visulisation skills, applied the SVD machinery to a true dataset, and learned what principal components can teach us of a dataset.\n",
    "The take home messages are:\n",
    "- always consider whether you should normalise your dataset. This avoids that a non-zero average, or that dimensions with different variation scales, bias your algorithms.\n",
    "- SVD on large datasets becomes more computationally expensive. \n",
    "- Principal components give us a set of features, ranging from very generic feature that are common to all datapoints / images, to very specific features (in our case, pixel-wise features).\n",
    "- When writing code, try to identify reusable bits (in our case, the projection routine) and pack them into functions to improve readability of your code.\n",
    "\n",
    "A final consideration: you may wonder if we need to explicitly use the SVD decomposition numpy function to perform principal component analysis, or if there is a more convenient way to to the analysis, and access the relevant informations. \n",
    "The answer is: this is Python, of course there is a more convenient way of doing principal component analysis! \n",
    "Check the [documentation](https://scikit-learn.org/stable/modules/generated/sklearn.decomposition.PCA.html)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
