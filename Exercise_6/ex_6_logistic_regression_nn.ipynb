{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "abd71f5b-7ad8-4570-ba67-f044acd061f0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93e37f8b-bc27-4430-be53-fd137b453c02",
   "metadata": {},
   "source": [
    "# Logistic Regression and Neural networks\n",
    "\n",
    "Today's task is to do even/odd binary classification on the MNIST dataset. Each data sample is an image of an handwritten digit from 0 to 9, and is labelled with the digit it represents. We will first perform logistic regression, and then train a neural network to perform this task.\n",
    "1. In Exercise 1, we will load and preprocess our dataset and its labels.\n",
    "2. In Exercise 2, we will perform classification by doing logistic regression.\n",
    "3. In Exercise 3, we will train a neural network to classify the dataset. We will then compare the performance of logistic regression and neural networks."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5805235b-2417-4631-be05-d93c39666717",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Ex 1. Dataset loading and preprocessing\n",
    "We first preprocess the mnist dataset in order to use apply logistic regression first and neural networks later.\n",
    "Our task is to do even/odd binary classification on the MNIST dataset. \n",
    "The dataset is composed of 10000 images, each of which is 28x28 pixels. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9d9b2af1-acd5-4fb7-bc68-3551e879c2e4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#load the mnist dataset\n",
    "mnist= np.load(\"mnist.npy\")\n",
    "mnist_labels = np.load(\"mnist_labels.npy\")\n",
    "\n",
    "X=mnist\n",
    "y=(mnist_labels%2)*2.0-1 # 1 for odd, -1 for even."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c049e435-797e-4d06-aa2b-2f8cf828a93a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "label=1.0\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#printing the first digit\n",
    "idx=0\n",
    "plt.imshow(X[idx])\n",
    "print(f\"label={y[idx]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9135a966-8ff5-407f-8e1f-db85bcee1101",
   "metadata": {},
   "source": [
    "#### Ex 1.1\n",
    "1. Reshape X so that it is a 10000 by 768 matrix.\n",
    "2. Do a 60:20:20  train, validation, test split. The train, validation and test set shall be named `X_train`,`y_train`, `X_val`, `y_val`, `X_test`,  `y_test`\n",
    "\n",
    "Before splitting, the lines of the data matrix should be randomly permuted to guarantee that samples in each set are independent.\n",
    "\n",
    "Hint: for the splitting, check for a dedicated function in skelarn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fcbe474d-510d-4a67-8869-7a621217d277",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "### your code goes here ###"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a46a4f7-4f1b-4354-b60f-f7da73447e31",
   "metadata": {},
   "source": [
    "#### Ex 1.2 Normalize the dataset. \n",
    "1. Compute the mean and standard deviation over the whole *training* dataset (i.e. the mean and std should be two scalars computed on all the elements of `X_train`). \n",
    "2. Then normalize the `X_train`,`X_val`,`X_test` using the mean and std computed above\n",
    "\n",
    "Why using `X_train` to compute the normalization and then applying the same normalization also to `X_val`,  `X_test`?\n",
    "1. If one computed the normalization on the whole `X`, this would have introduced correlations between the datasets\n",
    "2. In some cases `X_test,y_test` represent new data which are not available at the moment of training. Then it's important to have a unique way of normalizing, using only the training set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5603cccb-8fa0-4ba4-a358-7ab85f8c0fc0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "### your code goes here ###"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c3dfeec-19e5-443e-8391-201a9777a800",
   "metadata": {},
   "source": [
    "## Ex 2: Logistic regression\n",
    "In this exercise you will implement a classifier that uses logistic regression to distinguish even from odd digits in the MNIST dataset. The classifier will be trained using gradient descent.\n",
    "Let's set some notation: we call $n$ the number of datapoints in the training set and $d$ the dimensionality of each datapoint.\n",
    "\n",
    "\n",
    "\n",
    "In the logistic regression case, recall that our predictor is $z= \\vec x \\cdot \\vec w$, where both $\\vec x$ and $\\vec w$ live in $\\mathbb R^d$.\n",
    "\n",
    "The regularized training loss is\n",
    "$$\\mathcal L(\\vec w)=\\frac{1}{n}\\sum_{\\mu=1}^n\\ell(y_\\mu,\\vec X_\\mu\\cdot \\vec w)+\\frac{\\lambda}{n}\\sum_{i=1}^d w_i^2,\\quad \\ell(y,z)=\\log(1+e^{-yz})$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60a71ba7-c0a4-43eb-bf59-9ba724d6d076",
   "metadata": {},
   "source": [
    "The gradient of the loss with respect to $\\vec w $ is \n",
    "$$\\frac{\\partial \\mathcal L}{\\partial \\vec w_i}(\\vec w)=-\\frac{1}{n}\\sum_{\\mu=1}^n \\frac{e^{-y_\\mu \\vec X_\\mu\\cdot \\vec w}}{1+e^{-y_\\mu \\vec X_\\mu\\cdot \\vec w}} X_{\\mu i} y_\\mu+\\frac{2\\lambda}{n}\\vec w_i$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ef30c5a-d137-42c6-9d7d-51328a6b29aa",
   "metadata": {},
   "source": [
    "#### Ex 2.1:  Implementing the gradient\n",
    "Using the above formula, implement a function `loss_gradient` that takes as arguments `X_train,y_train,w,lambd` and returns a numpy array containing the gradient of the loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b48be59-6575-4562-9224-44bec28099e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "### your code goes here ###"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da193128-6394-417e-99f2-10692566fdef",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Ex 2.2: training\n",
    "Write a training loop that perfoms gradient descent.\n",
    "1. Set $\\lambda=1$ and choose a good step size that allows the algorithm to learn. 2000 steps should be enough to learn. For the initialization you can use a random one.\n",
    "2. Every 100 gradient steps, a measurement of training and validation accuracies (i.e. 1- error), should be taken and stored in lists. \n",
    "3. After the training make a plot of train and test accuracies vs time\n",
    "\n",
    "*Recall that the accuracy of a model is the fraction of times the model correctly predicts the class.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "40657f8c-8c90-49dc-b425-e7b1c4043a70",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "tmax=2000 #number of iterations\n",
    "lambd=1 #regularization\n",
    "\n",
    "### your code goes here ###\n",
    "\n",
    "for t in range(tmax):# training loop\n",
    "    ### your code goes here ###\n",
    "    pass "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42fa8c17-8d0e-461a-bed5-98df5447fb53",
   "metadata": {},
   "source": [
    "#### Ex 2.3: Early stopping\n",
    "Modify the training loop to implement early stopping (you can copy-paste the training loop you wrote and modify it). \n",
    "You will do so by still runnnig gradient descent for a fixed number of steps `tmax`.\n",
    "Whenever you take a measurement of the validation accuracy, check if this is the highest accuracy reached so far in the training. If it is, save in memory the current weight vector and time. This way at the end of training you will have your weight vector with highest validation accuracy.\n",
    "\n",
    "After training the model again make a showing the train and validation accuracies vs time. Mark in some way the time at which the highest validation accuracy was achieved. Finally compute the test error of the weight vector with highest validation error\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b93c6b9d-d7a6-4a8c-bfb2-b27ad7fa9b28",
   "metadata": {},
   "source": [
    "###  Ex 2.4: Logistic regression with sklearn\n",
    "the scikit-learn library contains a logistic regression implementation, allowing to train logistic regression in a couple of lines of code. \n",
    "1. Implement logistic regression without regularization\n",
    "2. Compute the test error\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "50f06971-6ecf-4112-8dcd-95ed628d9b13",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "### your code goes here ###"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c80b9e1-ad51-4a49-a9f3-9ddb2cc4a24e",
   "metadata": {},
   "source": [
    "## Ex 3: Neural networks\n",
    "\n",
    "#### Ex 3.1 To obtain some familiarity with neural networks, go to https://playground.tensorflow.org/.\n",
    "On the left you can choose a dataset on which to do classification. You can then modify the network by choosing the number of hidden layers, number of neurons in each layer, activation function. \n",
    "\n",
    "The hardest dataset to learn is the spiral: play with the parameters of the network until it manages to learn this dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e06fdc4-037c-422b-b93b-3d3c8f9f9c1d",
   "metadata": {},
   "source": [
    "### Neural network on MNIST\n",
    "Next you will implement a one hidden layer network to be trained using GD on MNIST. The neural network function will be the following:\n",
    "$f(\\vec x ;W^1,\\vec W^2)=\\sum_{a=1}^KW^2_a\\sigma\\left(\\vec W^1_a \\cdot \\vec x\\right)$,\n",
    "1. $K$ is the dimension (or width) of the hidden layer\n",
    "2. $W^1\\in\\mathbb R^{K\\times d}$ is the matrix of first layer weights. $W^1_a$ indicates the $a-th$ row of the matrix $W^1$.\n",
    "3. $\\vec W^2\\in\\mathbb R^K$ is the vector containing the second layer weights. \n",
    "4. $\\sigma$ is the activation function, or nonlinearity. It acts coordinate-wise on the array it is fed. We set it initially to the hyperbolic tangent function.\n",
    "\n",
    "### Gradient Computation\n",
    "We will do gradient descent on the logistic loss \n",
    "$$\\mathcal L(W^1, \\vec W^2)=\\frac{1}{n}\\sum_{\\mu=1}^n\\ell(y_\\mu,f(\\vec x ;W^1,\\vec W^2)) ,\\quad \\ell(y,z)=\\log(1+e^{-yz}).$$\n",
    "In the calculation we will use the notation $z_\\mu=f(\\vec X_\\mu ;W^1,\\vec W^2)$\n",
    "\n",
    "\n",
    "We have \n",
    "$$\\frac{\\partial \\mathcal L}{\\partial W^2_a}=\\frac{1}{n}\\sum_{\\mu=1}^n\\frac{\\partial \\ell}{\\partial z}(y_\\mu,z_\\mu)\\,\\,\\sigma\\left(\\vec W^1_a\\cdot X_\\mu\\right)$$\n",
    "$$\\frac{\\partial \\mathcal L}{\\partial W^1_{ai}}=\\frac{1}{n}\\sum_{\\mu=1}^n\\frac{\\partial \\ell}{\\partial z}(y_\\mu,z_\\mu)W^2_a\\,\\,\\sigma'\\left(\\vec W^1_a\\cdot X_\\mu\\right)X_{\\mu i},$$\n",
    "where the index $a$ runs from 1 to $K$ and $i$ runs from 1 to $d$.\n",
    "\n",
    "The following cell contains all the necessary functions, together with their documentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d27bcdad-4e1c-41a7-8835-a30105d9a16b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def sigma(x):\n",
    "    \"\"\"\n",
    "    activation function\n",
    "    \"\"\"\n",
    "    return np.tanh(x) #tanh activation\n",
    "\n",
    "def deriv_sigma(x):\n",
    "    \"\"\"\n",
    "    derivative of the activation function\n",
    "    \"\"\"\n",
    "    return 1-np.tanh(x)**2 #tanh derivative\n",
    "\n",
    "def neural_net(X,W_1,W_2): \n",
    "    \"\"\"\n",
    "    function implementing the neural network. \n",
    "    W_1 is a K by d matrix containing the first layer weights.\n",
    "    W_2 is a vector of length K, containing the second layer weights\n",
    "    X is a n by d matrix containing the inputs.\n",
    "    Notice this function will not accept one dimensional arrays as input (X), only two dimensional ones (matrices)\n",
    "    If X has shape (n,d) then the output will be a numpy array of shape (n,)\n",
    "    \"\"\"\n",
    "    return sigma(X@W_1.T)@W_2\n",
    "\n",
    "def deriv_ell(y,z):\n",
    "    \"\"\"\n",
    "    derivative of the logistic loss log(1+np.exp(-y*z)) w.r.t. z\n",
    "    \"\"\"\n",
    "    return -y*np.exp(-y*z)/(1+np.exp(-y*z)) \n",
    "\n",
    "def grad_W_2(X,W_1,y):\n",
    "    \"\"\"\n",
    "    gradient of the loss with respect to the second layer weights\n",
    "    returns vecror of length K (same shape as W_2)\n",
    "    \"\"\"\n",
    "    post_activations=sigma(X@W_1.T)\n",
    "    z=post_activations@W_2\n",
    "    return deriv_ell(y,z)@post_activations/y.shape[0]\n",
    "\n",
    "def grad_W_1(X,W_1,W_2,y):\n",
    "    \"\"\"\n",
    "    gradient of the loss with respect to the first layer weights\n",
    "    returns a K by d matrix (same shape as W_1)\n",
    "    \"\"\"\n",
    "    pre_activations=X@W_1.T\n",
    "    z=sigma(pre_activations)@W_2\n",
    "    return np.einsum('m,a,ma,mi->ai',deriv_ell(y,z),W_2,deriv_sigma(pre_activations),X)/y.shape[0]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42446192-7e2c-4f9b-988c-6879f77f3183",
   "metadata": {},
   "source": [
    "#### Ex 3.2 Training a neural network\n",
    "Using the functions above complete the training loop for the neural network below.\n",
    "1. Set $K=5$ (number of hidden units), initialize the weigths randomly with a small norm, $\\gamma=0.3$ seems to work well as step size. Expect around 100-200 iterations to be needed to learn \n",
    "2. Keep track of training and validation error along the dynamics, similarly to what you did with logistic regression. You do not need to implement early stopping (but you can do it as an extra)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "3e49ec80-d899-4fda-a198-022cbc21206e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "tmax=200\n",
    "K=5\n",
    "gamma=0.3 #learning rate\n",
    "\n",
    "###your code goes here###\n",
    "\n",
    "for t in range(tmax):#training loop\n",
    "    pass\n",
    "    ### your code goes here###"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc225e6e-c9e1-470c-91ee-c821ec1ec855",
   "metadata": {},
   "source": [
    "#### Ex 3.3 Plotting and testing\n",
    "1. plot the train and validation accuracy as a function of time\n",
    "2. compute the test accuracy on the weights with best validation accuracy (if you did the extra point), otherwise on the final weights of the training\n",
    "3. How does the test accuracy compare with that of logistic regression?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6ebf9597-5852-4198-84a0-f2b19bf52ea5",
   "metadata": {},
   "outputs": [],
   "source": [
    "### your code goes here ###"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ca9089f-131c-4715-b3e2-6e54137fe5e8",
   "metadata": {},
   "source": [
    "#### Ex 3.4: **Extra** (do this after doing 3.5) Changing activation function\n",
    "So far the neural network employed hyperbolic tangent activations.\n",
    "Change the code above to switch to sigmoid activations. Then retrain the network.\n",
    "You will have to change the value of learning rate and norm of the initialization, in order for the network to train effectively.\n",
    "\n",
    "Hint: changing the function `sigma` is not enough...\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f9b0c1e-e754-4c9a-b184-b6ce905345fd",
   "metadata": {},
   "source": [
    "#### Ex 3.5 Sklearn implementation of a neural network\n",
    "1. Use the class MLPClassifier to train a neural network with tanh activation functions\n",
    "2. Compute the test accuracy of the trained network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd1bec50-3a31-406f-a940-635259fc9194",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neural_network import MLPClassifier\n",
    "###your code goes here ###"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
