{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "abd71f5b-7ad8-4570-ba67-f044acd061f0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93e37f8b-bc27-4430-be53-fd137b453c02",
   "metadata": {},
   "source": [
    "# Gradient descent, Logistic Regression and Introduction to Neural Networks\n",
    "\n",
    "Today's task is to do implement the gradient descent algorithm on a simple case and visalize what it does. Then, we will implement logistic regression to classify images of even/odd digits of the MNIST dataset.\n",
    "1. In Exercise 1, we will implement the gradient descent algorithm in a 1D case and visualize its behavior.\n",
    "2. In Exercise 2, we will load and preprocess the MNIST and its labels.\n",
    "3. In Exercise 3, we will perform classification by doing logistic regression.\n",
    "4. In Exercise 4, we will obtain some familiarity with Neural Networks using a web playground."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c25a1f7",
   "metadata": {},
   "source": [
    "## Exercice 1: Gradient descent for a single variable function"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eee14410",
   "metadata": {},
   "source": [
    "In this exercise, we apply the gradient descend algorithm to find the minimum $\\hat{x}$ of the function $f(x)=x^2$, $x\\in\\mathbb{R}$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14e0fbf9",
   "metadata": {},
   "source": [
    "**1.1** Code the function $f(x)$ that takes as an input a scalar or a numpy array and returns its (element-wise) square. Your code should not include a loop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acb6232e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "285981fb",
   "metadata": {},
   "source": [
    "**1.2** To test your implementation, plot the function $f(x)$ between $-5$ and $5$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9b8bdb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "832ee7d5",
   "metadata": {},
   "source": [
    "**1.3** Let us now compute the gradient. Since $x$ is a scalar, the gradient is simply the derivative. Code a function that takes as an input a scalar or a numpy array and returns the derivative $f'(x)$ element wise at the given input values:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edc5a4c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e46cbcc3",
   "metadata": {},
   "source": [
    "**1.4** Implement the gradient descent algorithm to find the minimum $\\hat{x}$ of f. Set the learning rate $\\gamma=0.1$ and initialize $x=2$. Do $T=100$ iterations of the gradient descent algorithm (do not use a stopping/convergence condition). Print your obtained $\\hat{x}$. Check that your obtained result is close to the minimum of $f(x)= x^2$. Recall that one step of the gradient descent algorithm is\n",
    "$$\n",
    "x_{t+1}=x_t-\\gamma \\nabla f(x_t)\n",
    "$$\n",
    "and use your function from 1.3 to compute the gradient."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49ec7683",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5f79187",
   "metadata": {},
   "source": [
    "Your answer here:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13f72760",
   "metadata": {},
   "source": [
    "**1.5** We will now study the influence of the learning rate $\\gamma$. Apply the gradient descent algorithm for $T=7$ steps, $\\gamma=0.1$ and intialize $x=2$, and store the initial value of $x$ as well as the value of $x$ after each gradient descent step in a numpy array. Print the obtained array."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57c8dc33",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1709bfad",
   "metadata": {},
   "source": [
    "**1.6** Code a function that takes as an input a list of values of $x_i$ and that plots $f(x)$ in blue between $-3$ and $3$ and scatters the points $(x_i, f(x_i))$ in red on the same plot. The function should have an additional input which is a string that is used as the title of the figure. Call this function with the previously obtained list and a title indicated the used $\\gamma$. Describe and explain qualitatively what you obtain. Be concise (maximum 4-5 sentences). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2013a962",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b796c6a7",
   "metadata": {},
   "source": [
    "Your answer here."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5d6e30e",
   "metadata": {},
   "source": [
    "**1.7** Repeat 1.5 and 1.6 now with $T=5$ steps for $\\gamma=0.25, 0.5, 0.8 \\text{ and } 1.05$. Reuse your function from 1.6 to draw the figures (change the title to the appropriate learning rate). You can copy-past your code if needed, but in this case use a new cell for each learning rate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04560012",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6dc477c1",
   "metadata": {},
   "source": [
    "**1.8** Answer briefly the following questions using your obtained figures:\n",
    "- Compare $\\gamma=0.1$ and $\\gamma=0.25$. What changes ?\n",
    "- Describe what happens for $\\gamma=0.8$.\n",
    "- Describe what happens for $\\gamma=1.05$. What is the problem ?\n",
    "- What happens for $\\gamma=0.5$ ? Explain why this happens. Is this dependent on the initial $x$ in our case ?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a41775cb",
   "metadata": {},
   "source": [
    "Your answer here:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5805235b-2417-4631-be05-d93c39666717",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Ex 2. Dataset loading and preprocessing\n",
    "We first preprocess the mnist dataset.\n",
    "Our task is to do even/odd binary classification on the MNIST dataset. \n",
    "The dataset is composed of 10000 images, each of which is 28x28 pixels. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9d9b2af1-acd5-4fb7-bc68-3551e879c2e4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#load the mnist dataset\n",
    "mnist= np.load(\"mnist.npy\")\n",
    "mnist_labels = np.load(\"mnist_labels.npy\")\n",
    "\n",
    "X=mnist\n",
    "y=(mnist_labels%2)*2.0-1 # 1 for odd, -1 for even."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c049e435-797e-4d06-aa2b-2f8cf828a93a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "label=1.0\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAaEAAAGdCAYAAAC7EMwUAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjUuMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8qNh9FAAAACXBIWXMAAA9hAAAPYQGoP6dpAAAZzElEQVR4nO3df2xUZ37v8c+AYRbY8bQusWccHK+bgnYXU6QFFnD5YVBxcbsoxNnKSdTISLs02QAq10lRCOrFd3WFc1lBaesNq422LHRhg9oSggoN8S7YLCKkDiUFkSxyilkc4ZEvbuIxhoxxeO4fXKaZ2JicYYavZ/x+SUdizpzH58nJSd4+zMwZn3POCQAAA6OsJwAAGLmIEADADBECAJghQgAAM0QIAGCGCAEAzBAhAIAZIgQAMJNjPYHPu3nzpi5fvqxAICCfz2c9HQCAR8459fT0qLCwUKNGDX2tM+widPnyZRUVFVlPAwBwj9rb2zVp0qQhtxl2EQoEApKkefpj5WiM8WwAAF7164aO61D8/+dDSVuEXn75Zf3gBz9QR0eHpk6dqm3btmn+/Pl3HXf7r+ByNEY5PiIEABnn/9+R9Iu8pJKWNybs3btXa9eu1YYNG3T69GnNnz9flZWVunTpUjp2BwDIUGmJ0NatW/Wd73xH3/3ud/W1r31N27ZtU1FRkbZv356O3QEAMlTKI9TX16dTp06poqIiYX1FRYVOnDgxYPtYLKZoNJqwAABGhpRH6MqVK/r0009VUFCQsL6goECRSGTA9vX19QoGg/GFd8YBwMiRtg+rfv4FKefcoC9SrV+/Xt3d3fGlvb09XVMCAAwzKX933MSJEzV69OgBVz2dnZ0Dro4kye/3y+/3p3oaAIAMkPIrobFjx2rGjBlqbGxMWN/Y2KiysrJU7w4AkMHS8jmh2tpaPfXUU5o5c6bmzp2rH//4x7p06ZKeeeaZdOwOAJCh0hKh6upqdXV16fvf/746OjpUWlqqQ4cOqbi4OB27AwBkKJ9zzllP4rOi0aiCwaDK9Qh3TACADNTvbqhJr6u7u1u5ublDbstXOQAAzBAhAIAZIgQAMEOEAABmiBAAwAwRAgCYIUIAADNECABghggBAMwQIQCAGSIEADBDhAAAZogQAMAMEQIAmCFCAAAzRAgAYIYIAQDMECEAgBkiBAAwQ4QAAGaIEADADBECAJghQgAAM0QIAGCGCAEAzBAhAIAZIgQAMEOEAABmiBAAwAwRAgCYIUIAADNECABghggBAMwQIQCAGSIEADBDhAAAZogQAMAMEQIAmCFCAAAzRAgAYIYIAQDMECEAgBkiBAAwQ4QAAGaIEADADBECAJghQgAAM0QIAGCGCAEAzBAhAIAZIgQAMEOEAABmiBAAwAwRAgCYIUIAADMpj1BdXZ18Pl/CEgqFUr0bAEAWyEnHD506dap+8YtfxB+PHj06HbsBAGS4tEQoJyeHqx8AwF2l5TWh1tZWFRYWqqSkRI8//rguXLhwx21jsZii0WjCAgAYGVIeodmzZ2vXrl06fPiwXnnlFUUiEZWVlamrq2vQ7evr6xUMBuNLUVFRqqcEABimfM45l84d9Pb26uGHH9a6detUW1s74PlYLKZYLBZ/HI1GVVRUpHI9ohzfmHRODQCQBv3uhpr0urq7u5Wbmzvktml5TeizJkyYoGnTpqm1tXXQ5/1+v/x+f7qnAQAYhtL+OaFYLKb3339f4XA43bsCAGSYlEfo+eefV3Nzs9ra2vT222/r29/+tqLRqGpqalK9KwBAhkv5X8d9+OGHeuKJJ3TlyhU98MADmjNnjk6ePKni4uJU7woAkOFSHqFXX3011T8SAJCluHccAMAMEQIAmCFCAAAzRAgAYIYIAQDMECEAgBkiBAAwQ4QAAGaIEADADBECAJghQgAAM0QIAGAm7V9qh/ura+Vcz2MeeuqDpPb1684Cz2P6Yt6/LffBn3sfM/7Dq57HSNLNd99LahyA5HAlBAAwQ4QAAGaIEADADBECAJghQgAAM0QIAGCGCAEAzBAhAIAZIgQAMEOEAABmiBAAwAwRAgCYIUIAADPcRTvLrPvLPZ7HPDbho+R29nBywzwr9z7kYv+1pHb1N/93UVLjcP/8W2ex5zETtgST2lfOL08lNQ5fHFdCAAAzRAgAYIYIAQDMECEAgBkiBAAwQ4QAAGaIEADADBECAJghQgAAM0QIAGCGCAEAzBAhAIAZbmCaZf72xcc9j/mfv5/c7yK//b7zPOajr/k8jxn7+x97HrO5dJ/nMZL01+G3PY85eO3Lnsf8yfirnsfcT9ddn+cxb8cmeB5T/qUbnscoiX9Hv1f9tPf9SJryy6SGwQOuhAAAZogQAMAMEQIAmCFCAAAzRAgAYIYIAQDMECEAgBkiBAAwQ4QAAGaIEADADBECAJghQgAAM9zANMtM+CfvN3ec8E9pmMgd5N6n/fxdqDypcf/7D77ieUxu8weex2wu/z3PY+6nnOs3PY+ZcKbD85jfOfbPnsdMGzvG85jxF72Pwf3BlRAAwAwRAgCY8RyhY8eOadmyZSosLJTP59P+/fsTnnfOqa6uToWFhRo3bpzKy8t17ty5VM0XAJBFPEeot7dX06dPV0NDw6DPb968WVu3blVDQ4NaWloUCoW0ZMkS9fT03PNkAQDZxfMbEyorK1VZWTnoc845bdu2TRs2bFBVVZUkaefOnSooKNCePXv09NPJfbshACA7pfQ1oba2NkUiEVVUVMTX+f1+LVy4UCdOnBh0TCwWUzQaTVgAACNDSiMUiUQkSQUFBQnrCwoK4s99Xn19vYLBYHwpKipK5ZQAAMNYWt4d5/P5Eh475wasu239+vXq7u6OL+3t7emYEgBgGErph1VDoZCkW1dE4XA4vr6zs3PA1dFtfr9ffr8/ldMAAGSIlF4JlZSUKBQKqbGxMb6ur69Pzc3NKisrS+WuAABZwPOV0NWrV/XBB/99m5K2tja9++67ysvL00MPPaS1a9dq06ZNmjx5siZPnqxNmzZp/PjxevLJJ1M6cQBA5vMcoXfeeUeLFi2KP66trZUk1dTU6Kc//anWrVun69ev69lnn9VHH32k2bNn680331QgEEjdrAEAWcHnnHPWk/isaDSqYDCocj2iHB83HQQyRdd353oe89b/GvxD70PZ+l9f9TzmWMXDnsdIUn/H4O/qxdD63Q016XV1d3crN3fo2xZz7zgAgBkiBAAwQ4QAAGaIEADADBECAJghQgAAM0QIAGCGCAEAzBAhAIAZIgQAMEOEAABmiBAAwAwRAgCYSek3qwLIDjnFRZ7HNLzo/Y7YY3yjPY/5x7/5Q89jfqfjLc9jcH9wJQQAMEOEAABmiBAAwAwRAgCYIUIAADNECABghggBAMwQIQCAGSIEADBDhAAAZogQAMAMEQIAmOEGpgAG+PX/eNDzmFl+n+cx5/quex6T9941z2MwfHElBAAwQ4QAAGaIEADADBECAJghQgAAM0QIAGCGCAEAzBAhAIAZIgQAMEOEAABmiBAAwAwRAgCY4QamQBaL/cmspMb9+7f/OolRfs8jvvcXf+F5zLgT/+Z5DIYvroQAAGaIEADADBECAJghQgAAM0QIAGCGCAEAzBAhAIAZIgQAMEOEAABmiBAAwAwRAgCYIUIAADPcwBTIYpcqk/s988s+7zcjfaJtiecx49/4D89jnOcRGM64EgIAmCFCAAAzniN07NgxLVu2TIWFhfL5fNq/f3/C8ytWrJDP50tY5syZk6r5AgCyiOcI9fb2avr06WpoaLjjNkuXLlVHR0d8OXTo0D1NEgCQnTy/MaGyslKVlZVDbuP3+xUKhZKeFABgZEjLa0JNTU3Kz8/XlClTtHLlSnV2dt5x21gspmg0mrAAAEaGlEeosrJSu3fv1pEjR7Rlyxa1tLRo8eLFisVig25fX1+vYDAYX4qKilI9JQDAMJXyzwlVV1fH/1xaWqqZM2equLhYBw8eVFVV1YDt169fr9ra2vjjaDRKiABghEj7h1XD4bCKi4vV2to66PN+v19+v/cPxgEAMl/aPyfU1dWl9vZ2hcPhdO8KAJBhPF8JXb16VR988EH8cVtbm959913l5eUpLy9PdXV1euyxxxQOh3Xx4kW9+OKLmjhxoh599NGUThwAkPk8R+idd97RokWL4o9vv55TU1Oj7du36+zZs9q1a5c+/vhjhcNhLVq0SHv37lUgEEjdrAEAWcFzhMrLy+XcnW8hePjw4XuaEIDBjUriF7mn5h9Pal/Rm594HtO56Xc9j/HHWjyPQXbh3nEAADNECABghggBAMwQIQCAGSIEADBDhAAAZogQAMAMEQIAmCFCAAAzRAgAYIYIAQDMECEAgBkiBAAwk/ZvVgWQGq11Uz2P+ZeJLye1r0daH/M8xn+IO2LDO66EAABmiBAAwAwRAgCYIUIAADNECABghggBAMwQIQCAGSIEADBDhAAAZogQAMAMEQIAmCFCAAAz3MAUMND9Z3M8jzlT/beex/xn/w3PYyTp6v+Z5HmMXx1J7QsjG1dCAAAzRAgAYIYIAQDMECEAgBkiBAAwQ4QAAGaIEADADBECAJghQgAAM0QIAGCGCAEAzBAhAIAZbmAK3KOcBws9j1n7V3s9j/H7vP/n+vh/POV5jCQ98K8tSY0DvOJKCABghggBAMwQIQCAGSIEADBDhAAAZogQAMAMEQIAmCFCAAAzRAgAYIYIAQDMECEAgBkiBAAwww1Mgc/w5Xj/T2L6v3zoecyffrnL85jdPfmexxT8VXK/Z95MahTgHVdCAAAzRAgAYMZThOrr6zVr1iwFAgHl5+dr+fLlOn/+fMI2zjnV1dWpsLBQ48aNU3l5uc6dO5fSSQMAsoOnCDU3N2vVqlU6efKkGhsb1d/fr4qKCvX29sa32bx5s7Zu3aqGhga1tLQoFAppyZIl6unpSfnkAQCZzdOrsG+88UbC4x07dig/P1+nTp3SggUL5JzTtm3btGHDBlVVVUmSdu7cqYKCAu3Zs0dPP/106mYOAMh49/SaUHd3tyQpLy9PktTW1qZIJKKKior4Nn6/XwsXLtSJEycG/RmxWEzRaDRhAQCMDElHyDmn2tpazZs3T6WlpZKkSCQiSSooKEjYtqCgIP7c59XX1ysYDMaXoqKiZKcEAMgwSUdo9erVOnPmjH7+858PeM7n8yU8ds4NWHfb+vXr1d3dHV/a29uTnRIAIMMk9WHVNWvW6MCBAzp27JgmTZoUXx8KhSTduiIKh8Px9Z2dnQOujm7z+/3y+/3JTAMAkOE8XQk557R69Wrt27dPR44cUUlJScLzJSUlCoVCamxsjK/r6+tTc3OzysrKUjNjAEDW8HQltGrVKu3Zs0evv/66AoFA/HWeYDCocePGyefzae3atdq0aZMmT56syZMna9OmTRo/fryefPLJtPwDAAAyl6cIbd++XZJUXl6esH7Hjh1asWKFJGndunW6fv26nn32WX300UeaPXu23nzzTQUCgZRMGACQPXzOOWc9ic+KRqMKBoMq1yPK8Y2xng5GGN+MqZ7HHDzwD2mYyUBl61d5HvNbu95Kw0yAofW7G2rS6+ru7lZubu6Q23LvOACAGSIEADBDhAAAZogQAMAMEQIAmCFCAAAzRAgAYIYIAQDMECEAgBkiBAAwQ4QAAGaIEADADBECAJhJ6ptVgeFu9NenJDXuz199PcUzGdzX/977HbG/8g8n0zATwBZXQgAAM0QIAGCGCAEAzBAhAIAZIgQAMEOEAABmiBAAwAwRAgCYIUIAADNECABghggBAMwQIQCAGW5giqz062d/O6lxy8ZHUzyTwU1q6vM+yLnUTwQwxpUQAMAMEQIAmCFCAAAzRAgAYIYIAQDMECEAgBkiBAAwQ4QAAGaIEADADBECAJghQgAAM0QIAGCGG5hi2Ptk2Tc9j/nlsi1J7m18kuMAJIMrIQCAGSIEADBDhAAAZogQAMAMEQIAmCFCAAAzRAgAYIYIAQDMECEAgBkiBAAwQ4QAAGaIEADADDcwxbB3+Q9Gex7zUM79uxHp7p58z2PGRPs8j3GeRwDDH1dCAAAzRAgAYMZThOrr6zVr1iwFAgHl5+dr+fLlOn/+fMI2K1askM/nS1jmzJmT0kkDALKDpwg1Nzdr1apVOnnypBobG9Xf36+Kigr19vYmbLd06VJ1dHTEl0OHDqV00gCA7ODpjQlvvPFGwuMdO3YoPz9fp06d0oIFC+Lr/X6/QqFQamYIAMha9/SaUHd3tyQpLy8vYX1TU5Py8/M1ZcoUrVy5Up2dnXf8GbFYTNFoNGEBAIwMSUfIOafa2lrNmzdPpaWl8fWVlZXavXu3jhw5oi1btqilpUWLFy9WLBYb9OfU19crGAzGl6KiomSnBADIMEl/Tmj16tU6c+aMjh8/nrC+uro6/ufS0lLNnDlTxcXFOnjwoKqqqgb8nPXr16u2tjb+OBqNEiIAGCGSitCaNWt04MABHTt2TJMmTRpy23A4rOLiYrW2tg76vN/vl9/vT2YaAIAM5ylCzjmtWbNGr732mpqamlRSUnLXMV1dXWpvb1c4HE56kgCA7OTpNaFVq1bpZz/7mfbs2aNAIKBIJKJIJKLr169Lkq5evarnn39eb731li5evKimpiYtW7ZMEydO1KOPPpqWfwAAQObydCW0fft2SVJ5eXnC+h07dmjFihUaPXq0zp49q127dunjjz9WOBzWokWLtHfvXgUCgZRNGgCQHTz/ddxQxo0bp8OHD9/ThAAAIwd30QY+o77r657HvPVHX/E8xnWc9TwGyEbcwBQAYIYIAQDMECEAgBkiBAAwQ4QAAGaIEADADBECAJghQgAAM0QIAGCGCAEAzBAhAIAZIgQAMMMNTDHs/e4Lb3ke88cvfCMNM7mTyH3cF5BduBICAJghQgAAM0QIAGCGCAEAzBAhAIAZIgQAMEOEAABmiBAAwAwRAgCYIUIAADNECABgZtjdO845J0nq1w3JGU8GAOBZv25I+u//nw9l2EWop6dHknRch4xnAgC4Fz09PQoGg0Nu43NfJFX30c2bN3X58mUFAgH5fL6E56LRqIqKitTe3q7c3FyjGdrjONzCcbiF43ALx+GW4XAcnHPq6elRYWGhRo0a+lWfYXclNGrUKE2aNGnIbXJzc0f0SXYbx+EWjsMtHIdbOA63WB+Hu10B3cYbEwAAZogQAMBMRkXI7/dr48aN8vv91lMxxXG4heNwC8fhFo7DLZl2HIbdGxMAACNHRl0JAQCyCxECAJghQgAAM0QIAGAmoyL08ssvq6SkRF/60pc0Y8YM/epXv7Ke0n1VV1cnn8+XsIRCIetppd2xY8e0bNkyFRYWyufzaf/+/QnPO+dUV1enwsJCjRs3TuXl5Tp37pzNZNPobsdhxYoVA86POXPm2Ew2Terr6zVr1iwFAgHl5+dr+fLlOn/+fMI2I+F8+CLHIVPOh4yJ0N69e7V27Vpt2LBBp0+f1vz581VZWalLly5ZT+2+mjp1qjo6OuLL2bNnraeUdr29vZo+fboaGhoGfX7z5s3aunWrGhoa1NLSolAopCVLlsTvQ5gt7nYcJGnp0qUJ58ehQ9l1D8bm5matWrVKJ0+eVGNjo/r7+1VRUaHe3t74NiPhfPgix0HKkPPBZYhvfvOb7plnnklY99WvftW98MILRjO6/zZu3OimT59uPQ1Tktxrr70Wf3zz5k0XCoXcSy+9FF/3ySefuGAw6H70ox8ZzPD++PxxcM65mpoa98gjj5jMx0pnZ6eT5Jqbm51zI/d8+PxxcC5zzoeMuBLq6+vTqVOnVFFRkbC+oqJCJ06cMJqVjdbWVhUWFqqkpESPP/64Lly4YD0lU21tbYpEIgnnht/v18KFC0fcuSFJTU1Nys/P15QpU7Ry5Up1dnZaTymturu7JUl5eXmSRu758PnjcFsmnA8ZEaErV67o008/VUFBQcL6goICRSIRo1ndf7Nnz9auXbt0+PBhvfLKK4pEIiorK1NXV5f11Mzc/vc/0s8NSaqsrNTu3bt15MgRbdmyRS0tLVq8eLFisZj11NLCOafa2lrNmzdPpaWlkkbm+TDYcZAy53wYdnfRHsrnv9rBOTdgXTarrKyM/3natGmaO3euHn74Ye3cuVO1tbWGM7M30s8NSaquro7/ubS0VDNnzlRxcbEOHjyoqqoqw5mlx+rVq3XmzBkdP358wHMj6Xy403HIlPMhI66EJk6cqNGjRw/4Taazs3PAbzwjyYQJEzRt2jS1trZaT8XM7XcHcm4MFA6HVVxcnJXnx5o1a3TgwAEdPXo04atfRtr5cKfjMJjhej5kRITGjh2rGTNmqLGxMWF9Y2OjysrKjGZlLxaL6f3331c4HLaeipmSkhKFQqGEc6Ovr0/Nzc0j+tyQpK6uLrW3t2fV+eGc0+rVq7Vv3z4dOXJEJSUlCc+PlPPhbsdhMMP2fDB8U4Qnr776qhszZoz7yU9+4t577z23du1aN2HCBHfx4kXrqd03zz33nGtqanIXLlxwJ0+edN/61rdcIBDI+mPQ09PjTp8+7U6fPu0kua1bt7rTp0+73/zmN84551566SUXDAbdvn373NmzZ90TTzzhwuGwi0ajxjNPraGOQ09Pj3vuuefciRMnXFtbmzt69KibO3eue/DBB7PqOHzve99zwWDQNTU1uY6Ojvhy7dq1+DYj4Xy423HIpPMhYyLknHM//OEPXXFxsRs7dqz7xje+kfB2xJGgurrahcNhN2bMGFdYWOiqqqrcuXPnrKeVdkePHnWSBiw1NTXOuVtvy924caMLhULO7/e7BQsWuLNnz9pOOg2GOg7Xrl1zFRUV7oEHHnBjxoxxDz30kKupqXGXLl2ynnZKDfbPL8nt2LEjvs1IOB/udhwy6XzgqxwAAGYy4jUhAEB2IkIAADNECABghggBAMwQIQCAGSIEADBDhAAAZogQAMAMEQIAmCFCAAAzRAgAYIYIAQDM/D8lKJV+csJBcgAAAABJRU5ErkJggg==",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#printing the first digit\n",
    "idx=0\n",
    "plt.imshow(X[idx])\n",
    "print(f\"label={y[idx]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9135a966-8ff5-407f-8e1f-db85bcee1101",
   "metadata": {},
   "source": [
    "#### Ex 2.1\n",
    "1. Reshape X so that it is a 10000 by 768 matrix.\n",
    "2. Do a 60:20:20  train, validation, test split. The train, validation and test set shall be named `X_train`,`y_train`, `X_val`, `y_val`, `X_test`,  `y_test`\n",
    "\n",
    "Before splitting, the lines of the data matrix should be randomly permuted to guarantee that samples in each set are independent.\n",
    "\n",
    "Hint: for the splitting, check for a dedicated function in skelarn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fcbe474d-510d-4a67-8869-7a621217d277",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "### your code goes here ###"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a46a4f7-4f1b-4354-b60f-f7da73447e31",
   "metadata": {},
   "source": [
    "#### Ex 2.2 Normalize the dataset. \n",
    "1. Compute the mean and standard deviation (std) over the whole *training* dataset (i.e. the mean and std should be two scalars computed on all the elements of `X_train`). \n",
    "2. Then, normalize the `X_train`,`X_val`,`X_test` using the mean and std computed above.\n",
    "\n",
    "Why do we use `X_train` to compute the normalization and then applying the same normalization also to `X_val`,  `X_test`? For two reasons:\n",
    "1. If one computed the normalization on the whole `X`, this would have introduced correlations between the datasets\n",
    "2. In some cases `X_test,y_test` represent new data which are not available at the moment of training. Then it's important to have a unique way of normalizing, using only the training set\n",
    "\n",
    "An additional note on normalization: two weaks ago, we normalized the images by taking the mean and standard deviation for each pixel, last week we normalized the images by putting them in the range $0-1$, and this week we normalize them by taking the mean and standard deviation over all the pixels. As you can see, there is not a unique way of normalizing the data. Each method has its specific application and advantages. For instance, a normalization between $0-1$ may be more adapted when the distribution is not gaussian or when the standard deviation is very small, it is however sensitive to outliers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5603cccb-8fa0-4ba4-a358-7ab85f8c0fc0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "### your code goes here ###"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c3dfeec-19e5-443e-8391-201a9777a800",
   "metadata": {},
   "source": [
    "## Ex 3: Logistic regression\n",
    "In this exercise you will implement a classifier that uses logistic regression to distinguish even from odd digits in the MNIST dataset. The classifier will be trained using gradient descent.\n",
    "Let's set some notation: we call $n$ the number of datapoints in the training set and $d$ the dimensionality (in our case the number of pixels) of each datapoint.\n",
    "\n",
    "In the logistic regression case, recall that our predictor is $z= \\vec x \\cdot \\vec w$, where both $\\vec x$ and $\\vec w$ are in $\\mathbb R^d$.\n",
    "\n",
    "The regularized training loss is\n",
    "$$\\mathcal L(\\vec w)=\\frac{1}{n}\\sum_{\\mu=1}^n\\ell(y_\\mu,\\vec X_\\mu\\cdot \\vec w)+\\frac{\\lambda}{n}\\sum_{i=1}^d w_i^2,\\quad \\ell(y,z)=\\log(1+e^{-yz})$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60a71ba7-c0a4-43eb-bf59-9ba724d6d076",
   "metadata": {},
   "source": [
    "The gradient of the loss with respect to $\\vec w $ is \n",
    "$$\\frac{\\partial \\mathcal L}{\\partial \\vec w_i}(\\vec w)=-\\frac{1}{n}\\sum_{\\mu=1}^n \\frac{e^{-y_\\mu \\vec X_\\mu\\cdot \\vec w}}{1+e^{-y_\\mu \\vec X_\\mu\\cdot \\vec w}} X_{\\mu i} y_\\mu+\\frac{2\\lambda}{n}\\vec w_i$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ef30c5a-d137-42c6-9d7d-51328a6b29aa",
   "metadata": {},
   "source": [
    "#### Ex 3.1:  Implementing the gradient\n",
    "Using the above formula, implement a function `loss_gradient` that takes as arguments `X_train,y_train,w,lambd` and returns a numpy array containing the gradient of the loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b48be59-6575-4562-9224-44bec28099e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "### your code goes here ###"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da193128-6394-417e-99f2-10692566fdef",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Ex 3.2: training\n",
    "Write a training loop that perfoms gradient descent.\n",
    "1. Set $\\lambda=1$ and choose a good step size that allows the algorithm to learn. 2000 steps should be enough to learn. For the initialization you can use a random one.\n",
    "2. Every 100 gradient steps, a measurement of training and validation accuracies (i.e. 1- error), should be taken and stored in lists. \n",
    "3. After the training make a plot of train and test accuracies vs time\n",
    "\n",
    "*Recall that the accuracy of a model is the fraction of times the model correctly predicts the class.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "40657f8c-8c90-49dc-b425-e7b1c4043a70",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "tmax=2000 #number of iterations\n",
    "lambd=1 #regularization\n",
    "\n",
    "### your code goes here ###\n",
    "\n",
    "for t in range(tmax):# training loop\n",
    "    ### your code goes here ###\n",
    "    pass "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42fa8c17-8d0e-461a-bed5-98df5447fb53",
   "metadata": {},
   "source": [
    "#### Ex 3.3: Early stopping\n",
    "Modify the training loop to implement early stopping (you can copy-paste the training loop you wrote and modify it). \n",
    "You will do so by still runnnig gradient descent for a fixed number of steps `tmax`.\n",
    "Whenever you take a measurement of the validation accuracy, check if this is the highest accuracy reached so far in the training. If it is, save in memory the current weight vector and time. This way at the end of training you will have your weight vector with highest validation accuracy.\n",
    "\n",
    "After training the model again make a showing the train and validation accuracies vs time. Mark in some way the time at which the highest validation accuracy was achieved. Finally compute the test error of the weight vector with highest validation error\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b93c6b9d-d7a6-4a8c-bfb2-b27ad7fa9b28",
   "metadata": {},
   "source": [
    "###  Ex 3.4: Logistic regression with sklearn\n",
    "The scikit-learn library contains a logistic regression implementation, allowing to train logistic regression in a couple of lines of code. \n",
    "1. Implement logistic regression without regularization\n",
    "2. Compute the test error\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "50f06971-6ecf-4112-8dcd-95ed628d9b13",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "### your code goes here ###"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c80b9e1-ad51-4a49-a9f3-9ddb2cc4a24e",
   "metadata": {},
   "source": [
    "## Ex 4: Visualization of Neural Networks\n",
    "To obtain some familiarity with neural networks, go to https://playground.tensorflow.org/.\n",
    "On the left you can choose a dataset on which to do classification. You can then modify the network by choosing the number of hidden layers, number of neurons in each layer, activation function. \n",
    "\n",
    "The hardest dataset to learn is the spiral: play with the parameters of the network until it manages to learn this dataset."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "unianalytics_cell_mapping": [
   [
    "abd71f5b-7ad8-4570-ba67-f044acd061f0",
    "abd71f5b-7ad8-4570-ba67-f044acd061f0"
   ],
   [
    "93e37f8b-bc27-4430-be53-fd137b453c02",
    "93e37f8b-bc27-4430-be53-fd137b453c02"
   ],
   [
    "5c25a1f7",
    "5c25a1f7"
   ],
   [
    "eee14410",
    "eee14410"
   ],
   [
    "14e0fbf9",
    "14e0fbf9"
   ],
   [
    "acb6232e",
    "acb6232e"
   ],
   [
    "285981fb",
    "285981fb"
   ],
   [
    "a9b8bdb7",
    "a9b8bdb7"
   ],
   [
    "832ee7d5",
    "832ee7d5"
   ],
   [
    "edc5a4c9",
    "edc5a4c9"
   ],
   [
    "e46cbcc3",
    "e46cbcc3"
   ],
   [
    "49ec7683",
    "49ec7683"
   ],
   [
    "a5f79187",
    "a5f79187"
   ],
   [
    "13f72760",
    "13f72760"
   ],
   [
    "57c8dc33",
    "57c8dc33"
   ],
   [
    "1709bfad",
    "1709bfad"
   ],
   [
    "2013a962",
    "2013a962"
   ],
   [
    "b796c6a7",
    "b796c6a7"
   ],
   [
    "e5d6e30e",
    "e5d6e30e"
   ],
   [
    "04560012",
    "04560012"
   ],
   [
    "6dc477c1",
    "6dc477c1"
   ],
   [
    "a41775cb",
    "a41775cb"
   ],
   [
    "5805235b-2417-4631-be05-d93c39666717",
    "5805235b-2417-4631-be05-d93c39666717"
   ],
   [
    "9d9b2af1-acd5-4fb7-bc68-3551e879c2e4",
    "9d9b2af1-acd5-4fb7-bc68-3551e879c2e4"
   ],
   [
    "c049e435-797e-4d06-aa2b-2f8cf828a93a",
    "c049e435-797e-4d06-aa2b-2f8cf828a93a"
   ],
   [
    "9135a966-8ff5-407f-8e1f-db85bcee1101",
    "9135a966-8ff5-407f-8e1f-db85bcee1101"
   ],
   [
    "fcbe474d-510d-4a67-8869-7a621217d277",
    "fcbe474d-510d-4a67-8869-7a621217d277"
   ],
   [
    "6a46a4f7-4f1b-4354-b60f-f7da73447e31",
    "6a46a4f7-4f1b-4354-b60f-f7da73447e31"
   ],
   [
    "5603cccb-8fa0-4ba4-a358-7ab85f8c0fc0",
    "5603cccb-8fa0-4ba4-a358-7ab85f8c0fc0"
   ],
   [
    "1c3dfeec-19e5-443e-8391-201a9777a800",
    "1c3dfeec-19e5-443e-8391-201a9777a800"
   ],
   [
    "60a71ba7-c0a4-43eb-bf59-9ba724d6d076",
    "60a71ba7-c0a4-43eb-bf59-9ba724d6d076"
   ],
   [
    "2ef30c5a-d137-42c6-9d7d-51328a6b29aa",
    "2ef30c5a-d137-42c6-9d7d-51328a6b29aa"
   ],
   [
    "0b48be59-6575-4562-9224-44bec28099e3",
    "0b48be59-6575-4562-9224-44bec28099e3"
   ],
   [
    "da193128-6394-417e-99f2-10692566fdef",
    "da193128-6394-417e-99f2-10692566fdef"
   ],
   [
    "40657f8c-8c90-49dc-b425-e7b1c4043a70",
    "40657f8c-8c90-49dc-b425-e7b1c4043a70"
   ],
   [
    "42fa8c17-8d0e-461a-bed5-98df5447fb53",
    "42fa8c17-8d0e-461a-bed5-98df5447fb53"
   ],
   [
    "b93c6b9d-d7a6-4a8c-bfb2-b27ad7fa9b28",
    "b93c6b9d-d7a6-4a8c-bfb2-b27ad7fa9b28"
   ],
   [
    "50f06971-6ecf-4112-8dcd-95ed628d9b13",
    "50f06971-6ecf-4112-8dcd-95ed628d9b13"
   ],
   [
    "9c80b9e1-ad51-4a49-a9f3-9ddb2cc4a24e",
    "9c80b9e1-ad51-4a49-a9f3-9ddb2cc4a24e"
   ]
  ],
  "unianalytics_notebook_id": "9b2562e3-4e87-4218-858c-8beb1264ab41"
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
